# DigitalOcean Gradient AI Platform Integration Plan

## üéØ Executive Summary

**Gradient** is DigitalOcean's managed AI inference platform that provides:

- **GPU-accelerated model hosting** on DigitalOcean infrastructure
- **OpenAI-compatible API** (drop-in replacement for OpenAI SDK)
- **Pay-per-use pricing** (no idle GPU costs)
- **Fast cold starts** (~2-5 seconds)
- **Multiple model options** (Llama 3.1, Mistral, CodeLlama, etc.)

**Key Advantage:** Your agents are already on DigitalOcean droplet `45.55.173.72`, so Gradient inference stays within DO's network for **lower latency** and **no egress fees**.

---

## üöÄ Quickest Integration Path

### Option 1: OpenAI SDK Drop-in Replacement (Recommended)

Gradient provides an **OpenAI-compatible endpoint**, so you can use your existing Langfuse integration with minimal changes.

#### Step 1: Get Gradient API Key

```bash
# On your droplet or local machine
doctl auth init  # If not already authenticated
doctl serverless functions create --namespace gradient-ns
doctl serverless activations list  # Get your Gradient API key
```

Or via DigitalOcean Console:

1. Navigate to **AI/ML ‚Üí Gradient**
2. Click **Get Started**
3. Copy your **API Key** (format: `do-api-XXXXXXXX`)

#### Step 2: Update Environment Configuration

```bash
# config/env/.env (add these lines)
GRADIENT_API_KEY=do-api-XXXXXXXXXXXXXXXX
GRADIENT_BASE_URL=https://api.digitalocean.com/v2/ai/chat/completions
GRADIENT_MODEL=llama-3.1-8b-instruct  # or mistral-7b-instruct
```

#### Step 3: Update Agent Code (Zero Code Changes with Langfuse)

```python
# agents/feature-dev/main.py
from langfuse.openai import openai
import os

# Configure for DigitalOcean Gradient
client = openai.OpenAI(
    api_key=os.getenv("GRADIENT_API_KEY"),
    base_url=os.getenv("GRADIENT_BASE_URL", "https://api.digitalocean.com/v2/ai")
)

@app.post("/implement")
async def implement_feature(request: FeatureRequest):
    # Same code as OpenAI - Langfuse still traces automatically!
    response = client.chat.completions.create(
        model=os.getenv("GRADIENT_MODEL", "llama-3.1-8b-instruct"),
        messages=[
            {"role": "system", "content": "You are an expert software engineer."},
            {"role": "user", "content": request.description}
        ],
        temperature=0.7,
        max_tokens=2000
    )

    return {
        "code": response.choices[0].message.content,
        "model": "gradient-llama-3.1-8b",
        "tokens": response.usage.total_tokens
    }
```

**That's it!** Langfuse tracing still works because `langfuse.openai` wraps the client.

#### Step 4: Update Docker Compose

```yaml
# compose/docker-compose.yml
services:
  feature-dev:
    environment:
      - GRADIENT_API_KEY=${GRADIENT_API_KEY}
      - GRADIENT_BASE_URL=https://api.digitalocean.com/v2/ai
      - GRADIENT_MODEL=llama-3.1-8b-instruct
      # Keep Langfuse for tracing
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_HOST=https://us.cloud.langfuse.com
```

#### Step 5: Deploy

```bash
# On droplet
cd /opt/Dev-Tools
docker-compose build feature-dev code-review documentation
docker-compose up -d

# Test
curl -X POST http://45.55.173.72:8002/implement \
  -H "Content-Type: application/json" \
  -d '{"description": "Create a user login endpoint", "requirements": ["JWT", "bcrypt"]}'
```

---

## üìä Model Options & Use Cases

| Model                    | Size | Best For          | Speed   | Cost   |
| ------------------------ | ---- | ----------------- | ------- | ------ |
| `llama-3.1-8b-instruct`  | 8B   | General code/text | Fast    | Low    |
| `llama-3.1-70b-instruct` | 70B  | Complex reasoning | Slower  | Higher |
| `mistral-7b-instruct`    | 7B   | Fast inference    | Fastest | Lowest |
| `codellama-13b-instruct` | 13B  | Code-specific     | Medium  | Medium |

**Recommendation by Agent:**

```yaml
orchestrator: llama-3.1-70b-instruct # Complex task decomposition
feature-dev: codellama-13b-instruct # Code generation
code-review: llama-3.1-70b-instruct # Deep analysis
infrastructure: llama-3.1-8b-instruct # IaC templates
cicd: llama-3.1-8b-instruct # Pipeline configs
documentation: mistral-7b-instruct # Fast doc generation
```

---

## üîß Complete Implementation Script

```powershell
# scripts/setup-gradient.ps1
$ErrorActionPreference = "Stop"

Write-Host "üöÄ Setting up DigitalOcean Gradient AI Platform..." -ForegroundColor Cyan

# Validate environment
if (-not $env:GRADIENT_API_KEY) {
    Write-Host "‚ùå Error: GRADIENT_API_KEY not set" -ForegroundColor Red
    Write-Host "Get your key from: https://cloud.digitalocean.com/ai" -ForegroundColor Yellow
    exit 1
}

# Add Gradient config to .env
$envFile = "config/env/.env"
$gradientConfig = @"

# DigitalOcean Gradient AI Platform
GRADIENT_API_KEY=$env:GRADIENT_API_KEY
GRADIENT_BASE_URL=https://api.digitalocean.com/v2/ai
GRADIENT_MODEL=llama-3.1-8b-instruct
"@

if (-not (Select-String -Path $envFile -Pattern "GRADIENT_API_KEY" -Quiet)) {
    Add-Content -Path $envFile -Value $gradientConfig
    Write-Host "‚úì Added Gradient config to .env" -ForegroundColor Green
}

# Update agent-specific models (optional per-agent optimization)
$agentModels = @{
    "orchestrator" = "llama-3.1-70b-instruct"
    "feature-dev" = "codellama-13b-instruct"
    "code-review" = "llama-3.1-70b-instruct"
    "infrastructure" = "llama-3.1-8b-instruct"
    "cicd" = "llama-3.1-8b-instruct"
    "documentation" = "mistral-7b-instruct"
}

$composeFile = "compose/docker-compose.yml"
$composeContent = Get-Content $composeFile -Raw

foreach ($agent in $agentModels.Keys) {
    $model = $agentModels[$agent]
    Write-Host "  Setting $agent ‚Üí $model" -ForegroundColor White

    # Add GRADIENT_MODEL env var per agent (if not already present)
    # This is optional - could also use shared GRADIENT_MODEL
}

Write-Host "`n‚úì Gradient AI Platform configured!" -ForegroundColor Green
Write-Host "`nNext steps:" -ForegroundColor Cyan
Write-Host "1. Rebuild agents: docker-compose build" -ForegroundColor White
Write-Host "2. Restart services: docker-compose up -d" -ForegroundColor White
Write-Host "3. Test inference: curl http://localhost:8002/implement ..." -ForegroundColor White
Write-Host "4. Monitor traces: https://us.cloud.langfuse.com" -ForegroundColor White
```

---

## üîÑ Alternative: Native Gradient SDK (More Control)

If you need features beyond OpenAI compatibility:

```python
# agents/feature-dev/main.py
import httpx
from langfuse import observe

GRADIENT_API_KEY = os.getenv("GRADIENT_API_KEY")
GRADIENT_URL = "https://api.digitalocean.com/v2/ai/chat/completions"

@observe(name="gradient-inference")  # Langfuse still traces the function
async def call_gradient_llm(prompt: str, model: str = "llama-3.1-8b-instruct"):
    """Direct Gradient API call with custom parameters."""

    async with httpx.AsyncClient() as client:
        response = await client.post(
            GRADIENT_URL,
            headers={
                "Authorization": f"Bearer {GRADIENT_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": "You are a code generator."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 2000,
                # Gradient-specific params
                "top_p": 0.9,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0
            },
            timeout=30.0
        )

    result = response.json()

    # Manually log to Langfuse (if not using observe decorator on endpoint)
    return {
        "content": result["choices"][0]["message"]["content"],
        "tokens": result["usage"]["total_tokens"],
        "model": model
    }
```

---

## üí∞ Cost Comparison

| Service      | Model         | Cost/1M Tokens | Cold Start | Latency (from DO droplet) |
| ------------ | ------------- | -------------- | ---------- | ------------------------- |
| **Gradient** | Llama 3.1 8B  | $0.20          | ~2-5s      | **<50ms** (same DC)       |
| **OpenAI**   | GPT-4         | $30.00         | None       | ~200-500ms (internet)     |
| **OpenAI**   | GPT-3.5 Turbo | $1.50          | None       | ~100-300ms (internet)     |

**Savings Example:**

- 10M tokens/month on Gradient Llama 3.1 8B: **$2/month**
- Same on OpenAI GPT-3.5 Turbo: **$15/month**
- Same on OpenAI GPT-4: **$300/month**

---

## üéØ Recommended Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Droplet (45.55.173.72)                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                      ‚îÇ
‚îÇ  FastAPI Agents                                      ‚îÇ
‚îÇ  ‚îú‚îÄ orchestrator:8001                                ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Model: llama-3.1-70b (complex reasoning)     ‚îÇ
‚îÇ  ‚îú‚îÄ feature-dev:8002                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Model: codellama-13b (code generation)       ‚îÇ
‚îÇ  ‚îú‚îÄ code-review:8003                                 ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Model: llama-3.1-70b (deep analysis)         ‚îÇ
‚îÇ  ‚îú‚îÄ infrastructure:8004                              ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Model: llama-3.1-8b (fast IaC)               ‚îÇ
‚îÇ  ‚îú‚îÄ cicd:8005                                        ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Model: llama-3.1-8b (pipeline configs)       ‚îÇ
‚îÇ  ‚îî‚îÄ documentation:8006                               ‚îÇ
‚îÇ     ‚îî‚îÄ Model: mistral-7b (fast docs)                ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ         ‚Üì langfuse.openai wrapper                   ‚îÇ
‚îÇ         ‚Üì (traces all calls)                        ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ HTTPS (within DO network)
           ‚îÇ <50ms latency
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DigitalOcean Gradient AI Platform                  ‚îÇ
‚îÇ  (Managed GPU Inference)                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Llama 3.1 8B/70B                                 ‚îÇ
‚îÇ  ‚Ä¢ CodeLlama 13B                                     ‚îÇ
‚îÇ  ‚Ä¢ Mistral 7B                                        ‚îÇ
‚îÇ  ‚Ä¢ Pay-per-use (no idle GPU costs)                  ‚îÇ
‚îÇ  ‚Ä¢ Auto-scaling                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Traces exported
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Langfuse (us.cloud.langfuse.com)                   ‚îÇ
‚îÇ  ‚Ä¢ Token usage tracking                              ‚îÇ
‚îÇ  ‚Ä¢ Cost analytics                                    ‚îÇ
‚îÇ  ‚Ä¢ Latency monitoring                                ‚îÇ
‚îÇ  ‚Ä¢ Prompt/completion history                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Implementation Checklist

```bash
# 1. Get Gradient API key
doctl auth init
# Or visit: https://cloud.digitalocean.com/ai

# 2. Add to .env
echo "GRADIENT_API_KEY=do-api-XXXXXXXX" >> config/env/.env
echo "GRADIENT_BASE_URL=https://api.digitalocean.com/v2/ai" >> config/env/.env
echo "GRADIENT_MODEL=llama-3.1-8b-instruct" >> config/env/.env

# 3. Update docker-compose.yml (add GRADIENT_* env vars)

# 4. Update agent code (change base_url to Gradient)

# 5. Deploy
cd /opt/Dev-Tools
docker-compose build
docker-compose up -d

# 6. Verify
curl http://45.55.173.72:8002/health
curl -X POST http://45.55.173.72:8002/implement \
  -H "Content-Type: application/json" \
  -d '{"description": "test", "requirements": []}'

# 7. Monitor traces in Langfuse
open https://us.cloud.langfuse.com
```

---

## üîç Why Gradient > OpenAI for Your Setup

| Factor               | Gradient                     | OpenAI                |
| -------------------- | ---------------------------- | --------------------- |
| **Network latency**  | <50ms (same datacenter)      | 200-500ms (internet)  |
| **Egress costs**     | $0 (within DO)               | $0.01/GB (can add up) |
| **Cost per token**   | ~10x cheaper                 | Industry standard     |
| **Data sovereignty** | Stays in your DO account     | Leaves DO network     |
| **Cold start**       | 2-5s (first call)            | None (always warm)    |
| **Model selection**  | Open-source (Llama, Mistral) | Proprietary (GPT)     |
| **Customization**    | Can fine-tune                | Limited               |

---

## üö® Gotchas & Solutions

### Issue 1: Cold Starts

**Problem:** First request to each model takes 2-5 seconds
**Solution:** Implement health check that pings models every 5 minutes

```python
# agents/_shared/gradient_warmup.py
import asyncio
from langfuse.openai import openai

async def keep_warm():
    """Ping Gradient models to prevent cold starts."""
    while True:
        try:
            await openai.chat.completions.create(
                model="llama-3.1-8b-instruct",
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=5
            )
        except Exception as e:
            print(f"Warmup failed: {e}")
        await asyncio.sleep(300)  # Every 5 minutes
```

### Issue 2: Rate Limits

**Problem:** Gradient may have lower rate limits than OpenAI
**Solution:** Implement exponential backoff + queue

```python
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def call_gradient_with_retry(prompt: str):
    return await client.chat.completions.create(...)
```

### Issue 3: Token Limits

**Problem:** Some Gradient models have lower context windows
**Solution:** Chunk large prompts or use streaming

```python
if len(prompt) > 4000:
    # Split into chunks or use llama-3.1-70b (32k context)
    model = "llama-3.1-70b-instruct"
```

---

## üéâ Expected Results

After integration:

- ‚úÖ **<50ms LLM latency** (down from 200-500ms with OpenAI)
- ‚úÖ **10x cost reduction** (Llama 3.1 8B vs GPT-3.5)
- ‚úÖ **Langfuse traces still work** (zero code changes)
- ‚úÖ **Data sovereignty** (never leaves DigitalOcean)
- ‚úÖ **Prometheus metrics** (request counts, latencies via FastAPI instrumentator)

---

## üìö Next Steps

1. **Run setup script**: `./scripts/setup-gradient.ps1`
2. **Test one agent first**: Start with `feature-dev` on `codellama-13b`
3. **Monitor in Langfuse**: Check token usage, costs, latencies
4. **Roll out to all agents**: Apply per-agent model optimization
5. **Add warmup cron**: Prevent cold starts with periodic pings
6. **Document model selection**: Update agent READMEs with model rationale

**Estimated Time:** 30 minutes to first working agent üöÄ

---
