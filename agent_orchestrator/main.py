"""
DevOps Orchestrator Agent

Primary Role: Task delegation, context routing, and workflow coordination
- Analyzes incoming development requests and decomposes them into discrete subtasks
- Routes tasks to appropriate worker agents based on MECE responsibility boundaries
- Maintains task registry mapping request types to specialized agent capabilities
- Tracks task completion status and triggers hand-offs between agents

"""

import asyncio
import json
import logging
import os
import secrets
import uuid
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from fastapi import Depends, FastAPI, HTTPException, Request, Security
from fastapi.responses import StreamingResponse
from fastapi.security import APIKeyHeader
from langsmith import traceable
from lib.event_bus import Event, get_event_bus
from lib.github_permalink_generator import enrich_markdown_with_permalinks_stateless
from lib.guardrail import GuardrailOrchestrator, GuardrailReport, GuardrailStatus
from lib.hitl_manager import get_hitl_manager
from lib.intent_recognizer import IntentType, get_intent_recognizer, intent_to_task

# HybridMemory removed - deprecated langchain_memory.py replaced by agent_memory.py
from lib.langgraph_base import (
    BaseAgentState,
    create_workflow_config,
    get_postgres_checkpointer,
)
from lib.linear_client import get_linear_client
from lib.linear_project_manager import get_project_manager
from lib.llm_client import LLMClient, get_llm_client
from lib.mcp_client import MCPClient, resolve_manifest_path
from lib.mcp_discovery import get_mcp_discovery
from lib.mcp_tool_client import get_mcp_tool_client
from lib.notifiers import (
    EmailConfig,
    EmailNotifier,
    LinearWorkspaceNotifier,
    NotificationConfig,
)
from lib.progressive_mcp_loader import (
    ProgressiveMCPLoader,
    ToolLoadingStrategy,
    get_progressive_loader,
)
from lib.qdrant_client import get_qdrant_client
from lib.registry_client import AgentCapability, RegistryClient
from lib.risk_assessor import get_risk_assessor
from lib.session_manager import get_session_manager
from prometheus_client import Counter, Histogram

try:
    from prometheus_fastapi_instrumentator import Instrumentator

    INSTRUMENTATOR_AVAILABLE = True
except ImportError:
    import logging as _logging

    _logging.warning(
        "prometheus-fastapi-instrumentator not installed, metrics disabled"
    )
    INSTRUMENTATOR_AVAILABLE = False
    Instrumentator = None  # type: ignore

from pydantic import BaseModel, Field
from starlette.middleware.base import BaseHTTPMiddleware

try:
    from lib.dependency_handler import (
        DependencyErrorHandler,
        DependencyRemediationResult,
        get_dependency_handler,
    )
except ImportError:
    # Fallback for local development with shared.lib path
    from shared.lib.dependency_handler import (
        DependencyErrorHandler,
        DependencyRemediationResult,
        get_dependency_handler,
    )

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lifespan event handler for agent registry
from contextlib import asynccontextmanager


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan event handler for startup/shutdown"""
    # Validate LangSmith tracing configuration
    langsmith_tracing = os.getenv("LANGCHAIN_TRACING_V2", "false").lower() == "true"
    langsmith_project = os.getenv("LANGCHAIN_PROJECT")
    langsmith_api_key = os.getenv("LANGCHAIN_API_KEY")

    if langsmith_tracing:
        if not langsmith_project:
            logger.warning("[Tracing] LANGCHAIN_PROJECT not set, using default project")
        if not langsmith_api_key:
            logger.error(
                "[Tracing] LANGCHAIN_TRACING_V2=true but LANGCHAIN_API_KEY not set!"
            )
            raise RuntimeError("LangSmith tracing enabled but API key missing")

        # Test LangSmith connectivity
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    "https://api.smith.langchain.com/health",
                    headers={"x-api-key": langsmith_api_key},
                    timeout=5.0,
                )
                if response.status_code == 200:
                    logger.info(
                        f"[Tracing] ‚úì LangSmith connected (project: {langsmith_project})"
                    )
                else:
                    logger.warning(
                        f"[Tracing] LangSmith health check failed: {response.status_code}"
                    )
        except Exception as e:
            logger.warning(f"[Tracing] Could not verify LangSmith connectivity: {e}")
    else:
        logger.info("[Tracing] LangSmith tracing disabled")

    # Startup: Register with agent registry
    registry_url = os.getenv("AGENT_REGISTRY_URL", "http://agent-registry:8009")
    agent_id = "orchestrator"
    agent_name = "Orchestrator Agent"
    base_url = f"http://orchestrator:{os.getenv('PORT', '8001')}"

    global registry_client
    registry_client = RegistryClient(
        registry_url=registry_url,
        agent_id=agent_id,
        agent_name=agent_name,
        base_url=base_url,
    )

    # Define capabilities
    capabilities = [
        AgentCapability(
            name="orchestrate_task",
            description="Decompose and route complex development tasks",
            parameters={"task_description": "str"},
            cost_estimate="~50-100 tokens",
            tags=["coordination", "routing", "workflow"],
        ),
        AgentCapability(
            name="chat_interface",
            description="Natural language task submission and conversation",
            parameters={"message": "str", "session_id": "str"},
            cost_estimate="~30-80 tokens",
            tags=["chat", "nlp", "conversation"],
        ),
        AgentCapability(
            name="hitl_approval",
            description="Human-in-the-loop approval workflow management",
            parameters={"action": "str", "context": "dict"},
            cost_estimate="~20 tokens",
            tags=["approval", "hitl", "workflow"],
        ),
    ]

    # Register and start heartbeat
    try:
        await registry_client.register(capabilities)
        await registry_client.start_heartbeat()
        logger.info(f"‚úÖ Registered {agent_id} with agent registry")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to register with agent registry: {e}")

    # Connect to Event Bus
    try:
        await event_bus.connect()
        logger.info("‚úÖ Connected to Event Bus")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to connect to Event Bus: {e}")

    # Start HITL approval polling task (fallback for missed webhooks)
    import asyncio
    import random

    # Sampling rate for HITL polling traces (0.1 = 10% of traces logged)
    HITL_POLLING_SAMPLE_RATE = float(os.getenv("HITL_POLLING_SAMPLE_RATE", "0.1"))

    async def poll_pending_approvals():
        """Fallback: runs every 30s to catch missed webhooks.

        Uses LangSmith sampling to reduce trace volume for background polling.
        Only HITL_POLLING_SAMPLE_RATE (default 10%) of polls are traced.
        """
        while True:
            try:
                await asyncio.sleep(30)  # Poll every 30 seconds

                # Sampling: only trace a fraction of polling iterations
                should_trace = random.random() < HITL_POLLING_SAMPLE_RATE

                async with await hitl_manager._get_connection() as conn:
                    async with conn.cursor() as cursor:
                        # Find approved requests that haven't been resumed yet
                        await cursor.execute(
                            """
                            SELECT id, thread_id, checkpoint_id, workflow_id
                            FROM approval_requests 
                            WHERE status = 'approved' AND resumed_at IS NULL
                            AND updated_at > NOW() - INTERVAL '5 minutes'
                            """
                        )
                        pending = await cursor.fetchall()

                        # Log poll results only when sampled (reduces log volume)
                        if should_trace and pending:
                            logger.debug(
                                f"[HITL Polling] Sampled poll found {len(pending)} pending approvals"
                            )

                        for req_id, thread_id, checkpoint_id, workflow_id in pending:
                            # Always log actual resumptions (important events)
                            logger.info(
                                f"[HITL Polling] Found unresumed approval: {req_id}"
                            )
                            try:
                                from graph import app as workflow_app
                                from langchain_core.messages import HumanMessage

                                if thread_id:
                                    config = {"configurable": {"thread_id": thread_id}}
                                    resume_message = HumanMessage(
                                        content="HITL approval granted (via polling). Resuming workflow."
                                    )

                                    await workflow_app.ainvoke(  # type: ignore
                                        {"messages": [resume_message]},
                                        config=config,
                                    )

                                    # Mark as resumed
                                    await cursor.execute(
                                        "UPDATE approval_requests SET resumed_at = NOW() WHERE id = %s",
                                        (req_id,),
                                    )
                                    await conn.commit()
                                    logger.info(
                                        f"[HITL Polling] Resumed workflow for {req_id}"
                                    )
                            except Exception as e:
                                logger.error(
                                    f"[HITL Polling] Failed to resume {req_id}: {e}"
                                )

            except Exception as e:
                logger.error(f"[HITL Polling] Error in polling loop: {e}")

    # Start polling task in background
    hitl_polling_task = asyncio.create_task(poll_pending_approvals())
    logger.info("‚úÖ Started HITL approval polling task")

    yield

    # Shutdown: Cancel polling task
    hitl_polling_task.cancel()
    try:
        await hitl_polling_task
    except asyncio.CancelledError:
        pass
    logger.info("üõë Stopped HITL approval polling task")

    # Shutdown: Stop heartbeat
    try:
        await registry_client.stop_heartbeat()
        await registry_client.close()
        logger.info(f"üõë Unregistered {agent_id} from agent registry")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Failed to unregister from agent registry: {e}")


# =============================================================================
# API KEY AUTHENTICATION
# =============================================================================
# Secure the orchestrator API with API key authentication.
# - Set ORCHESTRATOR_API_KEY in .env to enable authentication
# - If not set, authentication is disabled (development mode)
# - Public endpoints: /health, /ready, /metrics, /docs, /openapi.json
# =============================================================================

# API Key configuration
ORCHESTRATOR_API_KEY = os.getenv("ORCHESTRATOR_API_KEY")
API_KEY_ENABLED = bool(ORCHESTRATOR_API_KEY)

# Public endpoints that don't require authentication
PUBLIC_PATHS = {
    "/health",
    "/ready",
    "/metrics",
    "/docs",
    "/openapi.json",
    "/redoc",
    "/",  # Root path (if exists)
}

# API Key header definition
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)


class APIKeyMiddleware(BaseHTTPMiddleware):
    """Middleware to validate API key for protected endpoints."""

    async def dispatch(self, request: Request, call_next):
        # Skip authentication if API key not configured (dev mode)
        if not API_KEY_ENABLED:
            return await call_next(request)

        # Allow public endpoints without authentication
        path = request.url.path.rstrip("/")
        if path in PUBLIC_PATHS or path == "":
            return await call_next(request)

        # Check X-API-Key header
        api_key = request.headers.get("X-API-Key")
        if not api_key:
            # Also check Authorization header (Bearer token)
            auth_header = request.headers.get("Authorization")
            if auth_header and auth_header.startswith("Bearer "):
                api_key = auth_header[7:]

        # Validate API key using constant-time comparison
        if not api_key or not secrets.compare_digest(
            api_key, ORCHESTRATOR_API_KEY or ""
        ):
            client_host = request.client.host if request.client else "unknown"
            logger.warning(
                f"üîí Unauthorized API request to {request.url.path} from {client_host}"
            )
            from fastapi.responses import JSONResponse

            return JSONResponse(
                status_code=401,
                content={"detail": "Invalid or missing API key"},
                headers={"WWW-Authenticate": "X-API-Key"},
            )

        return await call_next(request)


if API_KEY_ENABLED:
    logger.info("üîê API key authentication ENABLED")
else:
    logger.warning(
        "‚ö†Ô∏è  API key authentication DISABLED - set ORCHESTRATOR_API_KEY to enable"
    )

app = FastAPI(
    title="DevOps Orchestrator Agent",
    description="Task delegation, context routing, and workflow coordination",
    version="1.0.0",
    lifespan=lifespan,
)

# Add API key authentication middleware
app.add_middleware(APIKeyMiddleware)

# Enable Prometheus metrics collection
if INSTRUMENTATOR_AVAILABLE and Instrumentator:
    Instrumentator().instrument(app).expose(app)
    logger.info("Prometheus instrumentation enabled")
else:
    logger.warning("Prometheus instrumentation disabled")

# State Persistence Layer URL
STATE_SERVICE_URL = os.getenv("STATE_SERVICE_URL", "http://state-persistence:8008")

# Shared MCP client for tool access and telemetry
mcp_client = MCPClient(agent_name="orchestrator")

# Guardrail orchestrator for compliance checks
guardrail_orchestrator = GuardrailOrchestrator()

# MCP discovery for real-time server enumeration
mcp_discovery = get_mcp_discovery()

# Linear client for issue tracking and project management
linear_client = get_linear_client()

# MCP tool client for direct tool invocation (replaces HTTP gateway calls)
mcp_tool_client = get_mcp_tool_client("orchestrator")

# Progressive MCP loader for token-optimized tool disclosure
progressive_loader = get_progressive_loader(mcp_client, mcp_discovery)

# Risk evaluation + HITL orchestration
risk_assessor = get_risk_assessor()
hitl_manager = get_hitl_manager()

# LLM client for orchestrator operations
llm_client = get_llm_client("orchestrator")

# Chat interface components (Phase 5)
intent_recognizer = get_intent_recognizer(llm_client)
session_manager = get_session_manager()

# Event bus for notifications (Phase 5.2)
event_bus = get_event_bus()

# State client for workflow event sourcing
from lib.state_client import get_state_client

state_client = get_state_client()

# Workflow router for smart workflow selection
from workflows.workflow_router import get_workflow_router

# Agent registry client (Phase 6)
registry_client: Optional[RegistryClient] = None

# RAG service for vendor documentation context (Phase 7)
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "http://rag-context:8007")
RAG_TIMEOUT = int(os.getenv("RAG_TIMEOUT", "10"))

# Initialize notifiers
linear_notifier = LinearWorkspaceNotifier(agent_name="orchestrator")
email_notifier = EmailNotifier()

# Subscribe notifiers to event bus
event_bus.subscribe("approval_required", linear_notifier.on_approval_required)
event_bus.subscribe("approval_required", email_notifier.on_approval_required)

logger.info("Notification system initialized (Linear + Email)")

# Approval workflow metrics
approval_requests_total = Counter(
    "orchestrator_approval_requests_total",
    "Total approval requests triggered by the orchestrator",
    ["risk_level"],
)

approval_wait_time = Histogram(
    "orchestrator_approval_wait_seconds",
    "Time spent waiting for human approval before resuming workflows",
    ["risk_level"],
)

approval_decisions_total = Counter(
    "orchestrator_approval_decisions_total",
    "Total approval decisions made (approved/rejected)",
    ["decision", "risk_level"],
)

approval_expirations_total = Counter(
    "orchestrator_approval_expirations_total",
    "Total approval requests that expired without decision",
    ["risk_level"],
)

# RAG context metrics
rag_context_injected_total = Counter(
    "orchestrator_rag_context_injected_total",
    "Total tasks with RAG vendor context injected into LLM prompts",
    ["source"],
)

rag_vendor_keywords_detected = Counter(
    "orchestrator_rag_vendor_keywords_detected_total",
    "Total vendor keywords detected in task descriptions",
    ["keyword"],
)

rag_query_latency = Histogram(
    "orchestrator_rag_query_seconds",
    "RAG service query latency in seconds",
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0],
)

# Intent recognition metrics by mode
intent_recognition_total = Counter(
    "orchestrator_intent_recognition_total",
    "Total intent recognition attempts",
    ["session_mode", "intent_type", "mode_hint_source"],
)

intent_recognition_confidence = Histogram(
    "orchestrator_intent_recognition_confidence",
    "Intent recognition confidence scores",
    ["session_mode", "mode_hint_source"],
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
)

mode_switch_total = Counter(
    "orchestrator_mode_switch_total",
    "Total mode switches per session",
    ["from_mode", "to_mode"],
)

# Track approval-pending tasks awaiting resumption
pending_approval_registry: Dict[str, Dict[str, Any]] = {}

# LangGraph infrastructure
try:
    checkpointer = get_postgres_checkpointer()
    logger.info("LangGraph PostgreSQL checkpointer initialized")
except Exception as e:
    logger.warning(f"LangGraph checkpointer not available: {e}")
    checkpointer = None

# Qdrant Cloud client for vector operations
qdrant_client = get_qdrant_client()
if qdrant_client.is_enabled():
    logger.info("Qdrant Cloud client initialized")
else:
    logger.warning("Qdrant Cloud not configured")

# Hybrid memory deprecated - agents use AgentMemoryManager from agent_memory.py
# Memory accessed via RAG service HTTP endpoints for centralized management


# Agent types for task routing
class AgentType(str, Enum):
    FEATURE_DEV = "feature-dev"
    CODE_REVIEW = "code-review"
    INFRASTRUCTURE = "infrastructure"
    CICD = "cicd"
    DOCUMENTATION = "documentation"


# Task status tracking
class TaskStatus(str, Enum):
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"


# Request models
class TaskRequest(BaseModel):
    """Incoming development request"""

    description: str = Field(
        ..., description="Natural language description of the task"
    )
    project_context: Optional[Dict[str, Any]] = Field(
        default=None, description="Project context references"
    )
    workspace_config: Optional[Dict[str, Any]] = Field(
        default=None, description="Workspace configuration"
    )
    priority: Optional[str] = Field(default="medium", description="Task priority")


class SubTask(BaseModel):
    """Decomposed subtask for routing"""

    id: str
    agent_type: AgentType
    description: str
    context_refs: Optional[List[str]] = None
    dependencies: Optional[List[str]] = None
    status: TaskStatus = TaskStatus.PENDING
    created_at: datetime = Field(default_factory=datetime.utcnow)


class TaskResponse(BaseModel):
    """Orchestration response"""

    task_id: str
    subtasks: List[SubTask]
    routing_plan: Dict[str, Any]
    estimated_tokens: int
    guardrail_report: GuardrailReport
    workspace_context: Optional[Dict[str, Any]] = (
        None  # Workspace context from extension
    )
    linear_project: Optional[Dict[str, str]] = None  # Linear project info for caching


# In-memory task registry (in production, this would use State Persistence Layer)
task_registry: Dict[str, TaskResponse] = {}

# ============================================================================
# HITL Risk Assessment Helper Functions
# ============================================================================

RISK_OPERATION_KEYWORDS = {
    "delete": ["delete", "drop", "destroy", "remove", "truncate", "purge"],
    "deploy": ["deploy", "release", "ship", "roll out", "rollout"],
    "modify": ["modify", "change", "update", "patch", "tweak", "adjust"],
    "create": ["create", "provision", "spin up", "add", "bootstrap"],
    "migrate": ["migrate", "move", "transition", "rehost"],
}


def extract_operation_from_description(description: str) -> str:
    description_lower = description.lower()
    for operation, keywords in RISK_OPERATION_KEYWORDS.items():
        if any(keyword in description_lower for keyword in keywords):
            return operation
    return "modify"


def infer_environment(
    description: str, project_context: Optional[Dict[str, Any]]
) -> str:
    if project_context:
        env = project_context.get("environment") or project_context.get("env")
        if isinstance(env, str) and env:
            return env.lower()

    description_lower = description.lower()
    if any(term in description_lower for term in ["production", "prod"]):
        return "production"
    if any(term in description_lower for term in ["staging", "stage"]):
        return "staging"
    if any(term in description_lower for term in ["qa", "test"]):
        return "qa"
    return "dev"


RESOURCE_KEYWORDS = [
    ("database", ["database", "db", "table", "schema", "postgres", "mysql", "qdrant"]),
    (
        "infrastructure",
        [
            "infrastructure",
            "cluster",
            "kubernetes",
            "k8s",
            "terraform",
            "docker",
            "server",
            "network",
            "firewall",
            "load balancer",
        ],
    ),
    (
        "pipeline",
        ["pipeline", "ci/cd", "workflow", "github actions", "gitlab", "deployment"],
    ),
    ("secret", ["secret", "token", "credential", "password", "key", "certificate"]),
    ("data", ["data", "dataset", "export", "import", "backup"]),
    ("application", ["service", "api", "app", "frontend", "backend"]),
]


def infer_resource_type(description: str) -> str:
    description_lower = description.lower()
    for resource, keywords in RESOURCE_KEYWORDS:
        if any(keyword in description_lower for keyword in keywords):
            return resource
    return "code"


SENSITIVE_KEYWORDS = [
    "pii",
    "personally identifiable",
    "customer data",
    "credit card",
    "secret",
    "token",
    "credential",
    "password",
    "ssn",
]


def detect_sensitive_data(description: str) -> bool:
    description_lower = description.lower()
    return any(keyword in description_lower for keyword in SENSITIVE_KEYWORDS)


def estimate_operation_cost(
    description: str, environment: str, resource_type: str
) -> int:
    base_cost = 50 if environment == "production" else 20
    if resource_type in {"infrastructure", "pipeline"}:
        base_cost += 150
    if "cluster" in description.lower() or "autoscale" in description.lower():
        base_cost += 100
    return base_cost


def compile_risk_factors(
    operation: str, environment: str, resource_type: str, description: str
) -> List[str]:
    factors = []
    if environment == "production":
        factors.append("production-environment")
    if operation in {"delete", "deploy"}:
        factors.append(f"operation-{operation}")
    if resource_type in {"database", "secret"}:
        factors.append(f"resource-{resource_type}")
    if "security" in description.lower() or "permission" in description.lower():
        factors.append("security-impact")
    if detect_sensitive_data(description):
        factors.append("sensitive-data")
    return factors


def build_risk_context(request: TaskRequest) -> Dict[str, Any]:
    """Derive operation metadata for risk assessment and approval workflows."""
    description = request.description
    operation = extract_operation_from_description(description)
    environment = infer_environment(description, request.project_context)
    resource_type = infer_resource_type(description)
    data_sensitive = detect_sensitive_data(description)
    estimated_cost = estimate_operation_cost(description, environment, resource_type)
    risk_factors = compile_risk_factors(
        operation, environment, resource_type, description
    )

    return {
        "operation": operation,
        "environment": environment,
        "resource_type": resource_type,
        "description": description,
        "priority": request.priority,
        "project_context": request.project_context or {},
        "workspace_config": request.workspace_config or {},
        "estimated_cost": estimated_cost,
        "data_sensitive": data_sensitive,
        "risk_factors": risk_factors,
        "impact": (
            "high"
            if environment == "production"
            else "medium" if environment == "staging" else "low"
        ),
        "details": {
            "workspace_config": request.workspace_config or {},
            "project_context": request.project_context or {},
        },
    }


async def query_vendor_context(
    task_description: str, n_results: int = 2
) -> Optional[str]:
    """
    Query RAG service for relevant vendor documentation context.
    Detects vendor keywords and retrieves relevant documentation chunks.

    Args:
        task_description: Task description to analyze
        n_results: Number of results to retrieve (default: 2)

    Returns:
        Formatted context string or None if no relevant context found
    """
    # Vendor keywords that trigger RAG lookup
    vendor_keywords = [
        "gradient",
        "gradient ai",
        "digitalocean",
        "linear",
        "graphql",
        "linear api",
        "langsmith",
        "langchain",
        "langgraph",
        "qdrant",
        "vector",
        "embedding",
        "streaming",
        "serverless inference",
    ]

    # Check if task mentions any vendor keywords
    description_lower = task_description.lower()
    detected_keywords = [kw for kw in vendor_keywords if kw in description_lower]

    if not detected_keywords:
        return None

    # Track keyword detection
    for keyword in detected_keywords:
        rag_vendor_keywords_detected.labels(keyword=keyword).inc()

    try:
        import time

        start_time = time.time()

        async with httpx.AsyncClient(timeout=RAG_TIMEOUT) as client:
            response = await client.post(
                f"{RAG_SERVICE_URL}/query",
                json={
                    "query": task_description,
                    "collection": "vendor-docs",
                    "n_results": n_results,
                },
            )
            response.raise_for_status()
            result = response.json()

            # Record query latency
            query_time = time.time() - start_time
            rag_query_latency.observe(query_time)

            if not result.get("results"):
                return None

            # Format context for LLM
            context_parts = ["\n--- Relevant Vendor Documentation ---"]
            sources_used = set()

            for i, item in enumerate(result["results"], 1):
                source = item.get("metadata", {}).get("source", "unknown")
                sources_used.add(source)
                score = item.get("relevance_score", 0)
                content = item.get("content", "")[:500]  # Limit to 500 chars per result

                context_parts.append(
                    f"\n[Source {i}: {source} | Relevance: {score:.2f}]"
                )
                context_parts.append(content)

            context_parts.append("\n--- End Vendor Documentation ---\n")

            # Track RAG context injection
            for source in sources_used:
                rag_context_injected_total.labels(source=source).inc()

            logger.info(
                f"[RAG] Retrieved {len(result['results'])} vendor docs (latency: {result.get('retrieval_time_ms', 0):.0f}ms)"
            )

            # Track RAG usage
            await mcp_tool_client.create_memory_entity(
                name=f"rag_context_used_{uuid.uuid4().hex[:8]}",
                entity_type="rag_query",
                observations=[
                    f"Query: {task_description[:100]}",
                    f"Results: {len(result['results'])}",
                    f"Latency: {result.get('retrieval_time_ms', 0):.0f}ms",
                    f"Collection: vendor-docs",
                ],
            )

            return "".join(context_parts)

    except Exception as e:
        logger.warning(f"[RAG] Failed to query vendor context: {e}")
        return None


# ============================================================================
# Agent service endpoints (from docker-compose)
AGENT_ENDPOINTS = {
    AgentType.FEATURE_DEV: os.getenv("FEATURE_DEV_URL", "http://feature-dev:8002"),
    AgentType.CODE_REVIEW: os.getenv("CODE_REVIEW_URL", "http://code-review:8003"),
    AgentType.INFRASTRUCTURE: os.getenv(
        "INFRASTRUCTURE_URL", "http://infrastructure:8004"
    ),
    AgentType.CICD: os.getenv("CICD_URL", "http://cicd:8005"),
    AgentType.DOCUMENTATION: os.getenv(
        "DOCUMENTATION_URL", "http://documentation:8006"
    ),
}


# Agent manifest for tool-aware routing
def load_agent_manifest() -> Dict[str, Any]:
    """Load agent manifest with tool allocations"""
    import json

    manifest_path = resolve_manifest_path()
    try:
        with open(manifest_path, "r", encoding="utf-8-sig") as f:
            return json.load(f)
    except Exception as e:
        print(f"Failed to load agent manifest from {manifest_path}: {e}")
        return {"profiles": []}


AGENT_MANIFEST = load_agent_manifest()


def get_agent_profile(agent_name: str) -> Optional[Dict[str, Any]]:
    """Retrieve agent profile from manifest"""
    for profile in AGENT_MANIFEST.get("profiles", []):
        if profile.get("name") == agent_name:
            return profile
    return None


def get_required_tools_for_task(description: str) -> List[Dict[str, str]]:
    """
    Analyze task description to determine required MCP tools
    Returns list of {server, tool} dictionaries
    """
    description_lower = description.lower()
    required_tools = []

    # File operations
    if any(
        kw in description_lower
        for kw in ["file", "code", "implement", "create", "write", "read"]
    ):
        required_tools.append({"server": "rust-mcp-filesystem", "tool": "write_file"})
        required_tools.append({"server": "rust-mcp-filesystem", "tool": "read_file"})

    # Git operations
    if any(
        kw in description_lower
        for kw in ["commit", "branch", "pull request", "pr", "git"]
    ):
        required_tools.append({"server": "gitmcp", "tool": "create_branch"})
        required_tools.append({"server": "gitmcp", "tool": "commit_changes"})

    # Docker/Container operations
    if any(
        kw in description_lower for kw in ["docker", "container", "image", "deploy"]
    ):
        required_tools.append({"server": "dockerhub", "tool": "list_images"})

    # Documentation operations
    if any(kw in description_lower for kw in ["document", "readme", "doc", "api doc"]):
        required_tools.append({"server": "notion", "tool": "create_page"})

    # Testing operations
    if any(kw in description_lower for kw in ["test", "e2e", "selenium", "playwright"]):
        required_tools.append({"server": "playwright", "tool": "goto"})

    return required_tools


async def check_agent_tool_availability(
    agent_type: AgentType, required_tools: List[Dict[str, str]]
) -> Dict[str, Any]:
    """
    Check if agent has required tools available via manifest
    Returns availability status and missing tools
    """
    agent_name = agent_type.value
    profile = get_agent_profile(agent_name)

    if not profile:
        return {
            "available": False,
            "reason": f"Agent profile not found in manifest: {agent_name}",
            "missing_tools": required_tools,
        }

    recommended_tools = profile.get("mcp_tools", {}).get("recommended", [])
    shared_tools = profile.get("mcp_tools", {}).get("shared", [])

    # Build set of available tools
    available_tool_set = set()
    for tool_entry in recommended_tools:
        server = tool_entry.get("server")
        tools = tool_entry.get("tools", [])
        for tool in tools:
            available_tool_set.add(f"{server}/{tool}")

    # Shared tools have all capabilities (simplified assumption)
    for server in shared_tools:
        available_tool_set.add(f"{server}/*")

    # Check required tools
    missing_tools = []
    for req_tool in required_tools:
        server = req_tool["server"]
        tool = req_tool["tool"]
        tool_key = f"{server}/{tool}"
        wildcard_key = f"{server}/*"

        if (
            tool_key not in available_tool_set
            and wildcard_key not in available_tool_set
        ):
            missing_tools.append(req_tool)

    return {
        "available": len(missing_tools) == 0,
        "reason": (
            f"Missing {len(missing_tools)} required tools"
            if missing_tools
            else "All required tools available"
        ),
        "missing_tools": missing_tools,
        "agent_capabilities": profile.get("capabilities", []),
    }


@app.get("/health")
async def health_check():
    """Health check endpoint - basic liveness check"""
    return {
        "status": "ok",
        "service": "orchestrator",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
    }


@app.get("/ready")
async def readiness_check():
    """Readiness check endpoint - indicates if service is ready to handle traffic"""
    # Check if MCP toolkit is available
    mcp_available = mcp_tool_client._check_mcp_available()

    # Get list of available MCP servers
    available_servers = await mcp_tool_client.list_servers()

    # Check critical dependencies
    openrouter_ready = bool(os.getenv("OPENROUTER_API_KEY"))
    linear_ready = linear_client.is_enabled()

    # Service is ready if MCP is available and at least basic integrations work
    is_ready = mcp_available and openrouter_ready

    return {
        "ready": is_ready,
        "service": "orchestrator",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "1.0.0",
        "mcp": {
            "toolkit_available": mcp_available,
            "available_servers": available_servers,
            "server_count": len(available_servers),
            "access_method": "direct_stdio",
            "recommended_tool_servers": [
                entry.get("server") for entry in mcp_client.recommended_tools
            ],
            "shared_tool_servers": mcp_client.shared_tools,
            "capabilities": mcp_client.capabilities,
        },
        "integrations": {
            "linear": linear_ready,
            "openrouter": openrouter_ready,
            "openai_embeddings": bool(os.getenv("OPENAI_API_KEY")),
        },
        "chat": {
            "enabled": True,
            "endpoint": "/chat",
            "features": [
                "intent_recognition",
                "multi_turn",
                "task_submission",
                "status_query",
                "approval_decision",
            ],
        },
    }


@app.get("/metrics/tokens")
async def get_token_metrics():
    """
    Get real-time token usage statistics and cost attribution.

    Returns aggregated metrics per agent:
    - Token counts (prompt + completion)
    - Total cost in USD
    - Efficiency metrics (avg tokens/call, avg cost/call, avg latency)
    - Model information

    Prometheus metrics also available at /metrics endpoint.
    """
    from lib.token_tracker import token_tracker

    summary = token_tracker.get_summary()

    return {
        "per_agent": summary["per_agent"],
        "totals": summary["totals"],
        "tracking_since": summary["tracking_since"],
        "uptime_seconds": summary["uptime_seconds"],
        "timestamp": datetime.utcnow().isoformat(),
        "note": "Cost calculated from config/agents/models.yaml (cost_per_1m_tokens). See /metrics for Prometheus format.",
    }


# ============================================================================
# Linear Webhook Endpoint for HITL Approvals (Emoji Reactions)
# ============================================================================

from fastapi import Request
from lib.linear_webhook_processor import LinearWebhookProcessor

webhook_processor = LinearWebhookProcessor()


@app.post("/webhooks/linear")
async def linear_webhook(request: Request):
    """
    Handle Linear webhook events for HITL approvals via emoji reactions.

    Processes:
    - üëç reactions (approve workflow)
    - üëé reactions (deny workflow)
    - üí¨ comment replies (request more info)

    Triggered by Linear webhooks on Comment create/update events.
    """
    # Get raw payload for signature verification
    payload = await request.body()

    # Debug: Log all headers
    logger.info(f"Webhook headers: {dict(request.headers)}")

    # Verify webhook signature
    signature = request.headers.get("Linear-Signature")
    if not webhook_processor.verify_signature(signature, payload):
        logger.warning("Invalid Linear webhook signature")
        raise HTTPException(status_code=401, detail="Invalid webhook signature")

    # Parse event data
    try:
        event = await request.json()
    except Exception as e:
        logger.error(f"Failed to parse webhook payload: {e}")
        raise HTTPException(status_code=400, detail="Invalid JSON payload")

    # Log event type for debugging
    event_type = event.get("type")
    action = event.get("action")
    logger.info(f"üì® Received {event_type}.{action} webhook event")

    # Process webhook and get action
    result = await webhook_processor.process_webhook(event)
    logger.info(f"üîÑ Webhook processing result: {result.get('action')}")

    if result["action"] == "resume_workflow":
        metadata = result["metadata"]
        logger.info(
            f"‚úÖ Workflow approved by {metadata['approved_by_name']} - "
            f"Comment: {metadata['comment_url']}"
        )

        # Add confirmation comment to Linear
        try:
            from lib.linear_workspace_client import LinearWorkspaceClient

            linear_client = LinearWorkspaceClient()
            await linear_client.add_comment(
                metadata["issue_id"],
                f"‚úÖ **Approved by @{metadata['approved_by_name']}**\n\n"
                f"Workflow will resume automatically. Thank you for your approval!",
            )
        except Exception as e:
            logger.error(f"Failed to add confirmation comment: {e}")

        # NEW: Resume workflow by matching linear_issue_id
        try:
            from shared.lib.hitl_manager import get_hitl_manager

            hitl_manager = get_hitl_manager()
            async with await hitl_manager._get_connection() as conn:
                async with conn.cursor() as cursor:
                    await cursor.execute(
                        "SELECT id FROM approval_requests WHERE linear_issue_id = %s AND status = 'pending'",
                        (metadata["issue_id"],),
                    )
                    row = await cursor.fetchone()
                    if row:
                        approval_request_id = row[0]
                        # Call workflow resume logic
                        approval_result = await resume_workflow_from_approval(
                            approval_request_id, action="approved"
                        )
                        logger.info(
                            f"‚úÖ Workflow resumed for approval_request_id={approval_request_id}, result={approval_result}"
                        )

                        # Update Linear comment with resume status
                        if approval_result and approval_result.get("resumed"):
                            try:
                                from lib.linear_workspace_client import (
                                    LinearWorkspaceClient,
                                )

                                linear_client = LinearWorkspaceClient()
                                await linear_client.add_comment(
                                    metadata["issue_id"],
                                    f"‚úÖ **Workflow Resumed Successfully**\n\n"
                                    f"- Thread ID: `{approval_result.get('thread_id')}`\n"
                                    f"- Status: {approval_result.get('final_status')}\n"
                                    f"- Approved by: @{metadata['approved_by_name']}",
                                )
                            except Exception as comment_err:
                                logger.error(
                                    f"Failed to update Linear with resume status: {comment_err}"
                                )

                        return {
                            "status": "workflow_resumed",
                            "metadata": metadata,
                            "approval_result": approval_result,
                            "message": "Workflow approved and resumed",
                        }
                    else:
                        logger.warning(
                            f"No pending approval found for linear_issue_id={metadata['issue_id']}"
                        )
                        return {
                            "status": "no_pending_approval",
                            "metadata": metadata,
                            "message": "No pending approval request found for this issue",
                        }
        except Exception as e:
            logger.error(
                f"Failed to resume workflow from Linear approval: {e}", exc_info=True
            )
            return {
                "status": "error",
                "metadata": metadata,
                "error": str(e),
                "message": "Failed to resume workflow",
            }

    elif result["action"] == "cancel_workflow":
        # TODO: Cancel LangGraph workflow
        metadata = result["metadata"]
        logger.info(
            f"‚ùå Workflow denied by {metadata['denied_by_name']} - "
            f"Comment: {metadata['comment_url']}"
        )

        # Add confirmation comment to Linear
        try:
            from lib.linear_workspace_client import LinearWorkspaceClient

            linear_client = LinearWorkspaceClient()
            await linear_client.add_comment(
                metadata["issue_id"],  # Use issue_id, not comment_id
                f"‚ùå **Denied by @{metadata['denied_by_name']}**\n\n"
                f"Workflow has been cancelled. No actions will be taken.",
            )
        except Exception as e:
            logger.error(f"Failed to add confirmation comment: {e}")

        return {
            "status": "workflow_cancelled",
            "metadata": metadata,
            "message": "Workflow denied and cancelled",
        }

    elif result["action"] == "pause_workflow":
        # TODO: Keep workflow paused, await clarification
        metadata = result["metadata"]
        logger.info(
            f"üí¨ More information requested by {metadata['requested_by_name']} - "
            f"Comment: {metadata['comment_url']}"
        )

        return {
            "status": "workflow_paused",
            "metadata": metadata,
            "message": "Workflow paused, awaiting more information",
        }

    # Ignored event (not an approval comment)
    return {"status": "ignored", "message": "Event processed but no action taken"}


# ============================================================================
# Main Orchestration Endpoints
# ============================================================================


@app.post("/orchestrate", response_model=TaskResponse)
@traceable(name="orchestrate_task", tags=["orchestrator", "workflow", "routing"])
async def orchestrate_task(request: TaskRequest):
    """
    Main orchestration endpoint with progressive tool disclosure and workspace-aware context
    - Extracts workspace context (GitHub repo, Linear project)
    - Auto-creates Linear project for new workspaces
    - Enriches descriptions with GitHub permalinks
    - Analyzes request and decomposes into subtasks
    - Progressively loads only relevant MCP tools (10-30 vs 150+)
    - Validates tool availability before routing
    - Routes to appropriate specialized agents
    - Returns routing plan with minimal context pointers

    Token Optimization: Only loads relevant tools per task (80-90% token reduction)
    """
    import uuid

    task_id = str(uuid.uuid4())

    # Extract workspace context from request
    workspace_ctx = request.project_context or {}
    workspace_name = workspace_ctx.get("workspace_name", "unknown")
    github_repo_url = workspace_ctx.get("github_repo_url")
    commit_sha = workspace_ctx.get("github_commit_sha")
    linear_project_id = workspace_ctx.get("linear_project_id")

    logger.info(
        f"[Orchestrator] Task {task_id} - Workspace: {workspace_name}, GitHub: {github_repo_url}, Linear: {linear_project_id}"
    )

    # Get or create Linear project
    try:
        project_manager = get_project_manager()
        project = await project_manager.get_or_create_project(
            workspace_name=workspace_name,
            github_repo_url=github_repo_url,
            project_id=linear_project_id,
        )

        # Update workspace context with resolved project ID
        workspace_ctx["linear_project_id"] = project["id"]
        workspace_ctx["linear_project_name"] = project["name"]

        logger.info(
            f"[Orchestrator] Using Linear project: {project['name']} ({project['id']})"
        )
    except Exception as e:
        logger.error(
            f"[Orchestrator] Failed to get/create Linear project: {e}", exc_info=True
        )
        # Continue without Linear project
        project = {"id": "", "name": workspace_name, "url": ""}

    # Enrich description with permalinks (if GitHub context available)
    original_description = request.description
    if github_repo_url and commit_sha:
        try:
            enriched_description = enrich_markdown_with_permalinks_stateless(
                request.description, repo_url=github_repo_url, commit_sha=commit_sha
            )
            logger.info(
                f"[Orchestrator] Enriched description with GitHub permalinks for {github_repo_url}"
            )
            # Update request with enriched description
            request.description = enriched_description
        except Exception as e:
            logger.warning(
                f"[Orchestrator] Failed to enrich description with permalinks: {e}"
            )
            enriched_description = original_description
    else:
        logger.info(
            "[Orchestrator] No GitHub context available, skipping permalink enrichment"
        )
        enriched_description = original_description

    guardrail_report = await guardrail_orchestrator.run(
        "orchestrator",
        task_id=task_id,
        context={
            "endpoint": "orchestrate",
            "priority": request.priority,
        },
    )

    if (
        guardrail_orchestrator.should_block_failures
        and guardrail_report.status == GuardrailStatus.FAILED
    ):
        raise HTTPException(
            status_code=409,
            detail={
                "error": "Guardrail checks failed",
                "report": guardrail_report.model_dump(mode="json"),
            },
        )

    risk_context = build_risk_context(request)
    risk_level = risk_assessor.assess_task(risk_context)

    if risk_assessor.requires_approval(risk_level):
        approval_request_id = await hitl_manager.create_approval_request(
            workflow_id=task_id,
            thread_id=f"wf-thread-{task_id}",
            checkpoint_id=f"wf-checkpoint-{task_id}",
            task=risk_context,
            agent_name="orchestrator",
        )

        if approval_request_id:
            approval_requests_total.labels(risk_level=risk_level).inc()
            pending_approval_registry[task_id] = {
                "request_payload": request.model_dump(),
                "guardrail_report": guardrail_report,
                "risk_context": risk_context,
                "approval_request_id": approval_request_id,
                "risk_level": risk_level,
                "created_at": datetime.utcnow(),
            }

            # Emit approval_required event for notifications
            await event_bus.emit(
                "approval_required",
                {
                    "approval_id": approval_request_id,
                    "task_description": request.description,
                    "risk_level": risk_level,
                    "project_name": (
                        request.project_context.get("project", "ai-devops-platform")
                        if request.project_context
                        else "ai-devops-platform"
                    ),
                    "metadata": {
                        "task_id": task_id,
                        "priority": request.priority,
                        "agent": "orchestrator",
                        "timestamp": datetime.utcnow().isoformat(),
                    },
                },
                source="orchestrator",
                correlation_id=task_id,
            )
            logger.info(f"Emitted approval_required event for {approval_request_id}")

            response = TaskResponse(
                task_id=task_id,
                subtasks=[],
                routing_plan={
                    "status": "approval_pending",
                    "approval_request_id": approval_request_id,
                    "risk_level": risk_level,
                    "instructions": f"Pending approval. Run: task workflow:approve REQUEST_ID={approval_request_id}",
                    "submitted_at": datetime.utcnow().isoformat(),
                    "risk_context": risk_context,
                },
                estimated_tokens=0,
                guardrail_report=guardrail_report,
            )

            task_registry[task_id] = response
            await persist_task_state(
                task_id, request, response, status="approval_pending"
            )

            await mcp_tool_client.create_memory_entity(
                name=f"task_requires_approval_{task_id}",
                entity_type="orchestrator_event",
                observations=[
                    f"Task ID: {task_id}",
                    f"Risk level: {risk_level}",
                    f"Approval request: {approval_request_id}",
                    f"Priority: {request.priority}",
                ],
            )

            logger.info(
                "[Orchestrator] Task %s requires %s approval (request_id=%s)",
                task_id,
                risk_level,
                approval_request_id,
            )

            return response

        logger.info(
            "[Orchestrator] Risk level %s requested approval but no request ID returned; continuing",
            risk_level,
        )

    return await execute_orchestration_flow(
        task_id, request, guardrail_report, workspace_ctx, project
    )


@traceable(
    name="execute_orchestration_flow", tags=["orchestrator", "decomposition", "flow"]
)
async def execute_orchestration_flow(
    task_id: str,
    request: TaskRequest,
    guardrail_report: GuardrailReport,
    workspace_ctx: Dict[str, Any],
    project: Dict[str, str],
) -> TaskResponse:
    """Execute decomposition, validation, and persistence once approvals pass."""

    relevant_toolsets = progressive_loader.get_tools_for_task(
        task_description=request.description, strategy=ToolLoadingStrategy.MINIMAL
    )

    stats = progressive_loader.get_tool_usage_stats(relevant_toolsets)
    logger.info(f"[Orchestrator] Progressive loading stats: {stats}")
    await mcp_tool_client.create_memory_entity(
        name=f"tool_loading_stats_{task_id}",
        entity_type="orchestrator_metrics",
        observations=[
            f"Task: {task_id}",
            f"Loaded tools: {stats['loaded_tools']} / {stats['total_tools']}",
            f"Token savings: {stats['savings_percent']}%",
            f"Estimated tokens saved: {stats['estimated_tokens_saved']}",
        ],
    )

    available_tools_context = progressive_loader.format_tools_for_llm(relevant_toolsets)
    required_tools = get_required_tools_for_task(request.description)

    if llm_client.is_enabled():
        subtasks = await decompose_with_llm(
            request, task_id, available_tools=available_tools_context
        )
    else:
        subtasks = decompose_request(request)

    validation_results: Dict[str, Any] = {}
    for subtask in subtasks:
        agent_toolsets = progressive_loader.get_tools_for_task(
            task_description=subtask.description,
            assigned_agent=subtask.agent_type.value,
            strategy=ToolLoadingStrategy.PROGRESSIVE,
        )

        subtask_required_tools = get_required_tools_for_task(subtask.description)
        availability = await check_agent_tool_availability(
            subtask.agent_type, subtask_required_tools
        )
        validation_results[subtask.id] = {
            **availability,
            "loaded_toolsets": len(agent_toolsets),
            "tools_context": progressive_loader.format_tools_for_llm(agent_toolsets),
        }

        if not availability["available"]:
            logger.warning(
                "[Orchestrator] Agent %s missing tools for subtask %s: %s",
                subtask.agent_type,
                subtask.id,
                availability["missing_tools"],
            )
            await mcp_tool_client.create_memory_entity(
                name=f"tool_availability_warning_{task_id}_{subtask.id}",
                entity_type="orchestrator_warning",
                observations=[
                    f"Task: {task_id}",
                    f"Subtask: {subtask.id}",
                    f"Agent: {subtask.agent_type.value}",
                    f"Missing tools: {availability['missing_tools']}",
                ],
            )

    routing_plan = {
        "execution_order": [st.id for st in subtasks],
        "parallel_groups": identify_parallel_tasks(subtasks),
        "estimated_duration_minutes": estimate_duration(subtasks),
        "tool_validation": validation_results,
        "required_tools": required_tools,
    }

    estimated_tokens = len(request.description.split()) * 2

    response = TaskResponse(
        task_id=task_id,
        subtasks=subtasks,
        routing_plan=routing_plan,
        estimated_tokens=estimated_tokens,
        guardrail_report=guardrail_report,
        workspace_context=workspace_ctx,  # Include workspace context for extension
        linear_project=(
            {  # Include Linear project info for extension caching
                "id": project["id"],
                "name": project["name"],
                "url": project.get("url", ""),
            }
            if project["id"]
            else None
        ),
    )

    task_registry[task_id] = response
    await persist_task_state(task_id, request, response)

    tools_validated = (
        all(result["available"] for result in validation_results.values())
        if validation_results
        else True
    )

    await mcp_tool_client.create_memory_entity(
        name=f"task_orchestrated_{task_id}",
        entity_type="orchestrator_event",
        observations=[
            f"Task ID: {task_id}",
            f"Subtasks: {len(subtasks)}",
            f"Priority: {request.priority}",
            f"Agent: orchestrator",
            f"Tools validated: {tools_validated}",
            f"Guardrail status: {guardrail_report.status}",
        ],
    )

    return response


@app.post("/resume/{task_id}", response_model=TaskResponse)
async def resume_approved_task(task_id: str):
    """Resume a workflow once its approval request is satisfied."""

    pending_task = pending_approval_registry.get(task_id)
    if not pending_task:
        raise HTTPException(
            status_code=404, detail="Task not awaiting approval or not found"
        )

    approval_request_id = pending_task["approval_request_id"]
    status_info = await hitl_manager.check_approval_status(approval_request_id)
    approval_status = status_info.get("status")

    if approval_status == "approved":
        risk_level = pending_task["risk_level"]
        wait_seconds = (datetime.utcnow() - pending_task["created_at"]).total_seconds()
        approval_wait_time.labels(risk_level=risk_level).observe(max(wait_seconds, 0.0))

        request_payload = pending_task["request_payload"]
        guardrail_report = pending_task["guardrail_report"]
        pending_approval_registry.pop(task_id, None)

        request = TaskRequest(**request_payload)
        response = await execute_orchestration_flow(task_id, request, guardrail_report)

        await mcp_tool_client.create_memory_entity(
            name=f"task_resumed_{task_id}",
            entity_type="orchestrator_event",
            observations=[
                f"Task ID: {task_id}",
                f"Approval request: {approval_request_id}",
                f"Approver: {status_info.get('approver_id')}",
                f"Resumed at: {datetime.utcnow().isoformat()}",
            ],
        )

        return response

    if approval_status == "pending":
        raise HTTPException(status_code=409, detail="Approval still pending")

    pending_approval_registry.pop(task_id, None)

    if approval_status == "rejected":
        raise HTTPException(
            status_code=403,
            detail=f"Task rejected: {status_info.get('rejection_reason', 'No reason provided')}",
        )

    if approval_status == "expired":
        raise HTTPException(
            status_code=410,
            detail="Approval request expired. Submit a new orchestration request.",
        )

    raise HTTPException(
        status_code=400, detail=f"Unexpected approval status: {approval_status}"
    )


@app.post("/approve/{approval_id}")
async def approve_request(
    approval_id: str,
    approver_id: str,
    approver_role: str,
    justification: Optional[str] = None,
):
    """
    Approve a pending HITL request.

    Args:
        approval_id: UUID of the approval request
        approver_id: Email or ID of the approver
        approver_role: Role of the approver (developer, tech_lead, devops_engineer)
        justification: Optional justification for approval

    Returns:
        Success message with approval details
    """
    try:
        # Attempt to approve
        success = await hitl_manager.approve_request(
            request_id=approval_id,
            approver_id=approver_id,
            approver_role=approver_role,
            justification=justification,
        )

        if not success:
            raise HTTPException(
                status_code=403, detail="Approval failed: Unauthorized or expired"
            )

        # Get updated status for metrics
        status_info = await hitl_manager.check_approval_status(approval_id)
        risk_level = status_info.get("risk_level", "unknown")

        # Update metrics
        approval_decisions_total.labels(
            decision="approved", risk_level=risk_level
        ).inc()

        # Emit approval_approved event
        await event_bus.emit(
            "approval_approved",
            {
                "approval_id": approval_id,
                "approver_id": approver_id,
                "approver_role": approver_role,
                "risk_level": risk_level,
                "justification": justification,
                "timestamp": datetime.utcnow().isoformat(),
            },
            source="orchestrator",
            correlation_id=approval_id,
        )

        # Log to MCP memory
        await mcp_tool_client.create_memory_entity(
            name=f"approval_{approval_id}",
            entity_type="approval_decision",
            observations=[
                f"Approval ID: {approval_id}",
                f"Decision: APPROVED",
                f"Approver: {approver_id}",
                f"Role: {approver_role}",
                f"Risk Level: {risk_level}",
                f"Timestamp: {datetime.utcnow().isoformat()}",
            ],
        )

        return {
            "status": "approved",
            "approval_id": approval_id,
            "approver_id": approver_id,
            "timestamp": datetime.utcnow().isoformat(),
            "message": "Request approved successfully",
        }

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Approval error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.post("/reject/{approval_id}")
async def reject_request(
    approval_id: str, approver_id: str, approver_role: str, reason: str
):
    """
    Reject a pending HITL request.

    Args:
        approval_id: UUID of the approval request
        approver_id: Email or ID of the approver
        approver_role: Role of the approver
        reason: Reason for rejection (required)

    Returns:
        Success message with rejection details
    """
    try:
        # Attempt to reject
        success = await hitl_manager.reject_request(
            request_id=approval_id,
            approver_id=approver_id,
            approver_role=approver_role,
            reason=reason,
        )

        if not success:
            raise HTTPException(
                status_code=403, detail="Rejection failed: Unauthorized or expired"
            )

        # Get updated status for metrics
        status_info = await hitl_manager.check_approval_status(approval_id)
        risk_level = status_info.get("risk_level", "unknown")

        # Update metrics
        approval_decisions_total.labels(
            decision="rejected", risk_level=risk_level
        ).inc()

        # Emit approval_rejected event
        await event_bus.emit(
            "approval_rejected",
            {
                "approval_id": approval_id,
                "approver_id": approver_id,
                "approver_role": approver_role,
                "risk_level": risk_level,
                "reason": reason,
                "timestamp": datetime.utcnow().isoformat(),
            },
            source="orchestrator",
            correlation_id=approval_id,
        )

        # Log to MCP memory
        await mcp_tool_client.create_memory_entity(
            name=f"rejection_{approval_id}",
            entity_type="approval_decision",
            observations=[
                f"Approval ID: {approval_id}",
                f"Decision: REJECTED",
                f"Approver: {approver_id}",
                f"Role: {approver_role}",
                f"Reason: {reason}",
                f"Risk Level: {risk_level}",
                f"Timestamp: {datetime.utcnow().isoformat()}",
            ],
        )

        return {
            "status": "rejected",
            "approval_id": approval_id,
            "approver_id": approver_id,
            "reason": reason,
            "timestamp": datetime.utcnow().isoformat(),
            "message": "Request rejected",
        }

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Rejection error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.get("/approvals/pending")
async def list_pending_approvals(approver_role: Optional[str] = None, limit: int = 50):
    """
    List all pending approval requests.

    Args:
        approver_role: Filter by approver role (optional)
        limit: Maximum number of results (default 50)

    Returns:
        List of pending approval requests
    """
    try:
        approvals = await hitl_manager.list_pending_requests(
            approver_role=approver_role, limit=limit
        )

        return {"count": len(approvals), "approvals": approvals}

    except Exception as e:
        logger.error(f"List approvals error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.get("/approvals/{approval_id}")
async def get_approval_status(approval_id: str):
    """
    Get the status of a specific approval request.

    Args:
        approval_id: UUID of the approval request

    Returns:
        Approval request details and current status
    """
    try:
        status_info = await hitl_manager.check_approval_status(approval_id)
        return status_info

    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Get approval status error: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


async def persist_task_state(
    task_id: str, request: TaskRequest, response: TaskResponse, status: str = "pending"
):
    """Persist task state to State Persistence Layer"""
    try:
        async with httpx.AsyncClient() as client:
            # Create task record
            task_payload = {
                "task_id": task_id,
                "type": "orchestration",
                "status": status,
                "assigned_agent": "orchestrator",
                "payload": {
                    "description": request.description,
                    "priority": request.priority,
                    "subtasks": [
                        {
                            "id": st.id,
                            "agent_type": st.agent_type,
                            "description": st.description,
                            "status": st.status,
                        }
                        for st in response.subtasks
                    ],
                    "routing_plan": response.routing_plan,
                },
            }

            await client.post(
                f"{STATE_SERVICE_URL}/tasks", json=task_payload, timeout=5.0
            )

            # Create workflow record
            workflow_payload = {
                "workflow_id": task_id,
                "name": f"Task: {request.description[:50]}",
                "steps": [
                    {
                        "step_id": st.id,
                        "agent": st.agent_type,
                        "description": st.description,
                    }
                    for st in response.subtasks
                ],
                "status": status,
            }

            await client.post(
                f"{STATE_SERVICE_URL}/workflows", json=workflow_payload, timeout=5.0
            )

            await mcp_tool_client.create_memory_entity(
                name=f"orchestrator_state_persisted_{task_id}",
                entity_type="orchestrator_event",
                observations=[
                    f"Task ID: {task_id}",
                    f"Workflow steps: {len(response.subtasks)}",
                    f"Status: pending",
                ],
            )

    except Exception as e:
        print(f"State persistence failed (non-critical): {e}")
        await mcp_tool_client.create_memory_entity(
            name=f"orchestrator_state_persistence_failed_{task_id}",
            entity_type="orchestrator_error",
            observations=[f"Task ID: {task_id}", f"Error: {str(e)}"],
        )


@app.get("/tasks/{task_id}")
async def get_task_status(task_id: str):
    """Retrieve task status and subtask progress"""
    if task_id not in task_registry:
        raise HTTPException(status_code=404, detail="Task not found")

    task = task_registry[task_id]

    # Calculate completion status from subtasks
    subtasks_list = task.subtasks if hasattr(task, "subtasks") else []
    completed = sum(1 for s in subtasks_list if s.status == TaskStatus.COMPLETED)
    in_progress = sum(1 for s in subtasks_list if s.status == TaskStatus.IN_PROGRESS)
    total = len(subtasks_list)

    # Determine overall status
    if completed == total and total > 0:
        overall_status = "completed"
    elif in_progress > 0 or completed > 0:
        overall_status = "in_progress"
    else:
        overall_status = "pending"

    # Return extension-compatible format
    return {
        "task_id": task_id,
        "status": overall_status,
        "subtasks": [
            {
                "agent_type": (
                    s.agent_type.value
                    if hasattr(s.agent_type, "value")
                    else str(s.agent_type)
                ),
                "description": s.description,
                "status": (
                    s.status.value if hasattr(s.status, "value") else str(s.status)
                ),
                "priority": getattr(s, "priority", "medium"),
            }
            for s in subtasks_list
        ],
        "total_subtasks": total,
        "completed_subtasks": completed,
        "routing_plan": task.routing_plan if hasattr(task, "routing_plan") else {},
        "linear_project": (
            task.linear_project if hasattr(task, "linear_project") else None
        ),
    }


@app.get("/agents")
async def list_agents():
    """List available specialized agents and their endpoints"""
    return {
        "agents": [
            {"type": agent.value, "endpoint": endpoint, "status": "available"}
            for agent, endpoint in AGENT_ENDPOINTS.items()
        ]
    }


@app.get("/agents/{agent_name}/tools")
async def get_agent_tools(agent_name: str):
    """
    Get tool allocations for a specific agent from manifest
    Includes recommended tools, shared tools, and capabilities
    """
    profile = get_agent_profile(agent_name)

    if not profile:
        raise HTTPException(
            status_code=404, detail=f"Agent profile not found: {agent_name}"
        )

    return {
        "agent": agent_name,
        "display_name": profile.get("display_name"),
        "mission": profile.get("mission"),
        "mcp_tools": profile.get("mcp_tools", {}),
        "capabilities": profile.get("capabilities", []),
        "status": profile.get("status", "unknown"),
    }


@app.post("/validate-routing")
async def validate_routing(request: Dict[str, Any]):
    """
    Validate if an agent has required tools for a task
    Request: {"agent": "feature-dev", "description": "implement authentication"}
    """
    agent_name = request.get("agent")
    description = request.get("description", "")

    if not agent_name:
        raise HTTPException(status_code=400, detail="Agent name required")

    try:
        agent_type = AgentType(agent_name)
    except ValueError:
        raise HTTPException(status_code=400, detail=f"Invalid agent type: {agent_name}")

    required_tools = get_required_tools_for_task(description)
    availability = await check_agent_tool_availability(agent_type, required_tools)

    return {
        "agent": agent_name,
        "task_description": description,
        "required_tools": required_tools,
        "availability": availability,
    }


@app.get("/mcp/discover")
async def discover_mcp_servers():
    """
    Discover all MCP servers via Docker MCP Toolkit.

    Returns real-time server and tool inventory.
    """
    try:
        servers = mcp_discovery.discover_servers()
        return {
            "success": True,
            "discovery": servers,
            "timestamp": datetime.utcnow().isoformat(),
        }
    except Exception as e:
        logger.error(f"[Orchestrator] MCP discovery failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"MCP discovery failed: {str(e)}")


@app.get("/mcp/manifest")
async def get_agent_manifest():
    """
    Generate agent-to-tool mapping manifest based on discovered MCP servers.

    This replaces the static agents/agents-manifest.json with dynamic discovery.
    """
    try:
        manifest = mcp_discovery.generate_agent_manifest()
        return {"success": True, "manifest": manifest}
    except Exception as e:
        logger.error(f"[Orchestrator] Manifest generation failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail=f"Manifest generation failed: {str(e)}"
        )


@app.get("/mcp/server/{server_name}")
async def get_server_details(server_name: str):
    """Get details for a specific MCP server."""
    server = mcp_discovery.get_server(server_name)
    if not server:
        raise HTTPException(
            status_code=404, detail=f"MCP server '{server_name}' not found"
        )

    return {"success": True, "server": server}


# =============================================================================
# MCP TOOL DISCOVERY ENDPOINTS (CHEF-118)
# =============================================================================
# These endpoints enable MCP Bridge Clients to discover and invoke tools.
# They expose the ProgressiveMCPLoader functionality via REST API.
# =============================================================================


@app.get("/tools")
@traceable(name="api_get_all_tools", tags=["api", "mcp", "tools", "discovery"])
async def get_all_tools():
    """Get complete catalog of all MCP tools.

    Returns all available tools from all MCP servers (150+ tools).
    This is the full tool list for clients that need complete discovery.

    CHEF-118: Implements MCP Tool Discovery Endpoints.

    Response:
        {
            "success": true,
            "tools": [
                {
                    "name": "tool_name",
                    "server": "server_name",
                    "description": "Tool description",
                    "parameters": {...}
                },
                ...
            ],
            "count": 150,
            "servers": ["server1", "server2", ...],
            "strategy": "full"
        }
    """
    try:
        # Use progressive loader with FULL strategy to get all tools
        all_toolsets = progressive_loader.get_tools_for_task(
            task_description="*",  # Wildcard to get all tools
            strategy=ToolLoadingStrategy.FULL,
        )

        # Flatten toolsets into tool list
        tools = []
        servers = set()

        for toolset in all_toolsets:
            servers.add(toolset.server)
            for tool_name in toolset.tools:
                tools.append(
                    {
                        "name": tool_name,
                        "server": toolset.server,
                        "rationale": toolset.rationale,
                        "priority": toolset.priority,
                    }
                )

        logger.info(
            f"[Tools API] Returning {len(tools)} tools from {len(servers)} servers"
        )

        return {
            "success": True,
            "tools": tools,
            "count": len(tools),
            "servers": sorted(list(servers)),
            "strategy": "full",
            "timestamp": datetime.utcnow().isoformat(),
        }

    except Exception as e:
        logger.error(f"[Tools API] Failed to get all tools: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Tool discovery failed: {str(e)}")


@app.get("/tools/progressive")
@traceable(
    name="api_get_progressive_tools", tags=["api", "mcp", "tools", "progressive"]
)
async def get_progressive_tools(task: Optional[str] = None):
    """Get task-filtered MCP tools using progressive disclosure.

    Returns only tools relevant to the specified task (10-30 tools vs 150+).
    Uses keyword matching to select appropriate MCP servers and tools.

    CHEF-118: Implements progressive tool loading endpoint.

    Query Parameters:
        task: Task description for tool matching (e.g., "commit changes to git")

    Response:
        {
            "success": true,
            "tools": [...],
            "count": 15,
            "servers": ["gitmcp"],
            "strategy": "progressive",
            "keywords_matched": ["commit", "git"],
            "token_savings_estimate": "~85%"
        }
    """
    if not task:
        raise HTTPException(
            status_code=400,
            detail="Query parameter 'task' is required. Example: /tools/progressive?task=commit+changes",
        )

    try:
        # Use progressive loader with PROGRESSIVE strategy
        relevant_toolsets = progressive_loader.get_tools_for_task(
            task_description=task,
            strategy=ToolLoadingStrategy.PROGRESSIVE,
        )

        # Get stats about what was matched
        stats = progressive_loader.get_tool_usage_stats(relevant_toolsets)

        # Flatten toolsets into tool list
        tools = []
        servers = set()
        keywords_matched = set()

        for toolset in relevant_toolsets:
            servers.add(toolset.server)
            for tool_name in toolset.tools:
                tools.append(
                    {
                        "name": tool_name,
                        "server": toolset.server,
                        "rationale": toolset.rationale,
                        "priority": toolset.priority,
                    }
                )

        # Extract matched keywords from rationale
        for toolset in relevant_toolsets:
            if "keyword" in toolset.rationale.lower():
                # Parse keywords from rationale like "Matched keywords: git, commit"
                keywords_matched.update(
                    [kw.strip() for kw in toolset.rationale.split(":")[1].split(",")]
                    if ":" in toolset.rationale
                    else []
                )

        logger.info(
            f"[Tools API] Progressive: {len(tools)} tools for task '{task[:50]}...' "
            f"(servers: {', '.join(servers)})"
        )

        return {
            "success": True,
            "tools": tools,
            "count": len(tools),
            "servers": sorted(list(servers)),
            "strategy": "progressive",
            "keywords_matched": sorted(list(keywords_matched)),
            "task": task,
            "stats": stats,
            "token_savings_estimate": f"~{max(0, 100 - (len(tools) / 1.5)):.0f}%",
            "timestamp": datetime.utcnow().isoformat(),
        }

    except Exception as e:
        logger.error(f"[Tools API] Progressive discovery failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail=f"Progressive tool discovery failed: {str(e)}"
        )


class ToolInvocationRequest(BaseModel):
    """Request body for tool invocation."""

    arguments: Dict[str, Any] = Field(
        default_factory=dict, description="Tool arguments"
    )
    timeout: Optional[float] = Field(
        default=30.0, description="Request timeout in seconds"
    )


@app.post("/tools/{tool_name}")
@traceable(name="api_invoke_tool", tags=["api", "mcp", "tools", "invoke"])
async def invoke_tool(tool_name: str, request: ToolInvocationRequest):
    """Invoke an MCP tool by name.

    Routes the tool invocation to the appropriate MCP server and returns
    the result. This is the primary endpoint for tool execution.

    CHEF-118: Implements MCP tool invocation proxy.

    Path Parameters:
        tool_name: Name of the tool to invoke (e.g., "memory/read", "gitmcp/commit")

    Request Body:
        {
            "arguments": {"key": "value", ...},
            "timeout": 30.0
        }

    Response:
        {
            "success": true,
            "result": {...},
            "tool": "tool_name",
            "server": "server_name",
            "execution_time_ms": 150.5,
            "citations": [...]
        }
    """
    import time

    start_time = time.time()

    try:
        # Parse tool name to extract server if format is "server/tool"
        if "/" in tool_name:
            server_name, actual_tool = tool_name.split("/", 1)
        else:
            # Find server that has this tool
            server_name = None
            actual_tool = tool_name

            # Search through discovered servers for the tool
            all_toolsets = progressive_loader.get_tools_for_task(
                task_description=tool_name,
                strategy=ToolLoadingStrategy.PROGRESSIVE,
            )

            for toolset in all_toolsets:
                if tool_name in toolset.tools or actual_tool in toolset.tools:
                    server_name = toolset.server
                    break

            if not server_name:
                raise HTTPException(
                    status_code=404,
                    detail=f"Tool '{tool_name}' not found. Use /tools to list available tools.",
                )

        # Invoke the tool via MCP client
        result = await mcp_client.call_tool(  # type: ignore
            server=server_name,
            tool=actual_tool,
            params=request.arguments,
        )

        execution_time_ms = (time.time() - start_time) * 1000

        logger.info(
            f"[Tools API] Invoked {server_name}/{actual_tool} "
            f"(time: {execution_time_ms:.1f}ms, args: {list(request.arguments.keys())})"
        )

        return {
            "success": True,
            "result": result,
            "tool": actual_tool,
            "server": server_name,
            "execution_time_ms": execution_time_ms,
            "citations": (
                result.get("citations", []) if isinstance(result, dict) else []
            ),
            "timestamp": datetime.utcnow().isoformat(),
        }

    except HTTPException:
        raise
    except Exception as e:
        execution_time_ms = (time.time() - start_time) * 1000
        logger.error(
            f"[Tools API] Tool invocation failed: {tool_name} - {e}", exc_info=True
        )
        raise HTTPException(status_code=500, detail=f"Tool invocation failed: {str(e)}")


@app.get("/tools/servers")
@traceable(name="api_list_tool_servers", tags=["api", "mcp", "tools", "servers"])
async def list_tool_servers():
    """List all available MCP tool servers.

    Returns a list of MCP servers with their tool counts and status.

    Response:
        {
            "success": true,
            "servers": [
                {
                    "name": "gitmcp",
                    "tools_count": 15,
                    "status": "connected"
                },
                ...
            ],
            "total_servers": 18,
            "total_tools": 150
        }
    """
    try:
        discovery = mcp_discovery.discover_servers()

        servers = []
        total_tools = 0

        for server_name, server_info in discovery.get("servers", {}).items():
            tool_count = len(server_info.get("tools", []))
            total_tools += tool_count
            servers.append(
                {
                    "name": server_name,
                    "tools_count": tool_count,
                    "status": server_info.get("status", "unknown"),
                    "description": server_info.get("description", ""),
                }
            )

        return {
            "success": True,
            "servers": servers,
            "total_servers": len(servers),
            "total_tools": total_tools,
            "timestamp": datetime.utcnow().isoformat(),
        }

    except Exception as e:
        logger.error(f"[Tools API] Failed to list servers: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Server listing failed: {str(e)}")


@app.get("/linear/issues")
async def get_linear_issues():
    """Fetch issues from Linear roadmap."""
    if not linear_client.is_enabled():
        return {"success": False, "message": "Linear integration not configured"}

    issues = await linear_client.fetch_issues()
    return {"success": True, "count": len(issues), "issues": issues}


@app.post("/linear/issues")
async def create_linear_issue(request: Dict[str, Any]):
    """Create a new Linear issue."""
    if not linear_client.is_enabled():
        raise HTTPException(status_code=503, detail="Linear integration not configured")

    issue = await linear_client.create_issue(
        title=request["title"],
        description=request.get("description", ""),
        priority=request.get("priority", 0),
    )

    if issue:
        return {"success": True, "issue": issue}
    else:
        raise HTTPException(status_code=500, detail="Failed to create Linear issue")


@app.get("/linear/project/{project_id}")
async def get_linear_project(project_id: str):
    """Fetch Linear project roadmap."""
    if not linear_client.is_enabled():
        raise HTTPException(status_code=503, detail="Linear integration not configured")

    roadmap = await linear_client.fetch_project_roadmap(project_id)
    return {"success": True, "roadmap": roadmap}


@app.patch("/linear/issues/{issue_id}")
async def update_linear_issue(issue_id: str, request: Dict[str, Any]):
    """Update an existing Linear issue (description, state, etc.)."""
    if not linear_client.is_enabled():
        raise HTTPException(status_code=503, detail="Linear integration not configured")

    success = await linear_client.update_issue(issue_id, **request)

    if success:
        return {
            "success": True,
            "issue_id": issue_id,
            "updated_fields": list(request.keys()),
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to update Linear issue")


@app.post("/linear/roadmap/update-phase")
async def update_phase_completion(request: Dict[str, Any]):
    """
    Update a phase issue with completion status and metrics.

    Expected payload:
    {
        "issue_id": "...",
        "phase_name": "Phase 2: HITL Integration",
        "status": "COMPLETE",
        "components": ["Risk Assessment", "Approval Workflow", ...],
        "subtasks": [{"title": "Task 2.1", "status": "complete"}, ...],
        "metrics": {"total_requests": 4, "avg_time": "1.26s"},
        "artifacts": {"main.py": "lines 705-901", ...}
    }
    """
    if not linear_client.is_enabled():
        raise HTTPException(status_code=503, detail="Linear integration not configured")

    issue_id = request["issue_id"]
    phase_name = request["phase_name"]
    status = request.get("status", "IN PROGRESS")
    components = request.get("components", [])
    subtasks = request.get("subtasks", [])
    metrics = request.get("metrics", {})
    artifacts = request.get("artifacts", {})

    # Build comprehensive description
    description_parts = [
        f"## {phase_name} - {status}",
        "",
        "### Implementation Summary",
        request.get("summary", "Complete implementation verified in production."),
        "",
    ]

    if components:
        description_parts.extend(
            [
                "### Components Delivered",
                *[f"{i+1}. **{comp}**" for i, comp in enumerate(components)],
                "",
            ]
        )

    if subtasks:
        description_parts.extend(
            [
                "### Subtasks Completed",
                *[
                    f"- {'‚úÖ' if task.get('status') == 'complete' else '‚è≥'} {task['title']}"
                    for task in subtasks
                ],
                "",
            ]
        )

    if metrics:
        description_parts.extend(
            [
                f"### Production Metrics (as of {datetime.now().strftime('%Y-%m-%d')})",
                *[
                    f"- {key.replace('_', ' ').title()}: {value}"
                    for key, value in metrics.items()
                ],
                "",
            ]
        )

    if artifacts:
        description_parts.extend(
            [
                "### Artifacts",
                *[f"- `{path}`: {desc}" for path, desc in artifacts.items()],
                "",
            ]
        )

    description_parts.extend(
        [
            "### Testing",
            *[f"‚úÖ {test}" for test in request.get("tests", [])],
            "",
            f"**Status**: {status}",
            f"**Deployment**: {request.get('deployment_url', 'Production')}",
        ]
    )

    description = "\n".join(description_parts)

    success = await linear_client.update_issue(issue_id, description=description)

    if success:
        logger.info(f"Updated Linear phase issue {issue_id}: {phase_name} - {status}")
        return {
            "success": True,
            "issue_id": issue_id,
            "phase": phase_name,
            "status": status,
        }
    else:
        raise HTTPException(status_code=500, detail="Failed to update phase issue")


@app.post("/execute/{task_id}")
@traceable(name="execute_task_workflow", tags=["orchestrator", "execution", "agents"])
async def execute_task_by_id(task_id: str):
    """Execute workflow using LangGraph agents (not HTTP microservices).

    This endpoint invokes agents in-process via the LangGraph StateGraph,
    eliminating HTTP overhead and enabling proper state management.

    Execution flow:
    1. Look up task in registry to get subtasks
    2. For each subtask, invoke the corresponding LangGraph agent node
    3. Collect results and update task status

    Benefits over HTTP-based execution:
    - 50% faster (in-memory vs HTTP)
    - 83% memory reduction (no microservices)
    - Proper checkpointing via PostgreSQL
    - Progressive tool disclosure per agent
    """
    if task_id not in task_registry:
        raise HTTPException(status_code=404, detail="Task not found")

    task = task_registry[task_id]
    execution_results = []

    # Import LangGraph components
    from graph import WorkflowState, get_agent
    from langchain_core.messages import AIMessage, HumanMessage

    for subtask in task.subtasks:
        try:
            # Update subtask status
            subtask.status = TaskStatus.IN_PROGRESS

            # Map AgentType enum to agent name
            agent_name_map = {
                AgentType.FEATURE_DEV: "feature-dev",
                AgentType.CODE_REVIEW: "code-review",
                AgentType.INFRASTRUCTURE: "infrastructure",
                AgentType.CICD: "cicd",
                AgentType.DOCUMENTATION: "documentation",
            }

            agent_name = agent_name_map.get(subtask.agent_type)
            if not agent_name:
                logger.warning(f"[Execute] Unknown agent type: {subtask.agent_type}")
                subtask.status = TaskStatus.FAILED
                execution_results.append(
                    {
                        "subtask_id": subtask.id,
                        "agent": str(subtask.agent_type),
                        "status": "failed",
                        "error": f"Unknown agent type: {subtask.agent_type}",
                    }
                )
                continue

            logger.info(f"[Execute] Invoking {agent_name} for subtask {subtask.id}")

            # Get the real LangGraph agent
            agent = get_agent(agent_name)

            # Build message for agent
            messages = [HumanMessage(content=subtask.description)]

            # Add context from previous results if available
            if execution_results:
                prev_result = execution_results[-1]
                if prev_result.get("status") == "completed" and prev_result.get(
                    "result"
                ):
                    context_msg = f"\n\nContext from previous step ({prev_result.get('agent')}):\n{str(prev_result.get('result'))[:2000]}"
                    messages[0] = HumanMessage(
                        content=subtask.description + context_msg
                    )

            # Invoke agent via LangGraph (in-process, not HTTP)
            response = await agent.invoke(messages)

            # Extract result content
            result_content = (
                response.content if hasattr(response, "content") else str(response)
            )

            logger.info(
                f"[Execute] {agent_name} completed. Response length: {len(result_content)}"
            )

            subtask.status = TaskStatus.COMPLETED
            execution_results.append(
                {
                    "subtask_id": subtask.id,
                    "agent": str(subtask.agent_type),
                    "status": "completed",
                    "result": result_content[:5000],  # Truncate for response
                }
            )

        except Exception as e:
            logger.error(
                f"[Execute] Agent {subtask.agent_type} failed: {e}", exc_info=True
            )
            subtask.status = TaskStatus.FAILED
            execution_results.append(
                {
                    "subtask_id": subtask.id,
                    "agent": str(subtask.agent_type),
                    "status": "failed",
                    "error": str(e),
                }
            )

    # Update overall task status
    overall_status = (
        "completed"
        if all(r["status"] == "completed" for r in execution_results)
        else (
            "partial"
            if any(r["status"] == "completed" for r in execution_results)
            else "failed"
        )
    )

    logger.info(f"[Execute] Task {task_id} finished: {overall_status}")

    return {
        "task_id": task_id,
        "status": overall_status,
        "execution_results": execution_results,
        "subtasks": [
            {
                "id": st.id,
                "agent_type": str(st.agent_type),
                "status": str(st.status),
                "description": st.description,
            }
            for st in task.subtasks
        ],
    }


def decompose_request(request: TaskRequest) -> List[SubTask]:
    """
    Decompose incoming request into discrete subtasks (rule-based fallback)
    Uses simple keyword matching for MVP (production would use Task Router or LLM)
    """
    import uuid

    description_lower = request.description.lower()
    subtasks = []

    # Feature development detection
    if any(
        keyword in description_lower
        for keyword in ["implement", "create", "build", "develop", "feature"]
    ):
        subtasks.append(
            SubTask(
                id=str(uuid.uuid4()),
                agent_type=AgentType.FEATURE_DEV,
                description=f"Implement feature: {request.description}",
                context_refs=["codebase"],
            )
        )

    # Code review after feature dev
    if subtasks and subtasks[-1].agent_type == AgentType.FEATURE_DEV:
        review_task = SubTask(
            id=str(uuid.uuid4()),
            agent_type=AgentType.CODE_REVIEW,
            description=f"Review implementation: {request.description}",
            dependencies=[subtasks[-1].id],
        )
        subtasks.append(review_task)

    # Infrastructure changes detection
    if any(
        keyword in description_lower
        for keyword in ["deploy", "infrastructure", "terraform", "docker", "k8s"]
    ):
        subtasks.append(
            SubTask(
                id=str(uuid.uuid4()),
                agent_type=AgentType.INFRASTRUCTURE,
                description=f"Infrastructure changes: {request.description}",
            )
        )

    # CI/CD pipeline detection
    if any(
        keyword in description_lower
        for keyword in ["pipeline", "ci/cd", "continuous", "deployment"]
    ):
        subtasks.append(
            SubTask(
                id=str(uuid.uuid4()),
                agent_type=AgentType.CICD,
                description=f"Configure CI/CD: {request.description}",
            )
        )

    # Documentation detection
    if any(
        keyword in description_lower
        for keyword in ["document", "readme", "doc", "guide"]
    ):
        subtasks.append(
            SubTask(
                id=str(uuid.uuid4()),
                agent_type=AgentType.DOCUMENTATION,
                description=f"Generate documentation: {request.description}",
            )
        )

    # Default to feature dev if no matches
    if not subtasks:
        subtasks.append(
            SubTask(
                id=str(uuid.uuid4()),
                agent_type=AgentType.FEATURE_DEV,
                description=request.description,
            )
        )

    return subtasks


def identify_parallel_tasks(subtasks: List[SubTask]) -> List[List[str]]:
    """Identify subtasks that can run in parallel"""
    parallel_groups = []
    independent_tasks = []

    for task in subtasks:
        if not task.dependencies:
            independent_tasks.append(task.id)

    if len(independent_tasks) > 1:
        parallel_groups.append(independent_tasks)

    return parallel_groups


def estimate_duration(subtasks: List[SubTask]) -> int:
    """Estimate total execution duration in minutes"""
    # Simple heuristic: 5 minutes per subtask
    return len(subtasks) * 5


@app.post("/config/tool-loading")
async def configure_tool_loading(request: Dict[str, Any]):
    """
    Configure progressive tool loading strategy at runtime.

    Request:
        {
            "strategy": "minimal" | "agent_profile" | "progressive" | "full",
            "reason": "debugging" | "cost_optimization" | "high_complexity_task"
        }
    """
    strategy_name = request.get("strategy", "progressive")
    reason = request.get("reason", "runtime_config")

    try:
        strategy = ToolLoadingStrategy(strategy_name)
        progressive_loader.default_strategy = strategy

        await mcp_tool_client.create_memory_entity(
            name=f"tool_loading_config_change_{datetime.utcnow().isoformat()}",
            entity_type="orchestrator_config",
            observations=[
                f"Strategy changed to: {strategy_name}",
                f"Reason: {reason}",
                f"Timestamp: {datetime.utcnow().isoformat()}",
            ],
        )

        return {"success": True, "current_strategy": strategy_name, "reason": reason}
    except ValueError:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid strategy: {strategy_name}. Valid values: minimal, agent_profile, progressive, full",
        )


@app.get("/config/tool-loading/stats")
async def get_tool_loading_stats():
    """
    Get statistics about current tool loading configuration.
    """
    # Get current strategy tools
    sample_toolsets = progressive_loader.get_tools_for_task(
        task_description="sample task", strategy=progressive_loader.default_strategy
    )

    stats = progressive_loader.get_tool_usage_stats(sample_toolsets)

    return {
        "current_strategy": progressive_loader.default_strategy.value,
        "stats": stats,
        "recommendation": (
            "Consider using 'minimal' or 'progressive' for cost optimization"
            if stats["savings_percent"] < 50
            else "Current strategy is well-optimized"
        ),
    }


@traceable(name="decompose_with_llm", tags=["orchestrator", "llm", "decomposition"])
async def decompose_with_llm(
    request: TaskRequest, task_id: str, available_tools: Optional[str] = None
) -> List[SubTask]:
    """
    Decompose task using Gradient AI with LangChain tool binding.

    Uses progressive tool disclosure pattern:
    1. Discover relevant MCP tools for task (via progressive_loader)
    2. Convert MCP tools to LangChain BaseTool instances
    3. Bind tools to LLM via bind_tools() for function calling
    4. LLM can now INVOKE tools, not just see documentation

    This enables actual tool usage during task decomposition.
    """
    import json
    import uuid

    from langchain_core.messages import HumanMessage, SystemMessage

    system_prompt = """You are an expert DevOps orchestrator. Analyze development requests and decompose them into discrete subtasks for specialized agents.

Available agents:
- feature-dev: Application code generation and feature implementation
- code-review: Quality assurance, static analysis, security scanning
- infrastructure: Infrastructure-as-code generation (Docker, K8s, Terraform)
- cicd: CI/CD pipeline generation (GitHub Actions, GitLab CI)
- documentation: Documentation generation (README, API docs)

Return JSON with this structure:
{
  "subtasks": [
    {
      "agent_type": "feature-dev",
      "description": "Implement user authentication",
      "dependencies": []
    }
  ]
}

You have access to MCP tools. Use them to gather context before decomposing tasks."""

    # Query RAG for vendor documentation context
    vendor_context = await query_vendor_context(request.description)

    user_prompt = f"""Task: {request.description}

Project Context: {json.dumps(request.project_context) if request.project_context else "General project"}
Priority: {request.priority}

{vendor_context if vendor_context else ""}

Break this down into subtasks. Consider dependencies and execution order.
Use available tools to gather context if needed."""

    try:
        logger.info(
            f"[Orchestrator] Attempting LLM-powered decomposition with tool binding for task {task_id}"
        )

        # PROGRESSIVE TOOL DISCLOSURE: Discover relevant MCP tools for this task
        # Uses keyword matching to filter 150+ tools down to ~10-30 relevant ones
        from lib.progressive_mcp_loader import ToolLoadingStrategy

        relevant_toolsets = progressive_loader.get_tools_for_task(
            task_description=request.description, strategy=ToolLoadingStrategy.MINIMAL
        )

        # Get token savings stats
        tool_stats = progressive_loader.get_tool_usage_stats(relevant_toolsets)
        logger.info(
            f"[Orchestrator] Progressive disclosure: {tool_stats['loaded_tools']}/{tool_stats['total_tools']} tools "
            f"({tool_stats['savings_percent']}% reduction, ~{tool_stats['estimated_tokens_saved']} tokens saved)"
        )

        # TOOL BINDING: Convert MCP tools to LangChain BaseTool instances
        # This enables ACTUAL FUNCTION CALLING instead of text-only documentation
        langchain_tools = mcp_client.to_langchain_tools(relevant_toolsets)
        logger.info(
            f"[Orchestrator] Converted {len(langchain_tools)} MCP tools to LangChain tools"
        )

        # Get LLM with tools bound for function calling
        llm_with_tools = llm_client.get_llm_with_tools(
            tools=langchain_tools, temperature=0.3, max_tokens=1000
        )

        # Prepare messages
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt),
        ]

        # Invoke LLM - it can now CALL tools, not just see documentation
        response = await llm_with_tools.ainvoke(messages)

        # Extract structured response (JSON parsing)
        from langchain_core.output_parsers import JsonOutputParser

        parser = JsonOutputParser()

        # Handle tool calls if LLM made any
        raw_content = response.content
        if hasattr(response, "tool_calls") and response.tool_calls:
            logger.info(
                f"[Orchestrator] LLM made {len(response.tool_calls)} tool calls during decomposition"
            )
            # Tool calls would be executed automatically in full agent loop
            # For now, we just log them

        # Parse response content as JSON
        if isinstance(raw_content, str):
            parsed = parser.parse(raw_content)
        else:
            parsed = {"subtasks": []}  # Fallback

        result = {
            "content": parsed,
            "tokens": tool_stats.get("estimated_tokens_used", 0),
        }

        logger.info(
            f"[Orchestrator] LLM decomposition successful: ~{result.get('tokens', 0)} tokens used"
        )

        # Parse LLM response
        llm_subtasks = result["content"].get("subtasks", [])
        logger.debug(
            f"[Orchestrator] LLM returned {len(llm_subtasks)} subtasks: {llm_subtasks}"
        )

        # Create SubTask objects with proper IDs
        subtasks = []
        id_map = {}  # Map indices to UUIDs for dependencies

        for i, st in enumerate(llm_subtasks):
            subtask_id = str(uuid.uuid4())
            id_map[i] = subtask_id

            # Validate agent type
            try:
                agent_type = AgentType(st["agent_type"])
            except ValueError:
                print(
                    f"[WARNING] Invalid agent type: {st['agent_type']}, defaulting to feature-dev"
                )
                agent_type = AgentType.FEATURE_DEV

            subtasks.append(
                SubTask(
                    id=subtask_id,
                    agent_type=agent_type,
                    description=st["description"],
                    dependencies=[],  # Will populate after all IDs are assigned
                )
            )

        # Resolve dependencies (map from indices to UUIDs)
        for i, st in enumerate(llm_subtasks):
            dep_indices = st.get("dependencies", [])
            if dep_indices and isinstance(dep_indices, list):
                # Filter to only valid integer indices
                valid_deps = []
                for dep_idx in dep_indices:
                    if isinstance(dep_idx, int) and dep_idx in id_map:
                        valid_deps.append(id_map[dep_idx])
                    else:
                        logger.warning(
                            f"[Orchestrator] Invalid dependency index: {dep_idx} (type: {type(dep_idx).__name__})"
                        )

                subtasks[i].dependencies = valid_deps

        print(
            f"[LLM] Decomposed task into {len(subtasks)} subtasks using {result['tokens']} tokens"
        )
        return subtasks

    except Exception as e:
        logger.error(f"[Orchestrator] LLM decomposition failed: {e}", exc_info=True)
        print(
            f"[ERROR] LLM decomposition failed: {type(e).__name__}: {e}, falling back to rule-based"
        )
        return []  # Return empty list to allow fallback to rule-based decomposition


# ============================================================================
# Chat Interface (Phase 5: Copilot Integration)
# ============================================================================


class ChatRequest(BaseModel):
    """Chat message from user."""

    message: str = Field(..., description="User's message")
    session_id: Optional[str] = Field(
        None, description="Session ID (auto-generated if not provided)"
    )
    user_id: Optional[str] = Field(None, description="User identifier")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context")


class ChatResponse(BaseModel):
    """Chat response to user."""

    message: str = Field(..., description="Assistant's response")
    session_id: str = Field(..., description="Session ID for multi-turn conversation")
    task_id: Optional[str] = Field(None, description="Task ID if task was created")
    intent: str = Field(..., description="Recognized intent type")
    confidence: float = Field(..., description="Intent confidence score")
    suggestions: Optional[List[str]] = Field(None, description="Suggested next actions")
    metadata: Optional[Dict[str, Any]] = Field(
        None, description="Additional response metadata"
    )


@app.post("/chat", response_model=ChatResponse, tags=["chat"])
@traceable(name="chat_endpoint", tags=["chat", "nlp", "conversation"])
async def chat_endpoint(request: ChatRequest):
    """
    Natural language chat interface for task submission and status queries.

    Supports:
    - Task submission: "Add error handling to login endpoint"
    - Status queries: "What's the status of task abc123?"
    - Clarification: "Use JWT authentication"
    - Approval decisions: "Approve" or "Reject"

    Uses intent recognition to understand user's message and route appropriately.
    Multi-turn conversations are supported via session_id.
    """
    # Create or load session
    session_id = request.session_id or f"session-{uuid.uuid4()}"

    try:
        # Ensure session exists
        existing_session = await session_manager.get_session(session_id)
        if not existing_session:
            await session_manager.create_session(
                session_id=session_id,
                user_id=request.user_id,
                metadata=request.context or {},
            )
            logger.info(f"Created new chat session: {session_id}")

        # Load conversation history
        history = await session_manager.load_conversation_history(session_id, limit=10)

        # Save user message
        await session_manager.add_message(
            session_id=session_id,
            role="user",
            content=request.message,
            metadata=request.context or {},
        )

        # Recognize intent
        intent = await intent_recognizer.recognize(request.message, history)

        logger.info(
            f"Chat [{session_id}]: Intent={intent.type}, Confidence={intent.confidence:.2f}"
        )

        # Handle different intent types
        if intent.type == IntentType.TASK_SUBMISSION:
            if intent.needs_clarification:
                # Need more information
                response_message = (
                    intent.clarification_question or "I need more details to proceed."
                )

                # Save assistant message
                await session_manager.add_message(
                    session_id=session_id, role="assistant", content=response_message
                )

                return ChatResponse(
                    message=response_message,
                    session_id=session_id,
                    task_id=None,
                    intent=intent.type,
                    confidence=intent.confidence,
                    suggestions=[
                        "feature-dev",
                        "code-review",
                        "infrastructure",
                        "cicd",
                        "documentation",
                    ],
                    metadata={},
                )

            # Convert intent to task request
            try:
                task_payload = await intent_to_task(intent, history)

                # Submit to orchestrator
                task_request = TaskRequest(**task_payload)
                orchestrate_response = await orchestrate_task(task_request)

                task_id = orchestrate_response.task_id
                response_message = f"‚úì Task created: {task_id}\n\n"
                response_message += f"Breaking down into {len(orchestrate_response.subtasks)} subtasks:\n"
                for i, subtask in enumerate(orchestrate_response.subtasks[:3], 1):
                    response_message += (
                        f"{i}. {subtask.description} ({subtask.agent_type})\n"
                    )

                if len(orchestrate_response.subtasks) > 3:
                    response_message += (
                        f"... and {len(orchestrate_response.subtasks) - 3} more\n"
                    )

                # Note: Approval check happens earlier in orchestrate_task_with_llm()
                # via risk_assessor.requires_approval(), not via guardrail status

                # Save assistant message
                await session_manager.add_message(
                    session_id=session_id,
                    role="assistant",
                    content=response_message,
                    metadata={"task_id": task_id},
                )

                return ChatResponse(
                    message=response_message,
                    session_id=session_id,
                    task_id=task_id,
                    intent=intent.type,
                    confidence=intent.confidence,
                    suggestions=None,
                    metadata={
                        "subtasks_count": len(orchestrate_response.subtasks),
                        "guardrail_status": orchestrate_response.guardrail_report.status,
                    },
                )

            except Exception as e:
                logger.error(f"Failed to create task from intent: {e}", exc_info=True)
                error_message = f"Sorry, I couldn't create that task: {str(e)}"

                await session_manager.add_message(
                    session_id=session_id, role="assistant", content=error_message
                )

                return ChatResponse(
                    message=error_message,
                    session_id=session_id,
                    task_id=None,
                    intent=intent.type,
                    confidence=intent.confidence,
                    suggestions=None,
                    metadata={},
                )

        elif intent.type == IntentType.STATUS_QUERY:
            # Look up task status
            if intent.entity_id:
                # Look up task in registry
                task_response = task_registry.get(intent.entity_id)
                if task_response:
                    status_counts = {}
                    for subtask in task_response.subtasks:
                        status_counts[subtask.status] = (
                            status_counts.get(subtask.status, 0) + 1
                        )

                    response_message = f"Task {intent.entity_id} status:\n"
                    response_message += (
                        f"- Total subtasks: {len(task_response.subtasks)}\n"
                    )
                    for status, count in status_counts.items():
                        response_message += f"- {status}: {count}\n"
                else:
                    response_message = f"Task {intent.entity_id} not found. It may have been completed or doesn't exist."
            else:
                response_message = (
                    "Which task would you like to check? Please provide a task ID."
                )

            await session_manager.add_message(
                session_id=session_id, role="assistant", content=response_message
            )

            return ChatResponse(
                message=response_message,
                session_id=session_id,
                task_id=None,
                intent=intent.type,
                confidence=intent.confidence,
                suggestions=None,
                metadata={},
            )

        elif intent.type == IntentType.APPROVAL_DECISION:
            # Handle approval/rejection
            # Look for pending approval in session context
            approval_id = None
            for msg in reversed(history):
                if msg.get("metadata", {}).get("approval_id"):
                    approval_id = msg["metadata"]["approval_id"]
                    break

            if approval_id:
                # Submit decision
                if intent.decision == "approve":
                    # Call approve endpoint
                    import httpx

                    async with httpx.AsyncClient() as client:
                        try:
                            await client.post(
                                f"http://localhost:8001/approve/{approval_id}",
                                params={
                                    "approver_id": request.user_id or "chat-user",
                                    "approver_role": "developer",
                                    "justification": "User approval via chat",
                                },
                            )
                        except Exception as e:
                            logger.error(f"Failed to approve: {e}")
                    response_message = (
                        f"‚úì Approved request {approval_id}. Resuming workflow..."
                    )
                else:
                    # Call reject endpoint
                    import httpx

                    async with httpx.AsyncClient() as client:
                        try:
                            await client.post(
                                f"http://localhost:8001/reject/{approval_id}",
                                params={
                                    "approver_id": request.user_id or "chat-user",
                                    "approver_role": "developer",
                                    "reason": "User rejection via chat",
                                },
                            )
                        except Exception as e:
                            logger.error(f"Failed to reject: {e}")
                    response_message = (
                        f"‚úó Rejected request {approval_id}. Workflow canceled."
                    )
            else:
                response_message = "I don't see any pending approvals in this conversation. Please provide an approval ID."

            await session_manager.add_message(
                session_id=session_id, role="assistant", content=response_message
            )

            return ChatResponse(
                message=response_message,
                session_id=session_id,
                task_id=None,
                intent=intent.type,
                confidence=intent.confidence,
                suggestions=None,
                metadata={"approval_id": approval_id} if approval_id else {},
            )

        else:
            # General query or unknown
            response_message = (
                intent.suggested_response
                or """I can help you with:

- **Creating tasks**: "Add error handling to login endpoint"
- **Checking status**: "What's the status of task-abc123?"
- **Approving requests**: "Approve" or "Reject"

What would you like to do?"""
            )

            await session_manager.add_message(
                session_id=session_id, role="assistant", content=response_message
            )

            return ChatResponse(
                message=response_message,
                session_id=session_id,
                task_id=None,
                intent=intent.type,
                confidence=intent.confidence,
                suggestions=["Create a task", "Check task status", "Approve a request"],
                metadata={},
            )

    except Exception as e:
        logger.error(f"Chat endpoint error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Chat processing failed: {str(e)}")


# ============================================================================
# Streaming Chat Interface (SSE - Server-Sent Events)
# ============================================================================


class ChatStreamRequest(BaseModel):
    """Streaming chat request from user."""

    message: str = Field(..., description="User's message")
    session_id: Optional[str] = Field(
        None, description="Session ID (auto-generated if not provided)"
    )
    user_id: Optional[str] = Field(None, description="User identifier")
    context: Optional[Dict[str, Any]] = Field(None, description="Additional context")
    workspace_config: Optional[Dict[str, Any]] = Field(
        None, description="Workspace configuration from VS Code extension"
    )
    project_context: Optional[Dict[str, Any]] = Field(
        None, description="Project context (Linear project ID, GitHub repo, workspace)"
    )
    file_attachments: Optional[List[str]] = Field(
        None, description="List of file paths to attach to the conversation"
    )
    active_file: Optional[str] = Field(
        None, description="Path to currently active file in editor"
    )
    workspace_root: Optional[str] = Field(
        None, description="Root path of the workspace"
    )


@app.post("/chat/stream", tags=["chat"])
@traceable(
    name="chat_stream",
    tags=["api", "streaming", "sse", "ask-mode"],
    metadata={"session_mode": "ask", "supports_mode_hints": True},
)
async def chat_stream_endpoint(request: ChatStreamRequest):
    """
    Stream chat response via Server-Sent Events (SSE) for interactive conversations.

    This endpoint powers the @chef chat participant in VS Code, providing real-time
    token-by-token responses with full LangGraph agent orchestration.

    **Conversational AI Features:**
    - Natural language understanding (no formal syntax required)
    - Multi-turn conversation support via session_id
    - Context-aware responses based on workspace and project
    - Automatic routing to specialized agents (code, review, infrastructure, etc.)
    - Human-in-the-loop approvals for high-risk operations

    **Request Body:**
    ```json
    {
      "message": "Fix the authentication bug in login.py",
      "session_id": "optional-session-id",  // Auto-generated if omitted
      "user_id": "user-123",  // Optional user identifier
      "project_context": {
        "linear_project_id": "PROJ-123",
        "github_repo_url": "https://github.com/user/repo",
        "workspace_name": "my-project"
      },
      "workspace_config": {
        "name": "my-workspace",
        "path": "/path/to/workspace"
      }
    }
    ```

    **SSE Event Types:**
    - `content`: LLM-generated text chunks (stream in real-time)
    - `agent_complete`: Agent finished processing (shows progress)
    - `tool_call`: MCP tool invocation (for transparency)
    - `done`: Stream complete with session_id (for follow-up messages)
    - `error`: Error occurred with user-friendly message

    **Example SSE Response:**
    ```
    data: {"type": "content", "content": "I'll help you"}
    data: {"type": "content", "content": " fix that"}
    data: {"type": "agent_complete", "agent": "feature_dev"}
    data: {"type": "content", "content": " authentication issue"}
    data: {"type": "done", "session_id": "stream-abc123"}
    ```

    **Error Handling:**
    - Authentication errors ‚Üí Check API key configuration
    - Rate limits ‚Üí Automatic retry with exponential backoff
    - Timeouts ‚Üí Request simpler task or retry
    - Model errors ‚Üí Contact support if persistent

    **Multi-Turn Conversations:**
    Save the session_id from the "done" event and include it in subsequent
    requests to maintain conversation context.
    """
    session_id = request.session_id or f"stream-{uuid.uuid4()}"

    async def event_generator():
        """Generate SSE events from LangGraph stream."""
        try:
            # Import the graph for streaming
            from graph import WorkflowState, get_graph
            from langchain_core.messages import AIMessage, HumanMessage

            # STEP 1: Recognize intent to determine if this is Ask or Agent mode
            # Use module-level intent_recognizer (initialized with llm_client)
            intent_error = None
            try:
                # Extract mode hint from request context
                mode_hint = None
                if request.context:
                    mode_hint = request.context.get("session_mode")  # 'ask' or 'agent'

                logger.debug(f"[Chat Stream] Mode hint from context: {mode_hint}")
                intent = await intent_recognizer.recognize(
                    request.message, mode_hint=mode_hint
                )
                logger.info(
                    f"[Chat Stream] Recognized intent: {intent.type} (confidence: {intent.confidence:.2f})"
                )

                # Record intent recognition metrics
                mode_hint_source = "context" if mode_hint else "none"
                intent_recognition_total.labels(
                    session_mode=mode_hint or "unknown",
                    intent_type=intent.type,
                    mode_hint_source=mode_hint_source,
                ).inc()

                intent_recognition_confidence.labels(
                    session_mode=mode_hint or "unknown",
                    mode_hint_source=mode_hint_source,
                ).observe(intent.confidence)

                # If this is a task submission, redirect to /execute/stream
                if intent.type == IntentType.TASK_SUBMISSION:
                    logger.info(
                        f"[Chat Stream] Task submission detected, redirecting to Agent mode"
                    )
                    yield f"data: {json.dumps({'type': 'content', 'content': 'üîÑ Switching to Agent mode for task execution...\n\n'})}\n\n"

                    # Stream a note about mode switch
                    task_desc = intent.task_description or request.message
                    task_data = {
                        "type": "content",
                        "content": f"**Task:** {task_desc}\n\n",
                    }
                    yield f"data: {json.dumps(task_data)}\n\n"
                    redirect_data = {
                        "type": "redirect",
                        "endpoint": "/execute/stream",
                        "reason": "task_submission",
                    }
                    yield f"data: {json.dumps(redirect_data)}\n\n"
                    done_data = {"type": "done", "session_id": session_id}
                    yield f"data: {json.dumps(done_data)}\n\n"
                    yield "data: [DONE]\n\n"
                    return
            except Exception as intent_error:
                logger.warning(
                    f"[Chat Stream] Intent recognition failed: {intent_error}, proceeding with Ask mode"
                )

            logger.info(f"[Chat Stream] Proceeding with Ask mode (conversational)")

            # STEP 2: For non-task intents, use conversational handler
            from graph import conversational_handler_node

            logger.info(f"[Chat Stream] Initializing graph for session {session_id}")
            graph = get_graph()
            if not graph:
                logger.error("[Chat Stream] Failed to get graph instance")
                yield f"data: {json.dumps({'type': 'error', 'error': 'Graph not available'})}\n\n"
                return
            logger.info(f"[Chat Stream] Graph loaded successfully, streaming enabled")

            # Load conversation history from session if exists
            conversation_history = []
            try:
                existing_session = await session_manager.get_session(session_id)
                if existing_session:
                    # Load previous messages
                    history = await session_manager.load_conversation_history(
                        session_id, limit=10
                    )
                    for msg in history:
                        role = msg.get("role", "user")
                        content = msg.get("content", "")
                        if role == "user":
                            conversation_history.append(HumanMessage(content=content))
                        elif role == "assistant":
                            conversation_history.append(AIMessage(content=content))
                    logger.info(
                        f"[Chat Stream] Loaded {len(conversation_history)} messages from session {session_id}"
                    )
                else:
                    # Create new session
                    await session_manager.create_session(
                        session_id=session_id,
                        user_id=request.user_id,
                        metadata={
                            "project_context": request.project_context,
                            "workspace_root": request.workspace_root,
                        },
                    )
                    logger.info(f"[Chat Stream] Created new session {session_id}")
            except Exception as e:
                logger.warning(f"[Chat Stream] Could not load session history: {e}")

            # Extract and enrich project context from request
            project_context = None
            if request.project_context:
                # Validate required fields
                required_fields = [
                    "linear_project_id",
                    "github_repo_url",
                    "workspace_name",
                ]
                provided_fields = [
                    f for f in required_fields if request.project_context.get(f)
                ]

                if not provided_fields:
                    logger.warning(
                        "[Chat Stream] project_context provided but missing all identifiers"
                    )

                proj_ctx = request.project_context
                project_context = {
                    "project_id": proj_ctx.get("linear_project_id")
                    or proj_ctx.get("github_repo_url"),
                    "repository_url": proj_ctx.get("github_repo_url"),
                    "workspace_name": proj_ctx.get("workspace_name"),
                    "branch": proj_ctx.get("branch", "main"),
                    "directory": proj_ctx.get("directory", "."),
                }

                # Add workspace root if available
                if request.workspace_root:
                    project_context["workspace_path"] = request.workspace_root

                logger.info(
                    f"[Chat Stream] Project context: {project_context.get('project_id', 'unknown')} "
                    f"({project_context.get('workspace_name', 'unknown')})"
                )
            elif request.workspace_config:
                # Fallback to workspace config for project identification
                workspace_cfg = request.workspace_config
                project_context = {
                    "workspace_name": workspace_cfg.get("name"),
                    "workspace_path": workspace_cfg.get("path")
                    or request.workspace_root,
                }
                logger.info(
                    f"[Chat Stream] Using workspace config: {project_context.get('workspace_name')}"
                )

            # Read file attachments and active file using MCP filesystem tools
            file_contents_text = ""
            files_read = []

            try:
                # Get MCP tool client for filesystem operations
                mcp_tool_client = get_mcp_tool_client("feature_dev")

                # Read active file if provided
                if request.active_file and request.workspace_root:
                    try:
                        active_file_path = request.active_file
                        # Use MCP rust-mcp-filesystem to read file
                        file_result = await mcp_tool_client.call_tool(
                            server_name="rust-mcp-filesystem",
                            tool_name="read_file",
                            arguments={"path": active_file_path},
                        )
                        if file_result and not file_result.get("isError"):
                            file_content = file_result.get("content", [])
                            if file_content and len(file_content) > 0:
                                content_text = file_content[0].get("text", "")
                                files_read.append(active_file_path)
                                file_contents_text += f"\\n\\n**Currently Active File: `{active_file_path}`**\\n```\\n{content_text}\\n```\\n"
                    except Exception as e:
                        logger.warning(
                            f"Could not read active file {request.active_file}: {e}"
                        )

                # Read additional file attachments
                if request.file_attachments:
                    for file_path in request.file_attachments:
                        try:
                            file_result = await mcp_tool_client.call_tool(
                                server_name="rust-mcp-filesystem",
                                tool_name="read_file",
                                arguments={"path": file_path},
                            )
                            if file_result and not file_result.get("isError"):
                                file_content = file_result.get("content", [])
                                if file_content and len(file_content) > 0:
                                    content_text = file_content[0].get("text", "")
                                    files_read.append(file_path)
                                    file_contents_text += f"\\n\\n**Attached File: `{file_path}`**\\n```\\n{content_text}\\n```\\n"
                        except Exception as e:
                            logger.warning(f"Could not read file {file_path}: {e}")

                if files_read:
                    logger.info(
                        f"[Chat Stream] Read {len(files_read)} file(s): {', '.join(files_read)}"
                    )

            except Exception as e:
                logger.warning(f"[Chat Stream] Error reading files: {e}")

            # Enrich message with file contents if any were read
            enriched_message = request.message
            if file_contents_text:
                enriched_message = f"{request.message}\\n\\n**Context from workspace:**{file_contents_text}"

            # Build message list: conversation history + new message
            all_messages = conversation_history + [
                HumanMessage(content=enriched_message)
            ]

            # Build initial state
            initial_state: WorkflowState = {
                "messages": all_messages,
                "current_agent": "orchestrator",
                "next_agent": None,
                "task_result": None,
                "approvals": [],
                "requires_approval": False,
                "workflow_id": session_id,
                "thread_id": session_id,  # Reuse same thread for continuity
                "pending_operation": None,
                "project_context": project_context,
            }

            config = {"configurable": {"thread_id": session_id}}

            # Define specialist agents (not supervisor)
            SPECIALIST_AGENTS = [
                "feature_dev",
                "feature-dev",
                "code_review",
                "code-review",
                "infrastructure",
                "cicd",
                "documentation",
            ]

            # Track current node for filtering
            current_node = None
            last_keepalive = asyncio.get_event_loop().time()
            keepalive_interval = 15  # Send keepalive every 15 seconds

            # Stream events from LangGraph
            async for event in graph.astream_events(
                initial_state, config, version="v2"
            ):
                event_kind = event.get("event", "")
                event_name = event.get("name", "")

                # Track which node we're in
                if event_kind == "on_chain_start":
                    if any(agent in event_name.lower() for agent in SPECIALIST_AGENTS):
                        current_node = event_name
                    elif "supervisor" in event_name.lower():
                        current_node = "supervisor"

                # Only stream LLM tokens from specialist agents (not supervisor)
                if event_kind == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, "content") and chunk.content:
                        # Check if this is from a specialist agent (not supervisor routing)
                        if current_node and current_node != "supervisor":
                            yield f"data: {json.dumps({'type': 'content', 'content': chunk.content})}\n\n"
                            last_keepalive = asyncio.get_event_loop().time()

                # Agent completed (for progress indication)
                elif event_kind == "on_chain_end":
                    name = event.get("name", "")
                    if any(agent in name.lower() for agent in SPECIALIST_AGENTS):
                        yield f"data: {json.dumps({'type': 'agent_complete', 'agent': name})}\n\n"
                        current_node = None  # Reset after completion
                        last_keepalive = asyncio.get_event_loop().time()

                # Tool calls from specialist agents only
                elif event_kind == "on_tool_start":
                    if current_node and current_node != "supervisor":
                        tool_name = event.get("name", "unknown")
                        yield f"data: {json.dumps({'type': 'tool_call', 'tool': tool_name})}\n\n"
                        last_keepalive = asyncio.get_event_loop().time()

                # Send keepalive comment to prevent connection timeout
                current_time = asyncio.get_event_loop().time()
                if current_time - last_keepalive > keepalive_interval:
                    yield ": keepalive\n\n"
                    last_keepalive = current_time

                # Add small delay to prevent overwhelming the client
                await asyncio.sleep(0.01)

            # Save messages to session history
            try:
                # Save user message
                await session_manager.add_message(
                    session_id=session_id,
                    role="user",
                    content=request.message,
                    metadata={
                        "file_attachments": request.file_attachments,
                        "active_file": request.active_file,
                    },
                )

                # Get final state to extract assistant response
                # Note: In streaming, we should collect the response as we go
                # For now, we'll save a completion marker
                await session_manager.add_message(
                    session_id=session_id,
                    role="assistant",
                    content="[Response streamed to client]",
                    metadata={"workflow_id": session_id},
                )
                logger.debug(f"[Chat Stream] Saved messages to session {session_id}")
            except Exception as e:
                logger.warning(f"[Chat Stream] Could not save to session: {e}")

            # Stream complete
            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id})}\n\n"

            # Send terminal [DONE] signal per SSE spec
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"Chat stream error: {e}", exc_info=True)

            # Capture LangSmith trace URL if available
            trace_url = None
            try:
                from langsmith import utils as langsmith_utils

                run_tree = langsmith_utils.get_current_run_tree()
                if run_tree and hasattr(run_tree, "trace_id"):
                    langsmith_project = os.getenv(
                        "LANGCHAIN_PROJECT", "code-chef-production"
                    )
                    trace_url = f"https://smith.langchain.com/o/default/projects/p/{langsmith_project}/r/{run_tree.trace_id}"
                    logger.info(f"[Chat Stream] Error trace: {trace_url}")
            except Exception as trace_err:
                logger.debug(f"Could not capture trace URL: {trace_err}")

            error_message = str(e)
            # Make error messages more user-friendly
            if "API key" in error_message or "401" in error_message:
                error_message = (
                    "Authentication failed. Please check your API configuration."
                )
            elif "429" in error_message or "rate limit" in error_message.lower():
                error_message = (
                    "Rate limit exceeded. Please wait a moment and try again."
                )
            elif "timeout" in error_message.lower():
                error_message = "Request timed out. The operation took too long. Please try a simpler task or try again."
            elif (
                "model" in error_message.lower()
                and "not found" in error_message.lower()
            ):
                error_message = "Model configuration error. Please contact support."

            error_response = {
                "type": "error",
                "error": error_message,
                "trace_url": trace_url,
                "session_id": session_id,
            }

            # Include full details in debug mode
            if logger.level <= logging.DEBUG:
                error_response["details"] = str(e)

            yield f"data: {json.dumps(error_response)}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
            "Access-Control-Allow-Origin": "*",
        },
    )


@app.post("/execute/stream", tags=["execution"])
@traceable(name="execute_stream", tags=["api", "streaming", "sse", "agent-mode"])
async def execute_stream_endpoint(request: ChatStreamRequest):
    """
    Execute task via Agent mode with workflow routing and full orchestration.

    This endpoint is for task execution (not conversational chat). It uses:
    - Workflow router to select appropriate template
    - Risk assessment for high-risk operations
    - HITL approval flow via Linear integration
    - Full multi-agent orchestration (no filtering)

    **Request Body:**
    ```json
    {
      "message": "Implement JWT authentication",
      "session_id": "optional-session-id",
      "user_id": "user-123",
      "project_context": {...},
      "workspace_config": {...}
    }
    ```

    **SSE Event Types:**
    - `content`: Agent output (all agents, not just specialists)
    - `agent_complete`: Agent finished processing
    - `tool_call`: MCP tool invocation
    - `workflow_status`: Workflow progress updates
    - `approval_required`: HITL approval needed
    - `done`: Stream complete with session_id
    - `error`: Error occurred
    """
    session_id = request.session_id or f"exec-{uuid.uuid4()}"

    async def event_generator():
        """Generate SSE events from workflow execution."""
        try:
            from graph import WorkflowState, get_graph
            from langchain_core.messages import AIMessage, HumanMessage

            logger.info(
                f"[Execute Stream] Initializing workflow for session {session_id}"
            )
            graph = get_graph()
            if not graph:
                logger.error("[Execute Stream] Failed to get graph instance")
                yield f"data: {json.dumps({'type': 'error', 'error': 'Graph not available'})}\n\n"
                return
        except ImportError as e:
            logger.error(f"[Execute Stream] Failed to import graph: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': 'Graph module not available'})}\n\n"
            return
        except Exception as e:
            logger.error(f"[Execute Stream] Initialization error: {e}")
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
            return

        try:

            # Create/load session
            try:
                existing_session = await session_manager.get_session(session_id)
                if not existing_session:
                    await session_manager.create_session(
                        session_id=session_id,
                        user_id=request.user_id,
                        metadata={
                            "mode": "agent",
                            "project_context": request.project_context,
                            "workspace_root": request.workspace_root,
                        },
                    )
                    logger.info(
                        f"[Execute Stream] Created new agent-mode session {session_id}"
                    )
            except Exception as e:
                logger.warning(f"[Execute Stream] Could not create session: {e}")

            # Extract project context
            project_context = None
            if request.project_context:
                project_context = {
                    "project_id": request.project_context.get("linear_project_id")
                    or request.project_context.get("github_repo_url"),
                    "repository_url": request.project_context.get("github_repo_url"),
                    "workspace_name": request.project_context.get("workspace_name"),
                    "branch": request.project_context.get("branch", "main"),
                    "directory": request.project_context.get("directory", "."),
                }
                if request.workspace_root:
                    project_context["workspace_path"] = request.workspace_root

            # Read file attachments using MCP filesystem
            file_contents_text = ""
            files_read = []
            try:
                mcp_tool_client = get_mcp_tool_client("feature_dev")
                if request.active_file and request.workspace_root:
                    try:
                        file_result = await mcp_tool_client.call_tool(
                            server_name="rust-mcp-filesystem",
                            tool_name="read_file",
                            arguments={"path": request.active_file},
                        )
                        if file_result and not file_result.get("isError"):
                            file_content = file_result.get("content", [])
                            if file_content and len(file_content) > 0:
                                content_text = file_content[0].get("text", "")
                                files_read.append(request.active_file)
                                file_contents_text += f"\n\n**Active File: `{request.active_file}`**\n```\n{content_text}\n```\n"
                    except Exception as e:
                        logger.warning(f"Could not read active file: {e}")

                if request.file_attachments:
                    for file_path in request.file_attachments:
                        try:
                            file_result = await mcp_tool_client.call_tool(
                                server_name="rust-mcp-filesystem",
                                tool_name="read_file",
                                arguments={"path": file_path},
                            )
                            if file_result and not file_result.get("isError"):
                                file_content = file_result.get("content", [])
                                if file_content and len(file_content) > 0:
                                    content_text = file_content[0].get("text", "")
                                    files_read.append(file_path)
                                    file_contents_text += f"\n\n**Attached File: `{file_path}`**\n```\n{content_text}\n```\n"
                        except Exception as e:
                            logger.warning(f"Could not read file {file_path}: {e}")
                if files_read:
                    logger.info(f"[Execute Stream] Read {len(files_read)} file(s)")
            except Exception as e:
                logger.warning(f"[Execute Stream] Error reading files: {e}")

            # Enrich message with file contents
            enriched_message = request.message
            if file_contents_text:
                enriched_message = f"{request.message}\n\n**Context from workspace:**{file_contents_text}"

            # Use workflow router to select appropriate workflow
            workflow_selection = None
            try:
                from workflows.workflow_router import get_workflow_router as _get_router

                workflow_router = _get_router()
                workflow_selection = await workflow_router.select_workflow(
                    task_description=request.message,
                    project_context=project_context or {},
                )
                logger.info(
                    f"[Execute Stream] Selected workflow: {workflow_selection.workflow_name} "
                    f"(confidence: {workflow_selection.confidence:.2f}, method: {workflow_selection.method})"
                )

                # Send workflow status event
                workflow_data = {
                    "type": "workflow_status",
                    "workflow": workflow_selection.workflow_name,
                    "confidence": workflow_selection.confidence,
                    "method": workflow_selection.method,
                }
                yield f"data: {json.dumps(workflow_data)}\n\n"
            except Exception as e:
                logger.warning(
                    f"[Execute Stream] Workflow routing failed, using default: {e}"
                )

            # Build initial state
            initial_state: WorkflowState = {
                "messages": [HumanMessage(content=enriched_message)],
                "current_agent": "orchestrator",
                "next_agent": None,
                "task_result": None,
                "approvals": [],
                "requires_approval": False,
                "workflow_id": session_id,
                "thread_id": session_id,
                "pending_operation": None,
                "project_context": project_context,
            }

            # Add workflow template if selected
            if workflow_selection:
                initial_state["workflow_template"] = workflow_selection.workflow_name
                initial_state["workflow_context"] = workflow_selection.context_variables
                initial_state["use_template_engine"] = True

            config = {"configurable": {"thread_id": session_id}}

            last_keepalive = asyncio.get_event_loop().time()
            keepalive_interval = 15

            # Stream ALL agent events (no filtering for Agent mode)
            async for event in graph.astream_events(
                initial_state, config, version="v2"
            ):
                event_kind = event.get("event", "")
                event_name = event.get("name", "")

                # Stream all LLM tokens (including supervisor reasoning)
                if event_kind == "on_chat_model_stream":
                    chunk = event.get("data", {}).get("chunk")
                    if chunk and hasattr(chunk, "content") and chunk.content:
                        yield f"data: {json.dumps({'type': 'content', 'content': chunk.content})}\n\n"
                        last_keepalive = asyncio.get_event_loop().time()

                # Agent completed
                elif event_kind == "on_chain_end":
                    name = event.get("name", "")
                    if any(
                        agent in name.lower()
                        for agent in [
                            "supervisor",
                            "feature_dev",
                            "feature-dev",
                            "code_review",
                            "code-review",
                            "infrastructure",
                            "cicd",
                            "documentation",
                        ]
                    ):
                        yield f"data: {json.dumps({'type': 'agent_complete', 'agent': name})}\n\n"
                        last_keepalive = asyncio.get_event_loop().time()

                # Tool calls
                elif event_kind == "on_tool_start":
                    tool_name = event.get("name", "unknown")
                    yield f"data: {json.dumps({'type': 'tool_call', 'tool': tool_name})}\n\n"
                    last_keepalive = asyncio.get_event_loop().time()

                # Keepalive
                current_time = asyncio.get_event_loop().time()
                if current_time - last_keepalive > keepalive_interval:
                    yield ": keepalive\n\n"
                    last_keepalive = current_time

                await asyncio.sleep(0.01)

            # Save to session history
            try:
                await session_manager.add_message(
                    session_id=session_id,
                    role="user",
                    content=request.message,
                    metadata={
                        "mode": "agent",
                        "workflow": (
                            workflow_selection.workflow_name
                            if workflow_selection
                            else None
                        ),
                    },
                )
                await session_manager.add_message(
                    session_id=session_id,
                    role="assistant",
                    content="[Agent execution streamed to client]",
                    metadata={"workflow_id": session_id},
                )
            except Exception as e:
                logger.warning(f"[Execute Stream] Could not save to session: {e}")

            yield f"data: {json.dumps({'type': 'done', 'session_id': session_id})}\n\n"
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error(f"Execute stream error: {e}", exc_info=True)
            error_message = str(e)
            if "API key" in error_message or "401" in error_message:
                error_message = (
                    "Authentication failed. Please check your API configuration."
                )
            elif "429" in error_message or "rate limit" in error_message.lower():
                error_message = (
                    "Rate limit exceeded. Please wait a moment and try again."
                )
            elif "timeout" in error_message.lower():
                error_message = (
                    "Request timed out. Please try a simpler task or try again."
                )

            yield f"data: {json.dumps({'type': 'error', 'error': error_message, 'session_id': session_id})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Access-Control-Allow-Origin": "*",
        },
    )


# === Agent-to-Agent Communication (Phase 6) ===

from lib.agent_events import AgentRequestEvent, AgentRequestType, AgentResponseEvent
from lib.agent_request_handler import handle_agent_request


@app.post(
    "/agent-request", response_model=AgentResponseEvent, tags=["agent-communication"]
)
async def agent_request_endpoint(request: AgentRequestEvent):
    """
    Handle requests from other agents.

    Supports:
    - DECOMPOSE_TASK: Break complex task into subtasks
    - ROUTE_REQUEST: Find appropriate agent for capability
    - AGGREGATE_RESULTS: Combine results from multiple agents
    - GET_STATUS: Query orchestrator state
    """
    return await handle_agent_request(
        request=request, handler=handle_orchestrator_request, agent_name="orchestrator"
    )


async def handle_orchestrator_request(request: AgentRequestEvent) -> Dict[str, Any]:
    """
    Process agent requests based on type.

    Args:
        request: AgentRequestEvent with request_type and payload

    Returns:
        Dict with result data

    Raises:
        ValueError: If request type not supported
    """
    request_type = request.request_type
    payload = request.payload

    if request_type == AgentRequestType.DECOMPOSE_TASK:
        # Decompose complex task into subtasks
        task_description = payload.get("task_description", "")

        if not task_description:
            raise ValueError("task_description required for DECOMPOSE_TASK")

        # Use Gradient AI to decompose (if available)
        if llm_client.is_enabled():
            prompt = f"""Decompose this development task into 3-5 subtasks:

Task: {task_description}

Return JSON array of subtasks with:
- description: What to do
- agent: Which agent should handle it (feature-dev, code-review, infrastructure, cicd, documentation)
- priority: high/normal/low
- dependencies: List of prerequisite subtask indices"""

            decomposition = await llm_client.generate(
                prompt=prompt, session_id=request.correlation_id or request.request_id
            )

            return {
                "subtasks": decomposition,
                "original_task": task_description,
                "decomposition_method": "llm",
            }
        else:
            # Fallback: Simple rule-based decomposition
            return {
                "subtasks": [
                    {
                        "description": task_description,
                        "agent": "feature-dev",
                        "priority": "normal",
                    }
                ],
                "decomposition_method": "fallback",
            }

    elif request_type == AgentRequestType.ROUTE_REQUEST:
        # Find appropriate agent for capability
        capability_query = payload.get("capability", "")

        if not capability_query:
            raise ValueError("capability required for ROUTE_REQUEST")

        # Query agent registry
        if registry_client:
            try:
                agents = await registry_client.search_capabilities(capability_query)

                if agents:
                    # Return first match (could implement scoring later)
                    best_agent = agents[0]
                    return {
                        "agent_id": best_agent["agent_id"],
                        "agent_name": best_agent["agent_name"],
                        "capabilities": best_agent["capabilities"],
                        "confidence": 0.9,
                    }
                else:
                    return {
                        "agent_id": None,
                        "error": f"No agents found with capability: {capability_query}",
                    }
            except Exception as e:
                logger.error(f"Registry query failed: {e}")
                return {"error": str(e)}
        else:
            return {"error": "Agent registry not available"}

    elif request_type == AgentRequestType.AGGREGATE_RESULTS:
        # Combine results from multiple agents
        results = payload.get("results", [])

        if not results:
            raise ValueError("results array required for AGGREGATE_RESULTS")

        # Simple aggregation (could use LLM for smarter merging)
        return {
            "aggregated_count": len(results),
            "results": results,
            "summary": f"Aggregated {len(results)} results from multiple agents",
        }

    elif request_type == AgentRequestType.GET_STATUS:
        # Return orchestrator health and stats
        return {
            "status": "healthy",
            "pending_approvals": len(pending_approval_registry),
            "active_sessions": (
                len(getattr(session_manager, "_sessions", {})) if session_manager else 0
            ),
            "mcp_tools_loaded": (
                len(getattr(progressive_loader, "_loaded_tools", []))
                if progressive_loader
                else 0
            ),
            "registry_connected": registry_client is not None,
        }

    else:
        raise ValueError(f"Unsupported request type: {request_type}")


# ============================================================================
# LANGGRAPH MULTI-AGENT WORKFLOW ENDPOINTS (Phase 2 Implementation)
# ============================================================================


@app.post("/orchestrate/langgraph", response_model=Dict[str, Any])
@traceable(name="orchestrate_langgraph", tags=["langgraph", "workflow", "multi-agent"])
async def orchestrate_langgraph(request: TaskRequest):
    """
    LangGraph multi-agent workflow orchestration (NEW)

    Uses LangGraph StateGraph with 6 specialized agent nodes:
    - Supervisor routes tasks to appropriate agents
    - Agents execute in-process (no HTTP overhead)
    - HITL approval nodes for high-risk operations
    - PostgreSQL checkpointing for workflow resume

    Benefits over /orchestrate:
    - 83% memory reduction (no microservices)
    - 50% faster (in-memory vs HTTP)
    - Same multi-agent capabilities
    - Progressive tool disclosure per agent
    """
    from graph import WorkflowState
    from graph import app as workflow_app
    from langchain_core.messages import HumanMessage

    task_id = str(uuid.uuid4())
    logger.info(
        f"[LangGraph] Starting workflow for task {task_id}: {request.description[:100]}"
    )

    # Extract project context from request
    project_context = None
    if request.project_context:
        project_context = {
            "project_id": request.project_context.get("linear_project_id")
            or request.project_context.get("github_repo_url"),
            "repository_url": request.project_context.get("github_repo_url"),
            "workspace_name": request.project_context.get("workspace_name"),
        }

    # Build initial workflow state
    initial_state: WorkflowState = {
        "messages": [HumanMessage(content=request.description)],
        "current_agent": "supervisor",
        "next_agent": "",
        "task_result": {},
        "approvals": [],
        "requires_approval": False,
        "project_context": project_context,
    }

    # Execute workflow
    try:
        # Run workflow with config (includes thread_id for checkpointing)
        config = {"configurable": {"thread_id": task_id}}
        final_state = await workflow_app.ainvoke(initial_state, config=config)

        # Extract results from final state
        messages = final_state.get("messages", [])
        final_message = messages[-1] if messages else None

        response = {
            "task_id": task_id,
            "status": (
                "completed"
                if not final_state.get("requires_approval")
                else "approval_pending"
            ),
            "result": final_message.content if final_message else "No response",
            "agents_invoked": [
                msg.content.split("Agent:")[1].split(",")[0].strip()
                for msg in messages
                if "Agent:" in str(msg.content)
            ],
            "approvals": final_state.get("approvals", []),
            "workflow_state": {
                "current_agent": final_state.get("current_agent"),
                "requires_approval": final_state.get("requires_approval", False),
            },
        }

        logger.info(f"[LangGraph] Workflow {task_id} completed: {response['status']}")
        return response

    except Exception as e:
        logger.error(f"[LangGraph] Workflow {task_id} failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail=f"Workflow execution failed: {str(e)}"
        )


@app.get("/orchestrate/langgraph/status")
async def langgraph_status():
    """Get LangGraph workflow system status"""
    from agents import get_agent

    try:
        # Test agent initialization
        supervisor = get_agent("supervisor")

        return {
            "status": "healthy",
            "workflow_engine": "langgraph",
            "agents_available": [
                "supervisor",
                "feature-dev",
                "code-review",
                "infrastructure",
                "cicd",
                "documentation",
            ],
            "supervisor_model": supervisor.config.get("agent", {}).get(
                "model", "unknown"
            ),
            "checkpointing": "postgresql",
            "progressive_tool_disclosure": "enabled",
        }
    except Exception as e:
        return {"status": "error", "error": str(e)}


# ============================================================================
# HITL APPROVAL WEBHOOK AND RESUME ENDPOINTS
# ============================================================================


@app.post("/webhooks/linear/approval")
@traceable(name="handle_linear_approval_webhook", tags=["webhook", "hitl", "linear"])
async def handle_linear_approval_webhook(request: Request):
    """Primary path: Handle Linear webhook for HITL approval.

    Linear calls this endpoint when an approval emoji is added to an issue.
    Verifies the webhook signature and resumes the paused workflow.

    Expected payload:
    - action: "approved" | "rejected"
    - issueId: Linear issue ID
    - comment: Comment with approval decision
    """
    import hashlib
    import hmac

    # Verify webhook signature
    signing_secret = os.getenv("LINEAR_WEBHOOK_SIGNING_SECRET", "")
    if signing_secret:
        signature = request.headers.get("linear-signature", "")
        body = await request.body()
        expected_sig = hmac.new(
            signing_secret.encode(), body, hashlib.sha256
        ).hexdigest()

        if not hmac.compare_digest(signature, expected_sig):
            logger.warning("Invalid Linear webhook signature")
            raise HTTPException(status_code=401, detail="Invalid signature")

    payload = await request.json()
    logger.info(f"[HITL Webhook] Received: {payload.get('type', 'unknown')}")

    # Extract approval info from comment or reaction
    action = payload.get("action", "")
    data = payload.get("data", {})

    # Look for approval pattern in comment body
    approval_request_id = None
    if "comment" in data:
        comment_body = data["comment"].get("body", "")
        # Extract approval_request_id from comment (format: "APPROVE:request-id" or reaction)
        if "APPROVE:" in comment_body.upper():
            parts = comment_body.split(":")
            if len(parts) >= 2:
                approval_request_id = parts[1].strip()

    if not approval_request_id:
        # Try to find from issue labels or metadata
        issue_id = data.get("issueId") or data.get("issue", {}).get("id")
        if issue_id:
            # Look up approval request by issue_id
            try:
                async with await hitl_manager._get_connection() as conn:
                    async with conn.cursor() as cursor:
                        await cursor.execute(
                            """
                            SELECT id, thread_id, checkpoint_id 
                            FROM approval_requests 
                            WHERE status = 'pending' 
                            AND task_description LIKE %s
                            ORDER BY created_at DESC LIMIT 1
                            """,
                            (f"%{issue_id}%",),
                        )
                        row = await cursor.fetchone()
                        if row:
                            approval_request_id = row[0]
            except Exception as e:
                logger.error(f"[HITL Webhook] DB lookup failed: {e}")

    if approval_request_id:
        result = await resume_workflow_from_approval(
            approval_request_id, action="approved"
        )
        return {
            "status": "resumed",
            "approval_request_id": approval_request_id,
            **result,
        }

    return {
        "status": "ignored",
        "message": "No approval request found in webhook payload",
    }


@app.post("/orchestrate/langgraph/resume/{thread_id}")
@traceable(name="resume_langgraph_workflow", tags=["langgraph", "hitl", "resume"])
async def resume_langgraph_workflow(
    thread_id: str, approval_decision: str = "approved"
):
    """Resume a paused LangGraph workflow after HITL approval.

    This endpoint is called either by:
    1. Linear webhook (automatic)
    2. Manual API call with approval decision

    Args:
        thread_id: LangGraph thread ID from checkpoint
        approval_decision: "approved" or "rejected"

    CHEF-207: On resume, injects captured_insights from checkpoint state as context.
    """
    from graph import WorkflowState
    from graph import app as workflow_app
    from lib.langgraph_base import get_postgres_checkpointer

    logger.info(
        f"[LangGraph Resume] Resuming thread {thread_id} with decision: {approval_decision}"
    )

    try:
        # Get checkpointer to load saved state
        checkpoint_conn = os.getenv("CHECKPOINT_POSTGRES_CONNECTION_STRING")
        if not checkpoint_conn:
            raise HTTPException(
                status_code=500, detail="Checkpoint database not configured"
            )

        # Load workflow state from checkpoint
        config = {"configurable": {"thread_id": thread_id}}

        if approval_decision == "rejected":
            # Mark workflow as rejected and end
            return {
                "thread_id": thread_id,
                "status": "rejected",
                "message": "Workflow terminated due to rejected approval",
            }

        # CHEF-207: Extract captured_insights from checkpoint for context injection
        # Get current state to read insights persisted at pause time
        checkpoint_state = await workflow_app.aget_state(config)
        captured_insights = []
        memory_context = ""

        if checkpoint_state and checkpoint_state.values:
            captured_insights = checkpoint_state.values.get("captured_insights", [])

            # Format insights as context for the resuming agent
            if captured_insights:
                insight_summaries = []
                for insight in captured_insights[-10:]:  # Last 10 insights
                    agent = insight.get("agent_id", "unknown")
                    itype = insight.get("insight_type", "general")
                    content = insight.get("content", "")[:200]  # Truncate for context
                    insight_summaries.append(f"- [{agent}] {itype}: {content}")

                memory_context = "\n\n## Prior Insights from Workflow:\n" + "\n".join(
                    insight_summaries
                )
                logger.info(
                    f"[LangGraph Resume] Injecting {len(captured_insights)} insights from checkpoint"
                )

        # Resume workflow execution from checkpoint
        # Pass the approval result as a message to continue, with insight context
        from langchain_core.messages import HumanMessage

        resume_content = (
            f"HITL approval granted. Continuing workflow execution.{memory_context}"
        )
        resume_message = HumanMessage(content=resume_content)

        final_state = await workflow_app.ainvoke(
            {
                "messages": [resume_message],
                "memory_context": memory_context,
            },
            config=config,
        )

        logger.info(f"[LangGraph Resume] Thread {thread_id} resumed successfully")

        return {
            "thread_id": thread_id,
            "status": (
                "completed"
                if not final_state.get("requires_approval")
                else "in_progress"
            ),
            "current_agent": final_state.get("current_agent"),
            "result": final_state.get("task_result", {}),
        }

    except Exception as e:
        logger.error(
            f"[LangGraph Resume] Failed to resume thread {thread_id}: {e}",
            exc_info=True,
        )
        raise HTTPException(
            status_code=500, detail=f"Failed to resume workflow: {str(e)}"
        )


@traceable(name="resume_workflow_from_approval", tags=["hitl", "resume", "workflow"])
async def resume_workflow_from_approval(
    approval_request_id: str, action: str = "approved"
) -> Dict[str, Any]:
    """Helper to resume workflow from approval request ID.

    Looks up the approval request, updates status, and resumes the workflow.
    """
    try:
        async with await hitl_manager._get_connection() as conn:
            async with conn.cursor() as cursor:
                # Get approval request details including GitHub PR info
                await cursor.execute(
                    """
                    SELECT thread_id, checkpoint_id, workflow_id, status,
                           pr_number, pr_url, github_repo, risk_level,
                           task_description, linear_issue_id, linear_issue_url
                    FROM approval_requests 
                    WHERE id = %s
                    """,
                    (approval_request_id,),
                )
                row = await cursor.fetchone()

                if not row:
                    return {"error": "Approval request not found"}

                (
                    thread_id,
                    checkpoint_id,
                    workflow_id,
                    status,
                    pr_number,
                    pr_url,
                    github_repo,
                    risk_level,
                    task_description,
                    linear_issue_id,
                    linear_issue_url,
                ) = row

                if status != "pending":
                    return {"error": f"Approval request already {status}"}

                # Update approval status
                await cursor.execute(
                    """
                    UPDATE approval_requests 
                    SET status = %s, approved_at = NOW(), resumed_at = NOW()
                    WHERE id = %s
                    """,
                    (action, approval_request_id),
                )
                await conn.commit()

                logger.info(f"[HITL] Approval {approval_request_id} -> {action}")

                # Phase 2: Post GitHub PR comment if PR is linked
                if pr_number and github_repo:
                    try:
                        # Extract owner and repo from github_repo (format: owner/repo)
                        owner, repo = (
                            github_repo.split("/")
                            if "/" in github_repo
                            else (None, None)
                        )

                        if owner and repo:
                            # Use activate_pull_request_management_tools to get GitHub PR tools
                            github_token = os.environ.get("GITHUB_TOKEN")
                            if github_token:
                                import requests

                                comment_body = f"""‚úÖ **HITL Approval Granted**

Workflow resumed after human approval.

**Details:**
- **Approval ID**: `{approval_request_id}`
- **Risk Level**: {risk_level}
- **Operation**: {task_description}
- **Status**: Resuming workflow execution

**Linear Tracking**: [{linear_issue_id}]({linear_issue_url})

*This approval was processed automatically via Linear webhook integration.*
"""

                                # Post comment to GitHub PR
                                api_url = f"https://api.github.com/repos/{owner}/{repo}/issues/{pr_number}/comments"
                                headers = {
                                    "Authorization": f"Bearer {github_token}",
                                    "Accept": "application/vnd.github.v3+json",
                                    "Content-Type": "application/json",
                                }

                                response = requests.post(
                                    api_url,
                                    headers=headers,
                                    json={"body": comment_body},
                                    timeout=10,
                                )

                                if response.status_code == 201:
                                    logger.info(
                                        f"[HITL] Posted approval comment to PR #{pr_number} in {github_repo}"
                                    )
                                else:
                                    logger.warning(
                                        f"[HITL] Failed to post PR comment: {response.status_code} - {response.text}"
                                    )
                            else:
                                logger.warning(
                                    "[HITL] GITHUB_TOKEN not set, skipping PR comment"
                                )
                        else:
                            logger.warning(
                                f"[HITL] Invalid github_repo format: {github_repo}"
                            )
                    except Exception as pr_err:
                        logger.error(
                            f"[HITL] Failed to post GitHub PR comment: {pr_err}"
                        )

                # Phase 4: Record approval resolution metrics
                try:
                    # Parse created_at from row (assuming it's in the query results)
                    # We'll need to query it if not already in row
                    async with conn.cursor() as cursor:
                        await cursor.execute(
                            "SELECT agent_name, created_at FROM approval_requests WHERE id = %s",
                            (approval_request_id,),
                        )
                        metric_row = await cursor.fetchone()
                        if metric_row:
                            agent_name_metric, created_at = metric_row
                            # Cast action to ApprovalStatus
                            from lib.hitl_manager import ApprovalStatus

                            approval_status = (
                                action
                                if action
                                in [
                                    "approved",
                                    "rejected",
                                    "pending",
                                    "expired",
                                    "cancelled",
                                ]
                                else "approved"
                            )  # type: ApprovalStatus
                            await hitl_manager.record_approval_resolution(
                                request_id=approval_request_id,
                                status=approval_status,  # type: ignore
                                agent_name=agent_name_metric,
                                risk_level=risk_level,
                                created_at=created_at,
                            )
                except Exception as metric_err:
                    logger.warning(
                        f"[HITL] Failed to record approval metrics: {metric_err}"
                    )

                # Resume the workflow with insight injection (CHEF-207)
                if thread_id:
                    from graph import app as workflow_app
                    from langchain_core.messages import HumanMessage

                    config = {"configurable": {"thread_id": thread_id}}

                    # CHEF-207: Extract captured_insights from checkpoint for context injection
                    memory_context = ""
                    try:
                        checkpoint_state = await workflow_app.aget_state(config)
                        if checkpoint_state and checkpoint_state.values:
                            captured_insights = checkpoint_state.values.get(
                                "captured_insights", []
                            )
                            if captured_insights:
                                insight_summaries = []
                                for insight in captured_insights[-10:]:
                                    agent = insight.get("agent_id", "unknown")
                                    itype = insight.get("insight_type", "general")
                                    content = insight.get("content", "")[:200]
                                    insight_summaries.append(
                                        f"- [{agent}] {itype}: {content}"
                                    )
                                memory_context = "\n\nPrior Insights:\n" + "\n".join(
                                    insight_summaries
                                )
                                logger.info(
                                    f"[HITL] Injecting {len(captured_insights)} insights on resume"
                                )
                    except Exception as insight_err:
                        logger.warning(f"[HITL] Could not load insights: {insight_err}")

                    resume_content = (
                        f"HITL approval {action}. Resuming workflow.{memory_context}"
                    )
                    resume_message = HumanMessage(content=resume_content)

                    final_state = await workflow_app.ainvoke(
                        {
                            "messages": [resume_message],
                            "memory_context": memory_context,
                        },
                        config=config,
                    )

                    return {
                        "thread_id": thread_id,
                        "workflow_id": workflow_id,
                        "resumed": True,
                        "final_status": (
                            "completed"
                            if not final_state.get("requires_approval")
                            else "in_progress"
                        ),
                    }

                return {
                    "workflow_id": workflow_id,
                    "resumed": False,
                    "reason": "No thread_id found",
                }

    except Exception as e:
        logger.error(f"[HITL] Resume failed for {approval_request_id}: {e}")
        return {"error": str(e)}


@app.get("/debug/task/{task_id}")
async def debug_task(task_id: str):
    """Debug endpoint to inspect task state and registry."""
    logger.info(f"[DEBUG] Checking task {task_id}")

    registry_info = {
        "total_tasks": len(task_registry),
        "recent_task_ids": list(task_registry.keys())[-10:],
    }

    if task_id not in task_registry:
        return {
            "found": False,
            "task_id": task_id,
            "registry": registry_info,
        }

    task = task_registry[task_id]
    return {
        "found": True,
        "task_id": task_id,
        "subtasks": [
            {
                "id": st.id,
                "agent_type": str(st.agent_type),
                "description": st.description[:200],
                "status": str(st.status),
            }
            for st in task.subtasks
        ],
        "registry": registry_info,
    }


# ============================================================================
# DECLARATIVE WORKFLOW ENGINE ENDPOINTS (Week 2 - DEV-172)
# ============================================================================


class WorkflowExecuteRequest(BaseModel):
    """Request to execute a declarative workflow."""

    template_name: str = Field(
        ...,
        description="Workflow template filename (e.g., 'pr-deployment.workflow.yaml')",
    )
    context: Dict[str, Any] = Field(
        ...,
        description="Initial context variables (pr_number, branch, environment, etc.)",
    )


class WorkflowStatusResponse(BaseModel):
    """Workflow execution status response."""

    workflow_id: str
    status: str
    current_step: Optional[str] = None
    started_at: datetime
    completed_at: Optional[datetime] = None
    step_statuses: Dict[str, str] = Field(default_factory=dict)
    error_message: Optional[str] = None


class WorkflowResumeRequest(BaseModel):
    """Request to resume a paused workflow."""

    approval_decision: str = Field(..., description="'approved' or 'rejected'")


class SmartWorkflowRequest(BaseModel):
    """Request for intelligent workflow selection and execution."""

    task_description: str = Field(
        ...,
        description="Natural language description of the task",
    )
    explicit_workflow: Optional[str] = Field(
        default=None,
        description="Optional explicit workflow name to bypass smart selection",
    )
    context: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Project context (files, branch, environment, etc.)",
    )
    dry_run: bool = Field(
        default=False,
        description="If true, return selection without executing the workflow",
    )
    confirm_threshold: Optional[float] = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Confidence threshold for requiring user confirmation (default: 0.7)",
    )


class SmartWorkflowResponse(BaseModel):
    """Response from smart workflow selection and execution."""

    workflow_name: str = Field(..., description="Name of the selected workflow")
    confidence: float = Field(..., description="Confidence score (0.0-1.0)")
    method: str = Field(
        ..., description="How the workflow was selected (heuristic, llm, explicit)"
    )
    reasoning: str = Field(default="", description="Explanation for the selection")
    requires_confirmation: bool = Field(
        default=False, description="Whether user confirmation is needed"
    )
    alternatives: List[Dict[str, Any]] = Field(
        default_factory=list, description="Alternative workflows"
    )
    context_variables: Dict[str, Any] = Field(
        default_factory=dict, description="Extracted context"
    )
    workflow_id: Optional[str] = Field(
        default=None, description="Workflow ID if execution started"
    )
    execution_status: Optional[str] = Field(
        default=None, description="Execution status if not dry_run"
    )


@app.post("/workflow/smart-execute", response_model=SmartWorkflowResponse)
@traceable(name="workflow_smart_execute", tags=["workflow", "routing", "smart"])
async def smart_execute_workflow(request: SmartWorkflowRequest):
    """
    Intelligently select and execute a workflow based on task description.

    This endpoint uses a two-phase selection process:
    1. Heuristic matching (keywords, file patterns, branch patterns) - fast, zero LLM tokens
    2. LLM fallback for semantic understanding when heuristics don't match

    Features:
    - Automatic workflow selection based on task description
    - Confidence scoring for selection quality
    - Dry-run mode for previewing selection without execution
    - User confirmation when confidence is below threshold
    - Context variable extraction for workflow templates

    Example:
        POST /workflow/smart-execute
        {
            "task_description": "Deploy PR #123 to production",
            "context": {
                "git_branch": "feature/new-api",
                "project_type": "python"
            },
            "dry_run": true
        }

    Returns:
        SmartWorkflowResponse with selection details and optional workflow_id
    """
    from workflows.workflow_router import WorkflowSelection, get_workflow_router

    # Get or create the workflow router
    router = get_workflow_router(
        llm_client=llm_client,
        confirm_threshold=request.confirm_threshold or 0.7,
    )

    # Select the appropriate workflow
    context = request.context or {}
    selection: WorkflowSelection = await router.select_workflow(
        task_description=request.task_description,
        context=context,
        explicit_workflow=request.explicit_workflow,
        dry_run=request.dry_run,
    )

    # Build response
    response = SmartWorkflowResponse(
        workflow_name=selection.workflow_name,
        confidence=selection.confidence,
        method=selection.method.value,
        reasoning=selection.reasoning,
        requires_confirmation=selection.requires_confirmation,
        alternatives=selection.alternatives,
        context_variables=selection.context_variables,
    )

    # If dry run or requires confirmation, return without executing
    if request.dry_run or selection.requires_confirmation:
        response.execution_status = (
            "preview" if request.dry_run else "awaiting_confirmation"
        )
        return response

    # Execute the workflow
    try:
        from workflows.workflow_engine import WorkflowEngine

        engine = WorkflowEngine(llm_client=llm_client, state_client=state_client)
        template_name = f"{selection.workflow_name}.workflow.yaml"

        # Merge extracted context with provided context
        workflow_context = {**context, **selection.context_variables}

        # Execute workflow
        state = await engine.execute_workflow(
            template_name=template_name,
            context=workflow_context,
        )

        response.workflow_id = state.workflow_id
        response.execution_status = state.status.value

    except FileNotFoundError:
        response.execution_status = "error"
        response.reasoning += (
            f" | Error: Template '{selection.workflow_name}.workflow.yaml' not found"
        )
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}")
        response.execution_status = "error"
        response.reasoning += f" | Error: {str(e)}"

    return response


@app.post("/workflow/execute", response_model=Dict[str, Any])
@traceable(name="workflow_execute", tags=["workflow", "declarative", "yaml"])
async def execute_workflow(request: WorkflowExecuteRequest):
    """
    Execute a declarative workflow from YAML template.

    Workflows:
    - pr-deployment.workflow.yaml: Full PR review, test, staging, approval, production
    - hotfix.workflow.yaml: Fast-track emergency fixes (skip staging)
    - feature.workflow.yaml: Standard feature development lifecycle
    - docs-update.workflow.yaml: Documentation-only changes (skip tests)
    - infrastructure.workflow.yaml: IaC changes with plan, approval, apply, rollback

    Features:
    - Sequential step execution with state persistence
    - LLM decision gates for dynamic routing
    - HITL approval integration with Linear
    - Resource locking to prevent concurrent operations
    - Jinja2 template rendering for dynamic payloads

    Example:
        POST /workflow/execute
        {
            "template_name": "pr-deployment.workflow.yaml",
            "context": {
                "pr_number": 123,
                "repo_url": "github.com/org/repo",
                "branch": "feature/new-api",
                "environment": "production"
            }
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=get_llm_client("orchestrator"),
        state_client=state_client,
    )

    try:
        workflow_state = await engine.execute_workflow(
            template_name=request.template_name,
            context=request.context,
        )

        return {
            "workflow_id": workflow_state.workflow_id,
            "status": workflow_state.status.value,
            "current_step": workflow_state.current_step,
            "started_at": workflow_state.started_at,
            "completed_at": workflow_state.completed_at,
            "step_statuses": {
                k: v.value for k, v in workflow_state.step_statuses.items()
            },
            "outputs": workflow_state.outputs,
            "started_at": workflow_state.started_at.isoformat(),
            "completed_at": (
                workflow_state.completed_at.isoformat()
                if workflow_state.completed_at
                else None
            ),
        }

    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Workflow execution failed: {e}", exc_info=True)
        raise HTTPException(
            status_code=500, detail=f"Workflow execution failed: {str(e)}"
        )


@app.get("/workflow/status/{workflow_id}", response_model=WorkflowStatusResponse)
@traceable(name="workflow_status", tags=["workflow", "status"])
async def get_workflow_status(workflow_id: str):
    """
    Get current status of a workflow execution.

    Returns:
    - workflow_id: Unique workflow identifier
    - status: pending, running, paused (awaiting approval), completed, failed, rolled_back
    - current_step: ID of currently executing step
    - step_statuses: Status of each step (pending, running, completed, failed, skipped)
    - outputs: Outputs from completed steps
    - error_message: Error details if workflow failed

    Example:
        GET /workflow/abc123/status
        {
            "workflow_id": "abc123",
            "status": "paused",
            "current_step": "approval_gate",
            "step_statuses": {
                "code_review": "completed",
                "run_tests": "completed",
                "deploy_staging": "completed",
                "approval_gate": "running",
                "deploy_production": "pending"
            }
        }
    """
    from lib.llm_client import LLMClient
    from workflows.workflow_engine import WorkflowEngine

    # Initialize LLM client for workflow engine
    workflow_llm_client = LLMClient(agent_name="supervisor")

    engine = WorkflowEngine(
        llm_client=workflow_llm_client,
        state_client=registry_client,
    )

    try:
        workflow_state = await engine.get_workflow_status(workflow_id)
        return WorkflowStatusResponse(
            workflow_id=workflow_state.workflow_id,
            status=workflow_state.status.value,
            current_step=workflow_state.current_step,
            started_at=workflow_state.started_at,
            completed_at=workflow_state.completed_at,
            step_statuses={k: v.value for k, v in workflow_state.step_statuses.items()},
            error_message=workflow_state.error_message,
        )

    except Exception as e:
        raise HTTPException(
            status_code=404, detail=f"Workflow not found: {workflow_id}"
        )


@app.post("/workflow/resume/{workflow_id}", response_model=Dict[str, Any])
@traceable(name="workflow_resume", tags=["workflow", "hitl", "approval"])
async def resume_workflow(workflow_id: str, request: WorkflowResumeRequest):
    """
    Resume a paused workflow after HITL approval.

    When a workflow reaches an HITL approval step, it pauses and creates a Linear issue.
    Once the issue is approved/rejected, call this endpoint to resume execution.

    Args:
    - workflow_id: Workflow to resume
    - approval_decision: "approved" or "rejected"

    Example:
        POST /workflow/resume/abc123
        {
            "approval_decision": "approved"
        }

    The workflow will continue from the next step based on the approval decision:
    - "approved" ‚Üí Proceeds to on_approved step (e.g., deploy_production)
    - "rejected" ‚Üí Proceeds to on_rejected step (e.g., notify_failure)
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=get_llm_client("orchestrator"),
        state_client=state_client,
    )

    if request.approval_decision not in ["approved", "rejected"]:
        raise HTTPException(
            status_code=400, detail="approval_decision must be 'approved' or 'rejected'"
        )

    try:
        workflow_state = await engine.resume_workflow(
            workflow_id=workflow_id,
            approval_decision=request.approval_decision,
        )

        return {
            "workflow_id": workflow_state.workflow_id,
            "status": workflow_state.status.value,
            "current_step": workflow_state.current_step,
            "approval_decision": request.approval_decision,
            "step_statuses": {
                k: v.value for k, v in workflow_state.step_statuses.items()
            },
            "completed_at": (
                workflow_state.completed_at.isoformat()
                if workflow_state.completed_at
                else None
            ),
        }

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Workflow resume failed: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Workflow resume failed: {str(e)}")


# ============================================================================
# EVENT SOURCING ENDPOINTS (Week 4 - DEV-174)
# ============================================================================


@app.get("/workflow/{workflow_id}/events")
async def get_workflow_events(
    workflow_id: str,
    offset: int = 0,
    limit: int = 100,
    action: Optional[str] = None,
):
    """
    Get all events for a workflow (with pagination).

    Args:
        workflow_id: Workflow to get events for
        offset: Number of events to skip (for pagination)
        limit: Maximum events to return (default: 100, max: 1000)
        action: Optional filter by action type (e.g., "complete_step")

    Returns:
        List of workflow events with metadata

    Example:
        GET /workflow/abc123/events?limit=10&action=complete_step
        {
            "workflow_id": "abc123",
            "total_events": 42,
            "events": [
                {
                    "event_id": "uuid",
                    "action": "complete_step",
                    "step_id": "code_review",
                    "timestamp": "2024-01-15T10:30:00Z",
                    "data": {...}
                },
                ...
            ]
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Load all events
        all_events = await engine._load_events(workflow_id)

        # Filter by action if specified
        if action:
            all_events = [e for e in all_events if e.action == action]

        # Apply pagination
        paginated_events = all_events[offset : offset + min(limit, 1000)]

        return {
            "workflow_id": workflow_id,
            "total_events": len(all_events),
            "offset": offset,
            "limit": limit,
            "events": [e.to_dict() for e in paginated_events],
        }

    except Exception as e:
        logger.error(f"Failed to get events for workflow {workflow_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/workflow/{workflow_id}/events/export")
async def export_workflow_events(
    workflow_id: str,
    format: str = "json",
):
    """
    Export workflow events as JSON, CSV, or PDF audit report.

    Args:
        workflow_id: Workflow to export
        format: Export format (json, csv, pdf)

    Returns:
        File download with events in requested format

    Example:
        GET /workflow/abc-123/events/export?format=pdf
        (Downloads audit-report-abc-123.pdf)
    """
    from workflows.workflow_engine import WorkflowEngine

    from shared.lib.workflow_events import export_events_to_csv, export_events_to_json

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Load events
        events = await engine._load_events(workflow_id)
        event_dicts = [e.to_dict() for e in events]

        if format == "json":
            content = export_events_to_json(event_dicts)
            media_type = "application/json"
            filename = f"workflow-events-{workflow_id}.json"

        elif format == "csv":
            content = export_events_to_csv(event_dicts)
            media_type = "text/csv"
            filename = f"workflow-events-{workflow_id}.csv"

        elif format == "pdf":
            # TODO: Implement PDF generation with reportlab
            # For now, return JSON
            content = export_events_to_json(event_dicts)
            media_type = "application/json"
            filename = f"workflow-events-{workflow_id}.json"
            logger.warning("PDF export not yet implemented, returning JSON")

        else:
            raise HTTPException(status_code=400, detail=f"Unknown format: {format}")

        from fastapi.responses import Response

        return Response(
            content=content,
            media_type=media_type,
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )

    except Exception as e:
        logger.error(f"Failed to export events for workflow {workflow_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/workflow/{workflow_id}/replay")
async def replay_workflow_events(workflow_id: str):
    """
    Replay all events to reconstruct workflow state (time-travel debugging).

    Args:
        workflow_id: Workflow to replay

    Returns:
        Reconstructed state from events

    Example:
        POST /workflow/abc123/replay
        {
            "workflow_id": "abc123",
            "total_events": 42,
            "final_state": {
                "status": "completed",
                "steps_completed": ["code_review", "run_tests", ...],
                "outputs": {...}
            }
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Load events
        events = await engine._load_events(workflow_id)

        # Replay to reconstruct state
        final_state = await engine._reconstruct_state_from_events(workflow_id)

        return {
            "workflow_id": workflow_id,
            "total_events": len(events),
            "final_state": final_state,
        }

    except Exception as e:
        logger.error(f"Failed to replay workflow {workflow_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/workflow/{workflow_id}/state-at/{timestamp}")
async def get_workflow_state_at_timestamp(workflow_id: str, timestamp: str):
    """
    Get workflow state at a specific timestamp (time-travel debugging).

    Args:
        workflow_id: Workflow to query
        timestamp: ISO 8601 timestamp (e.g., "2024-01-15T10:30:00Z")

    Returns:
        Workflow state as it was at the specified timestamp

    Example:
        GET /workflow/abc-123/state-at/2024-01-15T10:30:00Z
        {
            "workflow_id": "abc-123",
            "timestamp": "2024-01-15T10:30:00Z",
            "state": {
                "status": "running",
                "current_step": "run_tests",
                "steps_completed": ["code_review"],
                ...
            }
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    from shared.lib.workflow_reducer import get_state_at_timestamp

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Load events
        events = await engine._load_events(workflow_id)

        # Reconstruct state at timestamp
        state_at_time = get_state_at_timestamp(events, timestamp)

        return {
            "workflow_id": workflow_id,
            "timestamp": timestamp,
            "state": state_at_time,
        }

    except Exception as e:
        logger.error(
            f"Failed to get state at timestamp for workflow {workflow_id}: {e}"
        )
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/workflow/{workflow_id}/snapshots")
async def get_workflow_snapshots(workflow_id: str):
    """
    Get all state snapshots for a workflow.

    Snapshots are created every 10 events for performance optimization.

    Args:
        workflow_id: Workflow to get snapshots for

    Returns:
        List of snapshots with metadata

    Example:
        GET /workflow/abc-123/snapshots
        {
            "workflow_id": "abc-123",
            "snapshots": [
                {
                    "snapshot_id": "uuid",
                    "event_count": 10,
                    "created_at": "2024-01-15T10:30:00Z"
                },
                ...
            ]
        }
    """
    try:
        # Query snapshots from database
        snapshots = await state_client.fetch(
            """
            SELECT snapshot_id, event_count, created_at
            FROM workflow_snapshots
            WHERE workflow_id = $1
            ORDER BY created_at DESC
            """,
            workflow_id,
        )

        return {
            "workflow_id": workflow_id,
            "snapshots": [
                {
                    "snapshot_id": str(row["snapshot_id"]),
                    "event_count": row["event_count"],
                    "created_at": row["created_at"].isoformat(),
                }
                for row in snapshots
            ],
        }

    except Exception as e:
        logger.error(f"Failed to get snapshots for workflow {workflow_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/workflow/{workflow_id}/annotate")
async def annotate_workflow_event(
    workflow_id: str,
    annotation: Dict[str, Any],
):
    """
    Add operator annotation/comment to workflow event log.

    Useful for incident tracking and post-mortems.

    Args:
        workflow_id: Workflow to annotate
        annotation: Annotation data
            - operator: Name of operator
            - comment: Annotation text
            - event_id: Optional specific event to annotate

    Returns:
        Created annotation event

    Example:
        POST /workflow/abc-123/annotate
        {
            "operator": "alice@example.com",
            "comment": "Manually approved due to emergency deployment",
            "event_id": "uuid-of-approval-event"
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Emit annotation event (stored in workflow_events table)
        from shared.lib.workflow_reducer import WorkflowAction, WorkflowEvent

        annotation_event = await engine._emit_event(
            workflow_id=workflow_id,
            action=WorkflowAction.ANNOTATE,  # New action type
            step_id=annotation.get("event_id"),
            data={
                "operator": annotation.get("operator", "unknown"),
                "comment": annotation.get("comment", ""),
                "timestamp": datetime.utcnow().isoformat(),
            },
        )

        return {
            "workflow_id": workflow_id,
            "annotation": annotation_event.to_dict(),
        }

    except Exception as e:
        logger.error(f"Failed to annotate workflow {workflow_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/workflow/{workflow_id}")
async def cancel_workflow(
    workflow_id: str,
    reason: str = "User requested cancellation",
    cancelled_by: Optional[str] = None,
):
    """
    Cancel a running or paused workflow with cleanup.

    Cleanup includes:
    - Release all resource locks
    - Mark Linear approval issues as complete
    - Notify participating agents
    - Cascade cancellation to child workflows

    Args:
        workflow_id: Workflow to cancel
        reason: Cancellation reason
        cancelled_by: User who requested cancellation

    Returns:
        Cancellation summary with cleanup details

    Example:
        DELETE /workflow/abc-123?reason=Emergency+fix+deployed&cancelled_by=alice@example.com
        {
            "workflow_id": "abc-123",
            "status": "cancelled",
            "reason": "Emergency fix deployed",
            "cancelled_by": "alice@example.com",
            "cleanup": {
                "locks_released": 2,
                "linear_issues_closed": 1,
                "agents_notified": 3,
                "child_workflows_cancelled": 0
            }
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        result = await engine.cancel_workflow(
            workflow_id=workflow_id,
            reason=reason,
            cancelled_by=cancelled_by or "unknown",
        )

        return result

    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f'Failed to cancel workflow {workflow_id}: {e}")')
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/workflow/{workflow_id}/retry-from/{step_id}")
async def retry_workflow_from_step(
    workflow_id: str,
    step_id: str,
    max_retries: int = 3,
):
    """
    Retry a failed workflow from a specific step.

    Uses exponential backoff and error classification to determine
    if retries are appropriate.

    Args:
        workflow_id: Workflow to retry
        step_id: Step to retry from
        max_retries: Maximum retry attempts (default: 3)

    Returns:
        Retry result with status

    Example:
        POST /workflow/abc-123/retry-from/deploy_staging?max_retries=5
        {
            "workflow_id": "abc-123",
            "step_id": "deploy_staging",
            "retry_attempt": 1,
            "status": "retrying",
            "next_retry_at": "2024-01-15T10:30:02Z"
        }
    """
    from workflows.workflow_engine import WorkflowEngine

    from shared.lib.retry_logic import RetryConfig

    engine = WorkflowEngine(
        llm_client=llm_client,
        state_client=state_client,
    )

    try:
        # Reconstruct state from events
        state_dict = await engine._reconstruct_state_from_events(workflow_id)

        if state_dict.get("status") != "failed":
            raise HTTPException(
                status_code=400,
                detail=f"Cannot retry workflow with status {state_dict.get('status')}. Only failed workflows can be retried.",
            )

        # Check retry count
        retry_count = state_dict.get("retries", {}).get(step_id, 0)

        if retry_count >= max_retries:
            raise HTTPException(
                status_code=400,
                detail=f"Maximum retries ({max_retries}) exceeded for step {step_id}",
            )

        # Emit RETRY_STEP event
        from shared.lib.retry_logic import RetryConfig, calculate_backoff
        from shared.lib.workflow_reducer import WorkflowAction

        config = RetryConfig(max_retries=max_retries)
        backoff_delay = calculate_backoff(retry_count, config)

        await engine._emit_event(
            workflow_id=workflow_id,
            action=WorkflowAction.RETRY_STEP,
            step_id=step_id,
            data={
                "retry_attempt": retry_count + 1,
                "max_retries": max_retries,
                "backoff_delay": backoff_delay,
            },
        )

        # Resume workflow execution from failed step
        # (In production, this would be done asynchronously via task queue)
        return {
            "workflow_id": workflow_id,
            "step_id": step_id,
            "retry_attempt": retry_count + 1,
            "max_retries": max_retries,
            "status": "retrying",
            "backoff_delay": backoff_delay,
            "message": "Retry scheduled. Workflow will resume after backoff delay.",
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to retry workflow {workflow_id} from step {step_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# WORKFLOW TEMPLATES
# ============================================================================


@app.get("/workflow/templates")
async def list_workflow_templates():
    """
    List available workflow templates with detailed metadata.

    Returns:
    - template_name: Filename of workflow template
    - name: Human-readable workflow name
    - description: What the workflow does
    - version: Template version
    - required_context: Context variables required for this workflow
    - steps_count: Number of steps in the workflow
    - agents_involved: List of agents used in the workflow
    - estimated_duration_minutes: Estimated execution time
    - risk_level: Risk level (low, medium, high)

    Example:
        GET /workflow/templates
        {
            "templates": [
                {
                    "template_name": "pr-deployment.workflow.yaml",
                    "name": "PR Deployment Workflow",
                    "description": "Automated PR review, test, and deployment pipeline",
                    "version": "1.0",
                    "required_context": ["pr_number", "branch", "environment"],
                    "steps_count": 8,
                    "agents_involved": ["code-review", "cicd", "infrastructure"],
                    "estimated_duration_minutes": 15,
                    "risk_level": "high"
                },
                ...
            ]
        }
    """
    from workflows.workflow_router import get_workflow_router

    # Get templates from the workflow router (includes metadata)
    router = get_workflow_router()
    templates_meta = router.get_available_templates()

    templates = []
    for name, meta in templates_meta.items():
        templates.append(
            {
                "template_name": f"{name}.workflow.yaml",
                "name": meta.name,
                "description": meta.description,
                "version": meta.version,
                "required_context": meta.required_context,
                "optional_context": meta.optional_context,
                "steps_count": meta.steps_count,
                "agents_involved": meta.agents_involved,
                "estimated_duration_minutes": meta.estimated_duration_minutes,
                "risk_level": meta.risk_level,
            }
        )

    return {"templates": templates, "count": len(templates)}


if __name__ == "__main__":
    port = int(os.getenv("PORT", "8001"))
    uvicorn.run(app, host="0.0.0.0", port=port)
