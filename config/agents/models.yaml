# LLM Model Configuration - Single Source of Truth
# All agent model configurations, cost metadata, and environment overrides
# Last Updated: December 9, 2025

version: "1.1"
provider: openrouter # Default provider (gradient, claude, mistral, openai, openrouter)

# OpenRouter Provider Configuration (Multi-Model Gateway)
# Documentation: https://openrouter.ai/docs
# Models: https://openrouter.ai/models
openrouter:
  base_url: https://openrouter.ai/api/v1
  default_model: anthropic/claude-3-5-sonnet
  streaming: true # SSE streaming enabled
  # Fallback models for automatic failover
  fallback_models:
    - openai/gpt-4o
    - meta-llama/llama-3.1-70b-instruct
    - google/gemini-2.0-flash-exp:free
  # Development tier (free/cheap models for testing)
  dev_models:
    - google/gemini-2.0-flash-exp:free
    - meta-llama/llama-3.1-8b-instruct:free
  # Agent-specific model preferences (used when provider=openrouter)
  agent_models:
    orchestrator: anthropic/claude-3-5-sonnet
    feature_dev: anthropic/claude-3-5-sonnet # Best for code generation
    code_review: openai/gpt-4o # Good reasoning for reviews
    infrastructure: meta-llama/llama-3.1-70b-instruct
    cicd: meta-llama/llama-3.1-70b-instruct
    documentation: anthropic/claude-3-5-sonnet

# Cross-Agent Knowledge Sharing Configuration
memory:
  enabled: true
  collection: agent_memory

  # Context retrieval settings
  max_context_insights: 3 # Max insights to inject per request
  min_confidence: 0.6 # Minimum relevance score threshold

  # Insight extraction settings
  extract_insights: true
  insight_types:
    - architectural_decision
    - error_pattern
    - code_pattern
    - task_resolution
    - security_finding

  # Retention policies
  retention:
    max_age_days: 30 # TTL for insights
    max_per_agent: 1000 # Cap per agent
    prune_schedule: daily # Pruning frequency

  # Per-agent memory overrides
  agent_overrides:
    feature_dev:
      extract_insights: true
      insight_types: [code_pattern, error_pattern, architectural_decision]
    code_review:
      extract_insights: true
      insight_types: [security_finding, code_pattern]
    infrastructure:
      extract_insights: true
      insight_types: [architectural_decision, error_pattern]
    cicd:
      extract_insights: true
      insight_types: [error_pattern, task_resolution]
    documentation:
      extract_insights: false # Docs agent typically consumes, not produces insights

# Error Recovery Configuration (per-agent tier limits and fallback chains)
# Integrated with ErrorRecoveryEngine for tiered recovery behavior
error_recovery:
  enabled: true
  config_path: "config/error-handling.yaml" # Reference to main error config

  # Default tier limits (can be overridden per-agent)
  defaults:
    max_tier: TIER_2 # RAG-assisted recovery
    max_retries: 3
    fail_fast: false
    circuit_breaker_enabled: true

  # Per-agent error recovery configuration
  agent_overrides:
    orchestrator:
      max_tier: TIER_3 # Supervisor can escalate to agent-assisted diagnosis
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "openai/gpt-4o"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "meta-llama/llama-3.1-70b-instruct"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 60

    feature-dev:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 90
        - model: "openai/gpt-4o"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "codellama-13b"
          provider: "gradient"
          timeout_seconds: 90

    code-review:
      max_tier: TIER_2
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "openai/gpt-4o"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 45

    infrastructure:
      max_tier: TIER_1 # Fail-fast for infrastructure
      max_retries: 1
      fail_fast: true # Critical: no speculative recovery for infra
      fallback_chain:
        - model: "meta-llama/llama-3.1-70b-instruct"
          provider: "openrouter"
          timeout_seconds: 45
        # No additional fallbacks for infrastructure - fail fast

    cicd:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "meta-llama/llama-3.1-70b-instruct"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45

    documentation:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "openai/gpt-4o"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "mistral-nemo-instruct-2407"
          provider: "gradient"
          timeout_seconds: 45

agents:
  orchestrator:
    model: anthropic/claude-3-5-sonnet
    provider: openrouter
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 3.00 # Claude 3.5 Sonnet via OpenRouter
    context_window: 200000
    use_case: complex_reasoning
    tags: [routing, orchestration, supervisor, streaming]
    langsmith_project: code-chef-orchestrator

  feature-dev:
    model: anthropic/claude-3-5-sonnet
    provider: openrouter
    temperature: 0.7
    max_tokens: 4000
    cost_per_1m_tokens: 3.00
    context_window: 200000
    use_case: code_generation
    tags: [feature-development, python, streaming]
    langsmith_project: code-chef-feature-dev

  code-review:
    model: openai/gpt-4o
    provider: openrouter
    temperature: 0.3
    max_tokens: 4000
    cost_per_1m_tokens: 2.50 # GPT-4o via OpenRouter
    context_window: 128000
    use_case: code_analysis
    tags: [quality-assurance, security, streaming]
    langsmith_project: code-chef-code-review

  infrastructure:
    model: meta-llama/llama-3.1-70b-instruct
    provider: openrouter
    temperature: 0.5
    max_tokens: 2000
    cost_per_1m_tokens: 0.88 # Llama 3.1 70B via OpenRouter
    context_window: 128000
    use_case: infrastructure_config
    tags: [terraform, kubernetes, docker, streaming]
    langsmith_project: code-chef-infrastructure

  cicd:
    model: meta-llama/llama-3.1-70b-instruct
    provider: openrouter
    temperature: 0.5
    max_tokens: 2000
    cost_per_1m_tokens: 0.88
    context_window: 128000
    use_case: pipeline_generation
    tags: [github-actions, jenkins, streaming]
    langsmith_project: code-chef-cicd

  documentation:
    model: anthropic/claude-3-5-sonnet
    provider: openrouter
    temperature: 0.7
    max_tokens: 4000
    cost_per_1m_tokens: 3.00
    context_window: 200000
    use_case: documentation_generation
    tags: [markdown, technical-writing, streaming]
    langsmith_project: code-chef-documentation

# Optional: Environment-specific overrides
environments:
  production:
    # Production uses full-power OpenRouter models
    orchestrator:
      model: anthropic/claude-3-5-sonnet
      provider: openrouter
    code-review:
      model: openai/gpt-4o
      provider: openrouter
    feature-dev:
      model: anthropic/claude-3-5-sonnet
      provider: openrouter

  development:
    # Development uses free/cheaper models for cost savings
    orchestrator:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    code-review:
      model: meta-llama/llama-3.1-8b-instruct:free
      provider: openrouter
    feature-dev:
      model: meta-llama/llama-3.1-8b-instruct:free
      provider: openrouter
    infrastructure:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    cicd:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    documentation:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter

  # Gradient fallback (if OpenRouter is unavailable)
  gradient-fallback:
    orchestrator:
      model: llama3.3-70b-instruct
      provider: gradient
    code-review:
      model: llama3.3-70b-instruct
      provider: gradient
    feature-dev:
      model: codellama-13b
      provider: gradient
