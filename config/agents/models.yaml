# LLM Model Configuration - Single Source of Truth
# All agent model configurations, cost metadata, and environment overrides
# Last Updated: November 24, 2025

version: "1.0"
provider: gradient # Default provider (gradient, claude, mistral, openai)

# Cross-Agent Knowledge Sharing Configuration
memory:
  enabled: true
  collection: agent_memory

  # Context retrieval settings
  max_context_insights: 3 # Max insights to inject per request
  min_confidence: 0.6 # Minimum relevance score threshold

  # Insight extraction settings
  extract_insights: true
  insight_types:
    - architectural_decision
    - error_pattern
    - code_pattern
    - task_resolution
    - security_finding

  # Retention policies
  retention:
    max_age_days: 30 # TTL for insights
    max_per_agent: 1000 # Cap per agent
    prune_schedule: daily # Pruning frequency

  # Per-agent memory overrides
  agent_overrides:
    feature_dev:
      extract_insights: true
      insight_types: [code_pattern, error_pattern, architectural_decision]
    code_review:
      extract_insights: true
      insight_types: [security_finding, code_pattern]
    infrastructure:
      extract_insights: true
      insight_types: [architectural_decision, error_pattern]
    cicd:
      extract_insights: true
      insight_types: [error_pattern, task_resolution]
    documentation:
      extract_insights: false # Docs agent typically consumes, not produces insights

# Error Recovery Configuration (per-agent tier limits and fallback chains)
# Integrated with ErrorRecoveryEngine for tiered recovery behavior
error_recovery:
  enabled: true
  config_path: "config/error-handling.yaml" # Reference to main error config

  # Default tier limits (can be overridden per-agent)
  defaults:
    max_tier: TIER_2 # RAG-assisted recovery
    max_retries: 3
    fail_fast: false
    circuit_breaker_enabled: true

  # Per-agent error recovery configuration
  agent_overrides:
    orchestrator:
      max_tier: TIER_3 # Supervisor can escalate to agent-assisted diagnosis
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 60
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45
        - model: "gpt-4o-mini"
          provider: "openai"
          timeout_seconds: 30

    feature-dev:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "codellama-13b"
          provider: "gradient"
          timeout_seconds: 90
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 60
        - model: "gpt-4"
          provider: "openai"
          timeout_seconds: 60

    code-review:
      max_tier: TIER_2
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 60
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45

    infrastructure:
      max_tier: TIER_1 # Fail-fast for infrastructure
      max_retries: 1
      fail_fast: true # Critical: no speculative recovery for infra
      fallback_chain:
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45
        # No additional fallbacks for infrastructure - fail fast

    cicd:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45
        - model: "llama3.3-70b-instruct"
          provider: "gradient"
          timeout_seconds: 60

    documentation:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "mistral-nemo-instruct-2407"
          provider: "gradient"
          timeout_seconds: 45
        - model: "llama3-8b-instruct"
          provider: "gradient"
          timeout_seconds: 45

agents:
  orchestrator:
    model: llama3.3-70b-instruct
    provider: gradient
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 0.60
    context_window: 128000
    use_case: complex_reasoning
    tags: [routing, orchestration, supervisor]
    langsmith_project: code-chef-orchestrator

  feature-dev:
    model: codellama-13b
    provider: gradient
    temperature: 0.7
    max_tokens: 2000
    cost_per_1m_tokens: 0.30
    context_window: 16000
    use_case: code_generation
    tags: [feature-development, python]
    langsmith_project: code-chef-feature-dev

  code-review:
    model: llama3.3-70b-instruct
    provider: gradient
    temperature: 0.3
    max_tokens: 4000
    cost_per_1m_tokens: 0.60
    context_window: 128000
    use_case: code_analysis
    tags: [quality-assurance, security]
    langsmith_project: code-chef-code-review

  infrastructure:
    model: llama3-8b-instruct
    provider: gradient
    temperature: 0.5
    max_tokens: 2000
    cost_per_1m_tokens: 0.20
    context_window: 128000
    use_case: infrastructure_config
    tags: [terraform, kubernetes, docker]
    langsmith_project: code-chef-infrastructure

  cicd:
    model: llama3-8b-instruct
    provider: gradient
    temperature: 0.5
    max_tokens: 2000
    cost_per_1m_tokens: 0.20
    context_window: 128000
    use_case: pipeline_generation
    tags: [github-actions, jenkins]
    langsmith_project: code-chef-cicd

  documentation:
    model: mistral-nemo-instruct-2407
    provider: gradient
    temperature: 0.7
    max_tokens: 2000
    cost_per_1m_tokens: 0.20
    context_window: 8192
    use_case: documentation_generation
    tags: [markdown, technical-writing]
    langsmith_project: code-chef-documentation

# Optional: Environment-specific overrides
environments:
  production:
    orchestrator:
      model: llama3.3-70b-instruct
    code-review:
      model: llama3.3-70b-instruct

  development:
    orchestrator:
      model: llama3-8b-instruct # Cheaper for testing
    code-review:
      model: llama3-8b-instruct
    feature-dev:
      model: llama3-8b-instruct
