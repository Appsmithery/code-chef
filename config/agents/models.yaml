# LLM Model Configuration - Single Source of Truth
# All agent model configurations, cost metadata, and environment overrides
# Last Updated: December 9, 2025

version: "1.1"
provider: openrouter # Default provider (gradient, claude, mistral, openai, openrouter)

# OpenRouter Provider Configuration (Multi-Model Gateway)
# Documentation: https://openrouter.ai/docs
# Models: https://openrouter.ai/models
# Last optimized: December 9, 2025 - 72% cost reduction
openrouter:
  base_url: https://openrouter.ai/api/v1
  default_model: anthropic/claude-3-5-sonnet
  streaming: true # SSE streaming enabled
  # Fallback models for automatic failover
  fallback_models:
    - deepseek/deepseek-chat
    - google/gemini-2.0-flash-001
    - qwen/qwen-2.5-coder-32b-instruct
  # Development tier (free/cheap models for testing)
  dev_models:
    - google/gemini-2.0-flash-exp:free
    - qwen/qwen-2.5-coder-32b-instruct
  # Agent-specific model preferences (used when provider=openrouter)
  agent_models:
    orchestrator: anthropic/claude-3-5-sonnet # Routing needs top-tier reasoning
    supervisor: anthropic/claude-3-5-sonnet # Supervisor needs top-tier reasoning
    conversational: anthropic/claude-3-5-sonnet # Conversational handler
    intent_recognizer: qwen/qwen-2.5-coder-7b-instruct # Fast intent classification ($0.02/M, optimized for low latency)
    feature_dev: qwen/qwen-2.5-coder-32b-instruct # Purpose-built for code gen ($0.07/M)
    code_review: deepseek/deepseek-chat # DeepSeek V3 excels at code analysis ($0.75/M)
    infrastructure: google/gemini-2.0-flash-001 # Fast, 1M context, great function calling ($0.25/M)
    cicd: google/gemini-2.0-flash-001 # Same + high uptime for pipelines ($0.25/M)
    documentation: deepseek/deepseek-chat # Strong writing + 164K context ($0.75/M)

# Cross-Agent Knowledge Sharing Configuration
memory:
  enabled: true
  collection: agent_memory

  # Context retrieval settings
  max_context_insights: 3 # Max insights to inject per request
  min_confidence: 0.6 # Minimum relevance score threshold

  # Insight extraction settings
  extract_insights: true
  insight_types:
    - architectural_decision
    - error_pattern
    - code_pattern
    - task_resolution
    - security_finding

  # Retention policies
  retention:
    max_age_days: 30 # TTL for insights
    max_per_agent: 1000 # Cap per agent
    prune_schedule: daily # Pruning frequency

  # Per-agent memory overrides
  agent_overrides:
    feature_dev:
      extract_insights: true
      insight_types: [code_pattern, error_pattern, architectural_decision]
    code_review:
      extract_insights: true
      insight_types: [security_finding, code_pattern]
    infrastructure:
      extract_insights: true
      insight_types: [architectural_decision, error_pattern]
    cicd:
      extract_insights: true
      insight_types: [error_pattern, task_resolution]
    documentation:
      extract_insights: false # Docs agent typically consumes, not produces insights

# Error Recovery Configuration (per-agent tier limits and fallback chains)
# Integrated with ErrorRecoveryEngine for tiered recovery behavior
error_recovery:
  enabled: true
  config_path: "config/error-handling.yaml" # Reference to main error config

  # Default tier limits (can be overridden per-agent)
  defaults:
    max_tier: TIER_2 # RAG-assisted recovery
    max_retries: 3
    fail_fast: false
    circuit_breaker_enabled: true

  # Per-agent error recovery configuration
  agent_overrides:
    orchestrator:
      max_tier: TIER_3 # Supervisor can escalate to agent-assisted diagnosis
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "openai/gpt-4o"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "meta-llama/llama-3.1-70b-instruct"
          provider: "openrouter"
          timeout_seconds: 45

    feature-dev:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "qwen/qwen-2.5-coder-32b-instruct"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "deepseek/deepseek-chat"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 90

    code-review:
      max_tier: TIER_2
      max_retries: 2
      fail_fast: false
      fallback_chain:
        - model: "deepseek/deepseek-chat"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "google/gemini-2.0-flash-001"
          provider: "openrouter"
          timeout_seconds: 45

    infrastructure:
      max_tier: TIER_1 # Fail-fast for infrastructure
      max_retries: 1
      fail_fast: true # Critical: no speculative recovery for infra
      fallback_chain:
        - model: "google/gemini-2.0-flash-001"
          provider: "openrouter"
          timeout_seconds: 45
        # No additional fallbacks for infrastructure - fail fast

    cicd:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "google/gemini-2.0-flash-001"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "deepseek/deepseek-chat"
          provider: "openrouter"
          timeout_seconds: 60
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 60

    documentation:
      max_tier: TIER_2
      max_retries: 3
      fail_fast: false
      fallback_chain:
        - model: "deepseek/deepseek-chat"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "anthropic/claude-3-5-sonnet"
          provider: "openrouter"
          timeout_seconds: 45
        - model: "google/gemini-2.0-flash-001"
          provider: "openrouter"
          timeout_seconds: 45

agents:
  orchestrator:
    model: anthropic/claude-3-5-sonnet
    provider: openrouter
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 3.00 # Claude 3.5 Sonnet via OpenRouter
    context_window: 200000
    use_case: complex_reasoning
    tags: [routing, orchestration, supervisor, streaming]
    langsmith_project: code-chef-orchestrator

  supervisor:
    model: anthropic/claude-3-5-sonnet
    provider: openrouter
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 3.00 # Claude 3.5 Sonnet via OpenRouter
    context_window: 200000
    use_case: complex_reasoning
    tags: [routing, orchestration, supervisor, streaming]
    langsmith_project: code-chef-orchestrator

  feature-dev:
    model: qwen/qwen-2.5-coder-32b-instruct
    provider: openrouter
    temperature: 0.7
    max_tokens: 4000
    cost_per_1m_tokens: 0.07 # Qwen Coder 32B ($0.03 in / $0.11 out avg)
    context_window: 32768
    use_case: code_generation
    tags: [feature-development, python, typescript, streaming]
    langsmith_project: code-chef-feature-dev

  code-review:
    model: deepseek/deepseek-chat
    provider: openrouter
    temperature: 0.2
    max_tokens: 4000
    cost_per_1m_tokens: 0.75 # DeepSeek V3 ($0.30 in / $1.20 out avg)
    context_window: 163840
    use_case: code_analysis
    tags: [quality-assurance, security, streaming]
    langsmith_project: code-chef-code-review

  infrastructure:
    model: google/gemini-2.0-flash-001
    provider: openrouter
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 0.25 # Gemini Flash 2.0 ($0.10 in / $0.40 out avg)
    context_window: 1048576
    use_case: infrastructure_config
    tags: [terraform, kubernetes, docker, yaml, streaming]
    langsmith_project: code-chef-infrastructure

  cicd:
    model: google/gemini-2.0-flash-001
    provider: openrouter
    temperature: 0.3
    max_tokens: 2000
    cost_per_1m_tokens: 0.25 # Gemini Flash 2.0 ($0.10 in / $0.40 out avg)
    context_window: 1048576
    use_case: pipeline_generation
    tags: [github-actions, jenkins, yaml, streaming]
    langsmith_project: code-chef-cicd

  documentation:
    model: deepseek/deepseek-chat
    provider: openrouter
    temperature: 0.6
    max_tokens: 4000
    cost_per_1m_tokens: 0.75 # DeepSeek V3 ($0.30 in / $1.20 out avg)
    context_window: 163840
    use_case: documentation_generation
    tags: [markdown, technical-writing, streaming]
    langsmith_project: code-chef-documentation

# Optional: Environment-specific overrides
environments:
  production:
    # Production uses full-power OpenRouter models
    orchestrator:
      model: anthropic/claude-3-5-sonnet
      provider: openrouter
    code-review:
      model: openai/gpt-4o
      provider: openrouter
    feature-dev:
      model: anthropic/claude-3-5-sonnet
      provider: openrouter

  development:
    # Development uses cheap models for cost savings (still high quality)
    orchestrator:
      model: deepseek/deepseek-chat
      provider: openrouter
    code-review:
      model: qwen/qwen-2.5-coder-32b-instruct
      provider: openrouter
    feature-dev:
      model: qwen/qwen-2.5-coder-32b-instruct
      provider: openrouter
    infrastructure:
      model: google/gemini-2.0-flash-001
      provider: openrouter
    cicd:
      model: google/gemini-2.0-flash-001
      provider: openrouter
    documentation:
      model: deepseek/deepseek-chat
      provider: openrouter

  # Free tier fallback (for testing/demos)
  free-tier:
    orchestrator:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    code-review:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    feature-dev:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    infrastructure:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    cicd:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
    documentation:
      model: google/gemini-2.0-flash-exp:free
      provider: openrouter
