# Dev-Tools Runtime Environment Template
# Copy to config/env/.env and fill in with real values before running deploy scripts.

# Runtime Environment
NODE_ENV=production
PORT=8000
SERVICE_NAME=gateway-mcp

# ============================================================================
# ORCHESTRATOR API KEY AUTHENTICATION
# ============================================================================
# Secure the orchestrator API with API key authentication.
# Generate a secure key: python -c "import secrets; print(secrets.token_urlsafe(32))"
# 
# If not set, authentication is DISABLED (development mode only!)
# VS Code extension must be configured with the same key.
# ============================================================================
ORCHESTRATOR_API_KEY=

# MCP Gateway
MCP_GATEWAY_URL=http://gateway-mcp:8000

# ============================================================================
# LINEAR INTEGRATION - WORKSPACE-AWARE (November 2025)
# ============================================================================
# Team-Level Configuration (same team for all projects)
# Project IDs are now DYNAMIC - auto-created per workspace
#
# How it works:
# 1. VS Code extension extracts workspace context (repo URL, commit SHA)
# 2. Extension reads Linear project ID from .vscode/settings.json (if exists)
# 3. Orchestrator gets or creates Linear project for workspace
# 4. Extension caches project ID in workspace settings for future use
#
# Benefits:
# - Zero config for new projects (auto-create Linear project)
# - Multi-project workflow (each workspace has own Linear project)
# - Permalinks point to correct GitHub repo (workspace-aware)
# - Clean .env (only team-level config + secrets)
# ============================================================================

# Team ID (same team for all projects)
# Get from: Linear Settings → Teams → Your Team → URL has team ID
LINEAR_TEAM_ID=

# Linear API Key (OAuth token for GraphQL API access)
# Get from: https://linear.app/settings/api
LINEAR_API_KEY=

# OAuth Configuration (Secrets)
# Get from: Linear Settings → API → OAuth Applications
LINEAR_OAUTH_CLIENT_ID=
LINEAR_OAUTH_CLIENT_SECRET=
LINEAR_OAUTH_DEV_TOKEN=

# Webhook Security (Secrets)
# Get from: Linear Settings → API → Webhooks → Create webhook
LINEAR_ORCHESTRATOR_WEBHOOK_SECRET=
LINEAR_WEBHOOK_SIGNING_SECRET=

# Optional: Agent-specific template overrides (if different from orchestrator defaults)
# By default, all agents inherit orchestrator templates (defined in linear-config.yaml)
# Uncomment and set these ONLY if you need agent-specific templates:
# HITL_FEATURE_DEV_TEMPLATE_UUID=<uuid>
# HITL_CODE_REVIEW_TEMPLATE_UUID=<uuid>
# HITL_INFRASTRUCTURE_TEMPLATE_UUID=<uuid>
# HITL_CICD_TEMPLATE_UUID=<uuid>
# HITL_DOCUMENTATION_TEMPLATE_UUID=<uuid>

# SMTP Configuration (for email notifications)
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=
SMTP_PASS=
SMTP_TLS=true
NOTIFICATION_EMAIL_FROM=             # Sender email address
NOTIFICATION_EMAIL_TO=               # Recipient email address

# Database
DB_HOST=postgres
DB_PORT=5432
DB_NAME=devtools
DB_USER=devtools
DB_PASSWORD=changeme

# Logging
LOG_LEVEL=info

# ============================================================================
# WEEK 5: ZEN PATTERN INTEGRATION (Task 5.3 - Workflow TTL Management)
# ============================================================================
# Auto-expire abandoned workflows to prevent memory leaks
# Active workflows refresh TTL on every event (stay alive)
# Zen pattern: Battle-tested with 100+ concurrent conversations in production
#
# Recommended values:
# - Development: 3 hours (fast cleanup for testing)
# - Staging: 12 hours
# - Production: 24 hours (standard)
# ============================================================================
WORKFLOW_TTL_HOURS=24

# ============================================================================
# LANGSMITH TRACING (LangChain/LangGraph Native Observability)
# ============================================================================
# Multi-project isolation for trace separation:
# - code-chef (production): 30-day retention, live agent traces
# - code-chef-testing (E2E): 7-day retention, automated tests
# - code-chef-dev (local): 3-day retention, development
#
# Documentation: https://docs.langchain.com/langsmith/observability-quickstart
# Get keys from: https://smith.langchain.com/settings
# ============================================================================
LANGSMITH_TRACING=true
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_PROJECT=code-chef       # Override per environment: code-chef-testing, code-chef-dev
LANGCHAIN_API_KEY=                # Service Key (lsv2_sk_*) for production, or Personal Access Token (lsv2_pt_*)
LANGSMITH_API_KEY=                # Same as LANGCHAIN_API_KEY (both supported for compatibility)
LANGSMITH_WORKSPACE_ID=           # Get from LangSmith settings (UUID format)

# DigitalOcean Integration
DIGITAL_OCEAN_PAT=

# Docker Hub Configuration
DOCKER_USERNAME=appsmithery
IMAGE_TAG=latest

# DigitalOcean Gradient AI Platform (LLM Inference)
# Documentation: https://docs.digitalocean.com/products/gradient-ai-platform/how-to/use-serverless-inference/
LLM_PROVIDER=gradient
EMBEDDING_PROVIDER=gradient

# Serverless Inference (OpenAI-compatible chat completions)
GRADIENT_BASE_URL=https://inference.do-ai.run/v1
GRADIENT_MODEL_ACCESS_KEY=
GRADIENT_MODEL=llama3-8b-instruct
GRADIENT_EMBEDDING_MODEL=text-embedding-ada-002
EMBEDDING_TIMEOUT=60

# Agentic Cloud API (agent workspaces, knowledge bases, guardrails)
GRADIENT_GENAI_BASE_URL=https://api.digitalocean.com
GRADIENT_API_KEY=

# LLM Provider Keys (optional, for multi-provider support)
CLAUDE_API_KEY=
MISTRAL_API_KEY=
OPEN_AI_DEVTOOLS_KEY=

# ============================================================================
# OPENROUTER LLM PROVIDER (Multi-Model Gateway)
# ============================================================================
# OpenRouter provides access to 200+ models via single OpenAI-compatible API
# Get API key from: https://openrouter.ai/keys
# Model list: https://openrouter.ai/models
#
# Recommended models:
# - anthropic/claude-3-5-sonnet (best for code, recommended)
# - openai/gpt-4o (good all-rounder)
# - meta-llama/llama-3.1-70b-instruct (cost-effective)
# - google/gemini-2.0-flash-exp:free (free tier for development)
#
# Features:
# - Automatic model fallback with route: "fallback"
# - Cost tracking via /api/v1/generation endpoint
# - Native streaming support
# ============================================================================
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_DEFAULT_MODEL=anthropic/claude-3-5-sonnet

# LLM Provider Selection
# Options: gradient | openrouter | openai | claude | mistral
# NOTE: Set to "openrouter" to use OpenRouter's multi-model gateway
LLM_PROVIDER=gradient

# ============================================================================
# QDRANT CLOUD VECTOR DATABASE (RAG Semantic Search)
# ============================================================================
# Production cluster for semantic search across codebase, Linear issues, and docs
# 
# Setup:
# 1. Create cluster at https://cloud.qdrant.io
# 2. Generate API key (Settings → API Keys → Create with no expiration)
# 3. Copy cluster URL from Connection Details
#
# Current Production Collections (814 vectors total):
# - code_patterns (505): Python AST extraction from codebase [DEFAULT]
# - issue_tracker (155): Linear issues via GraphQL API
# - library_registry (56): Context7 MCP library ID cache (DEV-194)
# - vendor-docs (94): API documentation
# - feature_specs (4): Linear projects
# - task_context (0): Workflow events (future)
# - agent_memory (0): Agent conversations (future)
#
# NOTE: the-shop collection was deleted (Jan 2025) - contained stale data
#
# Indexing scripts: support/scripts/rag/index_*.py
# ============================================================================
QDRANT_URL=                       # Qdrant Cloud cluster URL (https://<cluster-id>.<region>.gcp.cloud.qdrant.io)
QDRANT_API_KEY=
QDRANT_COLLECTION=code_patterns
QDRANT_VECTOR_SIZE=1536
QDRANT_DISTANCE=cosine

# OpenAI Embeddings (for RAG indexing and queries)
# Required for: index_code_patterns.py, index_issue_tracker.py, index_library_registry.py
OPENAI_API_KEY=

# Legacy (deprecated - use QDRANT_URL and QDRANT_API_KEY above)
# QDRANT_HOST=qdrant
# QDRANT_PORT=6333

# DigitalOcean Knowledge Base Sync (DEPRECATED - the-shop collection deleted)
# DIGITALOCEAN_KB_UUID=
# DIGITALOCEAN_KB_REF=the-shop
# DIGITALOCEAN_KB_MANIFEST=config/env/workspaces/the-shop.json
# DIGITALOCEAN_KB_DOWNLOAD_DIR=./tmp/kb-sync

# Supabase Integration
SUPABASE_URL=
SUPABASE_TOKEN=
SUPABASE_ANON_KEY=
SUPABASE_SERVICE_ROLE_KEY=

# Reverse Proxy & OAuth (Caddy + oauth2-proxy)
CADDY_DOMAIN=                        # Production domain (e.g., codechef.example.com)
CADDY_ACME_EMAIL=                    # Email for Let's Encrypt certificates
OAUTH2_PROXY_PROVIDER=github
OAUTH2_PROXY_CLIENT_ID=
OAUTH2_PROXY_CLIENT_SECRET=
OAUTH2_PROXY_COOKIE_SECRET=
OAUTH2_PROXY_REDIRECT_URL=http://localhost/oauth2/callback
OAUTH2_PROXY_ALLOWED_EMAILS=
OAUTH2_PROXY_GITHUB_ORG=
