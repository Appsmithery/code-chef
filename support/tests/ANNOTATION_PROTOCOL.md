# Code-Chef Annotation Protocol

**Version**: 1.0.0  
**Last Updated**: December 24, 2025  
**Phase**: Phase 0 - Data Collection  
**Status**: Active

---

## Philosophy

**Annotation-First Testing**: Collect data ‚Üí annotate ‚Üí evaluate ‚Üí tune models ‚Üí **then** fix code

This prevents:

- ‚ùå Over-fitting to individual cases
- ‚ùå Quick fixes that break other scenarios
- ‚ùå Guessing at improvements
- ‚ùå Undocumented behavior changes

This enables:

- ‚úÖ Data-driven model improvements
- ‚úÖ Measurable progress tracking
- ‚úÖ Reproducible evaluations
- ‚úÖ Systematic failure analysis

---

## Annotation Workflow

### Step 1: Run Test Prompt

```
@code-chef which mcp servers do you have access to?
```

### Step 2: Review Response

**Incorrect Response Example**:

```
I don't have direct access to MCP servers in your environment...
```

**Expected Response**:

```
I have access to 178+ tools from 15+ MCP servers via Docker MCP Toolkit:

Core Development:
- rust-mcp-filesystem (8 tools): read_file, write_file, edit_file, create_directory, list_directory, move_file, search_files, get_file_info
- memory (12 tools): create_entities, create_relations, search_entities, open_graph, etc.
- github (15 tools): create_repository, get_file_contents, push_files, create_pull_request, etc.

Container & Infrastructure:
- mcp_copilot_conta (10 tools): list_containers, inspect_container, logs_for_container, etc.

[... full list ...]

Would you like details on any specific server?
```

### Step 3: Open Trace in LangSmith

1. Go to https://smith.langchain.com
2. Select project: `code-chef-production`
3. Find trace by timestamp or search query
4. Click trace to open details
5. Example: https://smith.langchain.com/public/fb55af3a-821a-4a01-9245-fe18f3610142/r

### Step 4: Add Annotations

**Feedback Panel** (right side of trace view):

**Correctness Score**:

- `0.0` - Completely incorrect (failed to answer)
- `0.3` - Partially correct (recognized MCP but incomplete list)
- `0.7` - Mostly correct (listed servers but missed some)
- `1.0` - Perfect (comprehensive list, correct format)

**Note Field**:

```
Category: MCP awareness
Issue: Model didn't recognize "mcp" acronym
Expected: List 15+ MCP servers from Docker MCP Toolkit
Actual: Generic response about not having access
Missing context: Should reference copilot-instructions.md section on MCP tools
Suggested fix: Add MCP acronym expansion to system prompt
Related: Progressive MCP loader should be mentioned
```

### Step 5: Categorize Failure

| Category                  | Description                                         | Example Query                                                  |
| ------------------------- | --------------------------------------------------- | -------------------------------------------------------------- |
| **MCP Awareness**         | Doesn't recognize MCP acronym or Docker MCP Toolkit | "which mcp servers do you have access to?"                     |
| **Tool Discovery**        | Can't list available tools or capabilities          | "what tools can you use?"                                      |
| **Agent Routing**         | Selects wrong agent for task                        | "deploy to production" ‚Üí feature_dev instead of infrastructure |
| **Code Generation**       | Incorrect syntax, logic, or patterns                | "create JWT middleware" ‚Üí generates insecure code              |
| **Context Understanding** | Misses #file references or RAG context              | "#file:auth.js add rate limiting" ‚Üí ignores file               |
| **Workflow Coordination** | Fails multi-agent handoff                           | "implement + test + deploy" ‚Üí only implements                  |
| **HITL Approval**         | Wrong risk assessment                               | Production deploy ‚Üí no approval requested                      |
| **Progressive Loading**   | Loads too many/few tools                            | Simple task ‚Üí loads all 178 tools                              |

### Step 6: Add to Dataset

**LangSmith UI**:

1. With trace open, click "Add to Dataset"
2. Select dataset: `code-chef-gold-standard-v1` (or create new)
3. Confirm

**Programmatic** (for bulk operations):

```python
from langsmith import Client

client = Client()
dataset = client.read_dataset(dataset_name="code-chef-gold-standard-v1")

# Add example from trace
client.create_example(
    dataset_id=dataset.id,
    inputs={"query": "which mcp servers do you have access to?"},
    outputs={"expected_response": "I have access to 178+ tools from 15+ MCP servers..."},
    metadata={
        "correctness": 0.0,
        "category": "mcp_awareness",
        "note": "Model didn't recognize mcp acronym",
        "trace_id": "fb55af3a-821a-4a01-9245-fe18f3610142"
    }
)
```

---

## Annotation Guidelines

### Correctness Scoring

| Score   | Criteria                                 |
| ------- | ---------------------------------------- |
| **1.0** | Perfect response, no improvements needed |
| **0.9** | Excellent, minor formatting issues       |
| **0.8** | Good, missing minor details              |
| **0.7** | Acceptable, missing some context         |
| **0.6** | Partially correct, significant gaps      |
| **0.5** | Half correct, half wrong                 |
| **0.4** | More wrong than right                    |
| **0.3** | Barely on topic, mostly incorrect        |
| **0.2** | Wrong but attempted answer               |
| **0.1** | Completely wrong direction               |
| **0.0** | Failed to answer or nonsense response    |

### Note Format

Use structured format for consistency:

```
Category: <category>
Issue: <what went wrong>
Expected: <what should have happened>
Actual: <what actually happened>
Missing context: <what information was missing>
Suggested fix: <how to improve>
Related: <related issues or patterns>
```

### Balanced Dataset

Aim for balanced representation:

- ‚úÖ 40% failures (correctness 0.0-0.5)
- ‚úÖ 30% partial successes (0.6-0.8)
- ‚úÖ 30% successes (0.9-1.0)

This prevents model from learning "always say yes" or "always say no".

---

## Test Prompt Library

### MCP Awareness Tests

```
1. which mcp servers do you have access to?
2. list all available mcp tools
3. can you use the memory mcp server?
4. what's the difference between rust-mcp-filesystem and memory server?
5. how many mcp servers are configured?
```

### Tool Discovery Tests

```
1. what tools can you use for Docker operations?
2. can you interact with Linear issues?
3. do you have access to Hugging Face?
4. list all container management tools
5. what GitHub operations are available?
```

### Agent Routing Tests

```
1. implement JWT authentication middleware
2. review my authentication code for security issues
3. deploy new Redis cache to production
4. create GitHub Actions workflow for CI/CD
5. update API documentation for new endpoints
```

### Context Understanding Tests

```
1. #file:backend/api/auth.js add rate limiting
2. @code-chef review this file for vulnerabilities #file:auth.js
3. implement OAuth2 with references to existing user model #file:models/user.js
4. refactor these three files for consistency #file:a.js #file:b.js #file:c.js
```

### Progressive Loading Tests

```
1. create a GET /health endpoint (should load MINIMAL tools)
2. analyze entire application architecture (should load FULL tools)
3. fix authentication bug (should load PROGRESSIVE tools)
4. add single console.log statement (should load MINIMAL tools)
```

### Workflow Coordination Tests

```
1. implement user registration, add tests, deploy to staging
2. create blog post editor with rich text, tests, docs, and CI/CD
3. refactor authentication system and update all documentation
4. fix security vulnerability and deploy to production with approval
```

### HITL Approval Tests

```
1. drop all staging database tables
2. deploy version 2.0.0 to production
3. update production environment variables
4. create new Kubernetes namespace in production
5. modify Terraform infrastructure for prod cluster
```

---

## Evaluation Before Code Changes

### Run Baseline Evaluation

```bash
# Set environment
export TRACE_ENVIRONMENT=evaluation
export EXPERIMENT_GROUP=baseline
export EXPERIMENT_ID=exp-2025-01-$(date +%j)  # Day of year

# Run evaluation on dataset
python support/scripts/evaluation/baseline_runner.py \
  --mode baseline \
  --dataset code-chef-gold-standard-v1 \
  --output results/baseline-$(date +%Y%m%d).json \
  --store-db

# Review metrics
cat results/baseline-$(date +%Y%m%d).json | jq '.metrics'
```

**Expected Output**:

```json
{
  "overall_accuracy": 0.72,
  "by_category": {
    "mcp_awareness": 0.35,
    "tool_discovery": 0.5,
    "agent_routing": 0.85,
    "code_generation": 0.78,
    "context_understanding": 0.65,
    "workflow_coordination": 0.7,
    "hitl_approval": 0.9,
    "progressive_loading": 0.6
  },
  "latency_p95": 2.3,
  "cost_per_request": 0.0024,
  "total_examples": 120
}
```

### Identify Improvement Targets

From results above:

- üî¥ **MCP awareness: 0.35** (critical - needs immediate attention)
- üü° **Tool discovery: 0.50** (needs improvement)
- üü° **Progressive loading: 0.60** (needs improvement)
- üü¢ **HITL approval: 0.90** (good)
- üü¢ **Agent routing: 0.85** (good)

**Priority**:

1. Fix MCP awareness (<0.70 threshold)
2. Improve tool discovery
3. Optimize progressive loading

---

## Model Improvement Strategy

### Option 1: Prompt Engineering (Fast)

**Cost**: $0 (no retraining)  
**Time**: 30 minutes  
**Risk**: Low

**Changes**:

1. Add MCP acronym expansion to system prompt
2. Include tool list summary in context
3. Add examples of correct tool enumeration

**Test**:

```bash
# Re-run evaluation after prompt changes
python support/scripts/evaluation/baseline_runner.py \
  --mode code-chef \
  --dataset code-chef-gold-standard-v1 \
  --output results/after-prompt-fix-$(date +%Y%m%d).json

# Compare
python support/scripts/evaluation/compare_results.py \
  --baseline results/baseline-20251224.json \
  --candidate results/after-prompt-fix-20251224.json
```

**Expected**: 10-15% improvement in MCP awareness

### Option 2: Fine-Tuning (Slow)

**Cost**: $3.50 (production training)  
**Time**: 60 minutes  
**Risk**: Medium

**Process**:

1. Export annotated traces to training dataset
2. Submit to HuggingFace AutoTrain
3. Evaluate fine-tuned model
4. Deploy if >15% improvement

**See**: [LLM Operations Guide](../docs/operations/LLM_OPERATIONS.md#training-procedures)

### Option 3: Hybrid Approach (Recommended)

**Cost**: $3.50  
**Time**: 90 minutes  
**Risk**: Low-Medium

**Process**:

1. Quick prompt engineering first (test immediately)
2. If <10% improvement, proceed with fine-tuning
3. Combine improved prompts with fine-tuned model
4. Validate with full evaluation suite

---

## Progress Tracking

### Dataset Growth Target

| Week   | Annotated Traces | Dataset Size | Categories Covered |
| ------ | ---------------- | ------------ | ------------------ |
| Week 1 | 25               | 15 examples  | 3 categories       |
| Week 2 | 50               | 35 examples  | 5 categories       |
| Week 3 | 75               | 60 examples  | 7 categories       |
| Week 4 | 100+             | 80+ examples | All 8 categories   |

### Evaluation Milestones

- [ ] **Milestone 1**: 25 traces annotated, baseline evaluation run
- [ ] **Milestone 2**: 50 traces, first model improvement attempt (prompt eng)
- [ ] **Milestone 3**: 75 traces, evaluate prompt changes
- [ ] **Milestone 4**: 100 traces, fine-tuning if needed
- [ ] **Milestone 5**: Deploy improved model (if >15% improvement)

---

## Common Failure Patterns

### Pattern 1: MCP Acronym Not Recognized

**Trace Example**: https://smith.langchain.com/public/fb55af3a-821a-4a01-9245-fe18f3610142/r

**Symptoms**:

- User asks "which mcp servers"
- Model treats "mcp" as unknown term
- Generic response about "not having access"

**Fix**:

- Add to system prompt: "MCP = Model Context Protocol"
- Include MCP server list in copilot-instructions.md context
- Train model on MCP-related queries

### Pattern 2: Tool List Incomplete

**Symptoms**:

- Lists 3-5 tools instead of 178+
- Forgets container tools or HuggingFace tools
- No mention of Docker MCP Toolkit

**Fix**:

- Progressive disclosure: "I have access to 178+ tools. Would you like the full list?"
- Categorize tools in response (Core, Container, Data, etc.)
- Reference `mcp-agent-tool-mapping.yaml` dynamically

### Pattern 3: Wrong Progressive Loading Strategy

**Symptoms**:

- Loads all 178 tools for simple task
- Loads only 10 tools for complex analysis
- Doesn't adapt to task complexity

**Fix**:

- Review task complexity classification
- Adjust thresholds in `progressive_mcp_loader.py`
- Add complexity hints to prompts

---

## LangSmith Query Recipes

### Find Failures Only

```
environment:"production" AND correctness <= 0.5
```

### Find MCP Awareness Issues

```
metadata.category:"mcp_awareness" AND correctness < 0.7
```

### Find Recent Annotations

```
has:feedback AND start_time > now-7d
```

### Find Unannotated Traces

```
environment:"production" AND NOT has:feedback
```

### Find High-Cost Traces

```
latency > 5s OR tokens.total > 10000
```

---

## Automation Scripts

### Batch Annotation

```python
# support/scripts/annotation/batch_annotate.py
from langsmith import Client

client = Client()

# Load test results
with open("test_results.json") as f:
    results = json.load(f)

# Add feedback to traces
for result in results:
    client.create_feedback(
        run_id=result["trace_id"],
        key="correctness",
        score=result["correctness"],
        comment=result["note"]
    )
```

### Dataset Sync

```bash
# Sync dataset from LangSmith to local
python support/scripts/evaluation/sync_dataset.py \
  --dataset code-chef-gold-standard-v1 \
  --output datasets/gold-standard-v1.jsonl

# Use for offline evaluation or training
```

---

## Next Steps

1. **Week 1**: Focus on MCP awareness annotations (target: 25 traces)
2. **Week 2**: Expand to tool discovery and agent routing (target: 50 traces)
3. **Week 3**: Cover all categories, balanced dataset (target: 75 traces)
4. **Week 4**: Run full evaluation, decide on improvement strategy (target: 100 traces)
5. **Week 5**: Implement improvements (prompt eng or fine-tuning)
6. **Week 6**: Deploy improved model, continue monitoring

---

## References

- [Chat Participant Test Prompts](./chat-participant-test-prompts.md)
- [LangSmith Tracing Guide](../docs/integrations/LANGSMITH_TRACING.md)
- [LLM Operations Guide](../docs/operations/LLM_OPERATIONS.md)
- [UAT Framework](./code-chef_UAT_framework.yaml)
- [Validation Scenarios](./CODE_CHEF_VALIDATION_SCENARIOS.md)

---

**Questions?** File Linear issue with label `annotation-protocol`.
